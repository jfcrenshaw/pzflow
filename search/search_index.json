{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pzflow","title":"PZFlow","text":"<p>PZFlow is a python package for probabilistic modeling of tabular data with normalizing flows.</p> <p>If your data consists of continuous variables that can be put into a Pandas DataFrame, pzflow can model the joint probability distribution of your data set.</p> <p>The <code>Flow</code> class makes building and training a normalizing flow simple. It also allows you to easily sample from the normalizing flow (e.g., for forward modeling or data augmentation), and calculate posteriors over any of your variables.</p> <p>There are several tutorial notebooks in the docs.</p>"},{"location":"#installation","title":"Installation","text":"<p>See the instructions in the docs.</p>"},{"location":"#citation","title":"Citation","text":"<p>We are preparing a paper on pzflow. If you use this package in your research, please check back here for a citation before publication. In the meantime, please cite the Zenodo release.</p>"},{"location":"#sources","title":"Sources","text":"<p>PZFlow was originally designed for forward modeling of photometric redshifts as a part of the Creation Module of the DESC RAIL project. The idea to use normalizing flows for photometric redshifts originated with Bryce Kalmbach. The earliest version of the normalizing flow in RAIL was based on a notebook by Francois Lanusse and included contributions from Alex Malz.</p> <p>The functional jax structure of the bijectors was originally based on <code>jax-flows</code> by Chris Waites. The implementation of the Neural Spline Coupling is largely based on the Tensorflow implementation, with some inspiration from <code>nflows</code>.</p> <p>Neural Spline Flows are based on the following papers:</p> <p>NICE: Non-linear Independent Components Estimation\\ Laurent Dinh, David Krueger, Yoshua Bengio\\ arXiv:1410.8516</p> <p>Density estimation using Real NVP\\ Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio\\ arXiv:1605.08803</p> <p>Neural Spline Flows\\ Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios\\ arXiv:1906.04032</p>"},{"location":"contributing/","title":"Contributing to PZFlow","text":"<p>If you notice any bugs, have any questions, or would like to request a feature, please submit an issue.</p> <p>To work on pzflow, after forking and cloning the repo:</p> <ol> <li>Create a virtual environment with Python E.g., with conda <code>conda create -n pzflow</code></li> <li>Activate the environment. E.g., <code>conda activate pzflow</code></li> <li>Install pzflow in edit mode with the <code>dev</code> flag I.e., in the root directory, <code>pip install -e \".[dev]\"</code></li> </ol>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages.\"\"\"\n</pre> \"\"\"Generate the code reference pages.\"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre># recurse through python files in pzflow\nfor path in sorted(Path(\"pzflow\").rglob(\"*.py\")):\n    # get the module path, not including the pzflow prefix\n    module_path = path.with_suffix(\"\")\n    # path where we'll save the markdown file for this module\n    doc_path = \"API\" / path.relative_to(\"pzflow\").with_suffix(\".md\")\n\n    # split up the path into its parts\n    parts = list(module_path.parts)\n\n    # we don't want to explicitly list __init__\n    if parts[-1] in [\"__init__\"]:\n        #continue\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n    # skip main files\n    elif parts[-1] == \"__main__\":\n        continue\n\n    # create the markdown file in the docs directory\n    with mkdocs_gen_files.open(doc_path, \"w\") as fd:\n        identifier = \".\".join(parts)\n        print(doc_path, identifier)\n        print(\"::: \" + identifier, file=fd)\n\n    mkdocs_gen_files.set_edit_path(doc_path, path)\n</pre> # recurse through python files in pzflow for path in sorted(Path(\"pzflow\").rglob(\"*.py\")):     # get the module path, not including the pzflow prefix     module_path = path.with_suffix(\"\")     # path where we'll save the markdown file for this module     doc_path = \"API\" / path.relative_to(\"pzflow\").with_suffix(\".md\")      # split up the path into its parts     parts = list(module_path.parts)      # we don't want to explicitly list __init__     if parts[-1] in [\"__init__\"]:         #continue         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")     # skip main files     elif parts[-1] == \"__main__\":         continue      # create the markdown file in the docs directory     with mkdocs_gen_files.open(doc_path, \"w\") as fd:         identifier = \".\".join(parts)         print(doc_path, identifier)         print(\"::: \" + identifier, file=fd)      mkdocs_gen_files.set_edit_path(doc_path, path)"},{"location":"gotchas/","title":"Common gotchas","text":"<ul> <li> <p>It is important to note that there are two different conventions in the literature for the direction of the bijection in normalizing flows. PZFlow defines the bijection as the mapping from the data space to the latent space, and the inverse bijection as the mapping from the latent space to the data space. This distinction can be important when designing more complicated bijections (e.g., in Example 2 above).</p> </li> <li> <p>If you get NaNs during training, try decreasing the learning rate (the default is 1e-3):</p> </li> </ul> <pre><code>import optax\n\nopt = optax.adam(learning_rate=...)\nflow.train(..., optimizer=opt)\n</code></pre>"},{"location":"install/","title":"Install","text":"<p>You can install PZFlow from PyPI with pip:</p> <pre><code>pip install pzflow\n</code></pre> <p>If you want to run PZFlow on a GPU with CUDA, you need to follow the GPU-enabled installation instructions for Jax here. You may also need to add cuda to your path. For example, I needed to add the following to my <code>.bashrc</code>:</p> <pre><code># cuda setup\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\nexport PATH=$PATH:/usr/local/cuda/bin\n</code></pre> <p>If you have the GPU enabled version of jax installed, but would like to run on a CPU, add the following to the top of your scripts/notebooks:</p> <pre><code>import jax\n# Global flag to set a specific platform, must be used at startup.\njax.config.update('jax_platform_name', 'cpu')\n</code></pre> <p>Note that if you run jax on GPU in multiple Jupyter notebooks simultaneously, you may get <code>RuntimeError: cuSolver internal error</code>. Read more here and here.</p>"},{"location":"API/","title":"API","text":"<p>Import modules and set version.</p>"},{"location":"API/bijectors/","title":"Bijectors","text":"<p>Define the bijectors used in the normalizing flows.</p>"},{"location":"API/bijectors/#pzflow.bijectors.Bijector","title":"<code>Bijector</code>","text":"<p>Wrapper class for bijector functions</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>class Bijector:\n    \"\"\"Wrapper class for bijector functions\"\"\"\n\n    def __init__(self, func: Callable) -&gt; None:\n        self._func = func\n        update_wrapper(self, func)\n\n    def __call__(self, *args, **kwargs) -&gt; Tuple[InitFunction, Bijector_Info]:\n        return self._func(*args, **kwargs)\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.ForwardFunction","title":"<code>ForwardFunction</code>","text":"<p>Return the output and log_det of the forward bijection on the inputs.</p> <p>ForwardFunction of a Bijector, originally returned by the InitFunction of the Bijector.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>A pytree of bijector parameters. This usually looks like a nested tuple or list of parameters.</p> required <code>inputs</code> <code>ndarray</code> <p>The data to be transformed by the bijection.</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>ndarray</code> <p>Result of the forward bijection applied to the inputs.</p> <code>log_det</code> <code>ndarray</code> <p>The log determinant of the Jacobian evaluated at the inputs.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>class ForwardFunction:\n    \"\"\"Return the output and log_det of the forward bijection on the inputs.\n\n    ForwardFunction of a Bijector, originally returned by the\n    InitFunction of the Bijector.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        A pytree of bijector parameters.\n        This usually looks like a nested tuple or list of parameters.\n    inputs : jnp.ndarray\n        The data to be transformed by the bijection.\n\n    Returns\n    -------\n    outputs : jnp.ndarray\n        Result of the forward bijection applied to the inputs.\n    log_det : jnp.ndarray\n        The log determinant of the Jacobian evaluated at the inputs.\n    \"\"\"\n\n    def __init__(self, func: Callable) -&gt; None:\n        self._func = func\n\n    def __call__(\n        self, params: Pytree, inputs: jnp.ndarray, **kwargs\n    ) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n        return self._func(params, inputs, **kwargs)\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.InitFunction","title":"<code>InitFunction</code>","text":"<p>Initialize the corresponding Bijector.</p> <p>InitFunction returned by the initialization of a Bijector.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>ndarray</code> <p>A Random Number Key from jax.random.PRNGKey.</p> required <code>input_dim</code> <code>int</code> <p>The input dimension of the bijection.</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>a Jax pytree</code> <p>A pytree of bijector parameters. This usually looks like a nested tuple or list of parameters.</p> <code>forward_fun</code> <code>ForwardFunction</code> <p>The forward function of the Bijector.</p> <code>inverse_fun</code> <code>InverseFunction</code> <p>The inverse function of the Bijector.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>class InitFunction:\n    \"\"\"Initialize the corresponding Bijector.\n\n    InitFunction returned by the initialization of a Bijector.\n\n    Parameters\n    ----------\n    rng : jnp.ndarray\n        A Random Number Key from jax.random.PRNGKey.\n    input_dim : int\n        The input dimension of the bijection.\n\n    Returns\n    -------\n    params : a Jax pytree\n        A pytree of bijector parameters.\n        This usually looks like a nested tuple or list of parameters.\n    forward_fun : ForwardFunction\n        The forward function of the Bijector.\n    inverse_fun : InverseFunction\n        The inverse function of the Bijector.\n    \"\"\"\n\n    def __init__(self, func: Callable) -&gt; None:\n        self._func = func\n\n    def __call__(\n        self, rng: jnp.ndarray, input_dim: int, **kwargs\n    ) -&gt; Tuple[Pytree, ForwardFunction, InverseFunction]:\n        return self._func(rng, input_dim, **kwargs)\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.InverseFunction","title":"<code>InverseFunction</code>","text":"<p>Return the output and log_det of the inverse bijection on the inputs.</p> <p>InverseFunction of a Bijector, originally returned by the InitFunction of the Bijector.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>A pytree of bijector parameters. This usually looks like a nested tuple or list of parameters.</p> required <code>inputs</code> <code>ndarray</code> <p>The data to be transformed by the bijection.</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>ndarray</code> <p>Result of the inverse bijection applied to the inputs.</p> <code>log_det</code> <code>ndarray</code> <p>The log determinant of the Jacobian evaluated at the inputs.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>class InverseFunction:\n    \"\"\"Return the output and log_det of the inverse bijection on the inputs.\n\n    InverseFunction of a Bijector, originally returned by the\n    InitFunction of the Bijector.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        A pytree of bijector parameters.\n        This usually looks like a nested tuple or list of parameters.\n    inputs : jnp.ndarray\n        The data to be transformed by the bijection.\n\n    Returns\n    -------\n    outputs : jnp.ndarray\n        Result of the inverse bijection applied to the inputs.\n    log_det : jnp.ndarray\n        The log determinant of the Jacobian evaluated at the inputs.\n    \"\"\"\n\n    def __init__(self, func: Callable) -&gt; None:\n        self._func = func\n\n    def __call__(\n        self, params: Pytree, inputs: jnp.ndarray, **kwargs\n    ) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n        return self._func(params, inputs, **kwargs)\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.Chain","title":"<code>Chain(*inputs)</code>","text":"<p>Bijector that chains multiple InitFunctions into a single InitFunction.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>(Bijector1(), Bijector2(), ...)</code> <p>A container of Bijector calls to be chained together.</p> <code>()</code> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the total chained Bijector.</p> <code>Bijector_Info</code> <p>Tuple('Chain', Tuple(Bijector_Info for each bijection in the chain)) This allows the chain to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef Chain(\n    *inputs: Sequence[Tuple[InitFunction, Bijector_Info]]\n) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that chains multiple InitFunctions into a single InitFunction.\n\n    Parameters\n    ----------\n    inputs : (Bijector1(), Bijector2(), ...)\n        A container of Bijector calls to be chained together.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the total chained Bijector.\n    Bijector_Info\n        Tuple('Chain', Tuple(Bijector_Info for each bijection in the chain))\n        This allows the chain to be recreated later.\n    \"\"\"\n\n    init_funs = tuple(i[0] for i in inputs)\n    bijector_info = (\"Chain\", tuple(i[1] for i in inputs))\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        all_params, forward_funs, inverse_funs = [], [], []\n        for init_f in init_funs:\n            rng, layer_rng = random.split(rng)\n            param, forward_f, inverse_f = init_f(layer_rng, input_dim)\n\n            all_params.append(param)\n            forward_funs.append(forward_f)\n            inverse_funs.append(inverse_f)\n\n        def bijector_chain(params, bijectors, inputs, **kwargs):\n            log_dets = jnp.zeros(inputs.shape[0])\n            for bijector, param in zip(bijectors, params):\n                inputs, log_det = bijector(param, inputs, **kwargs)\n                log_dets += log_det\n            return inputs, log_dets\n\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            return bijector_chain(params, forward_funs, inputs, **kwargs)\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            return bijector_chain(\n                params[::-1], inverse_funs[::-1], inputs, **kwargs\n            )\n\n        return all_params, forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.ColorTransform","title":"<code>ColorTransform(ref_idx, mag_idx)</code>","text":"<p>Bijector that calculates photometric colors from magnitudes.</p> <p>Using ColorTransform restricts and impacts the order of columns in the corresponding normalizing flow. See the notes below for an example.</p> <p>Parameters:</p> Name Type Description Default <code>ref_idx</code> <code>int</code> <p>The index corresponding to the column of the reference band, which serves as a proxy for overall luminosity.</p> required <code>mag_idx</code> <code>arraylike of int</code> <p>The indices of the magnitude columns from which colors will be calculated.</p> required <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the ColorTransform Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Notes <p>ColorTransform requires careful management of column order in the bijector. This is best explained with an example:</p> <p>Assume we have data [redshift, u, g, ellipticity, r, i, z, y, mass] Then ColorTransform(ref_idx=4, mag_idx=[1, 2, 4, 5, 6, 7]) will output [redshift, ellipticity, mass, r, u-g, g-r, r-i, i-z, z-y]</p> <p>Notice how the non-magnitude columns are aggregated at the front of the array, maintaining their relative order from the original array. These values are then followed by the reference magnitude, and the new colors.</p> <p>Also notice that the magnitudes indices in mag_idx are assumed to be adjacent colors. E.g. mag_idx=[1, 2, 5, 4, 6, 7] would have produced the colors [u-g, g-i, i-r, r-z, z-y]. You can chain multiple ColorTransforms back-to-back to create colors in a non-adjacent manner.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef ColorTransform(\n    ref_idx: int, mag_idx: int\n) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that calculates photometric colors from magnitudes.\n\n    Using ColorTransform restricts and impacts the order of columns in the\n    corresponding normalizing flow. See the notes below for an example.\n\n    Parameters\n    ----------\n    ref_idx : int\n        The index corresponding to the column of the reference band, which\n        serves as a proxy for overall luminosity.\n    mag_idx : arraylike of int\n        The indices of the magnitude columns from which colors will be calculated.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the ColorTransform Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n\n    Notes\n    -----\n    ColorTransform requires careful management of column order in the bijector.\n    This is best explained with an example:\n\n    Assume we have data\n    [redshift, u, g, ellipticity, r, i, z, y, mass]\n    Then\n    ColorTransform(ref_idx=4, mag_idx=[1, 2, 4, 5, 6, 7])\n    will output\n    [redshift, ellipticity, mass, r, u-g, g-r, r-i, i-z, z-y]\n\n    Notice how the non-magnitude columns are aggregated at the front of the\n    array, maintaining their relative order from the original array.\n    These values are then followed by the reference magnitude, and the new colors.\n\n    Also notice that the magnitudes indices in mag_idx are assumed to be\n    adjacent colors. E.g. mag_idx=[1, 2, 5, 4, 6, 7] would have produced\n    the colors [u-g, g-i, i-r, r-z, z-y]. You can chain multiple ColorTransforms\n    back-to-back to create colors in a non-adjacent manner.\n    \"\"\"\n\n    # validate parameters\n    if ref_idx &lt;= 0:\n        raise ValueError(\"ref_idx must be a positive integer.\")\n    if not isinstance(ref_idx, int):\n        raise ValueError(\"ref_idx must be an integer.\")\n    if ref_idx not in mag_idx:\n        raise ValueError(\"ref_idx must be in mag_idx.\")\n\n    bijector_info = (\"ColorTransform\", (ref_idx, mag_idx))\n\n    # convert mag_idx to an array\n    mag_idx = jnp.array(mag_idx)\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        # array of all the indices\n        all_idx = jnp.arange(input_dim)\n        # indices for columns to stick at the front\n        front_idx = jnp.setdiff1d(all_idx, mag_idx)\n        # the index corresponding to the first magnitude\n        mag0_idx = len(front_idx)\n\n        # the new column order\n        new_idx = jnp.concatenate((front_idx, mag_idx))\n        # the new column for the reference magnitude\n        new_ref = jnp.where(new_idx == ref_idx)[0][0]\n\n        # define a convenience function for the forward_fun below\n        # if the first magnitude is the reference mag, do nothing\n        if ref_idx == mag_idx[0]:\n\n            def mag0(outputs):\n                return outputs\n\n        # if the first magnitude is not the reference mag,\n        # then we need to calculate the first magnitude (mag[0])\n        else:\n\n            def mag0(outputs):\n                return outputs.at[:, mag0_idx].set(\n                    outputs[:, mag0_idx] + outputs[:, new_ref],\n                    indices_are_sorted=True,\n                    unique_indices=True,\n                )\n\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            # re-order columns and calculate colors\n            outputs = jnp.hstack(\n                (\n                    inputs[:, front_idx],  # other values\n                    inputs[:, ref_idx, None],  # ref mag\n                    -jnp.diff(inputs[:, mag_idx]),  # colors\n                )\n            )\n            # determinant of Jacobian is zero\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            # convert all colors to be in terms of the first magnitude, mag[0]\n            outputs = jnp.hstack(\n                (\n                    inputs[:, 0:mag0_idx],  # other values unchanged\n                    inputs[:, mag0_idx, None],  # reference mag unchanged\n                    jnp.cumsum(\n                        inputs[:, mag0_idx + 1 :], axis=-1\n                    ),  # all colors mag[i-1] - mag[i] --&gt; mag[0] - mag[i]\n                )\n            )\n            # calculate mag[0]\n            outputs = mag0(outputs)\n            # mag[i] = mag[0] - (mag[0] - mag[i])\n            outputs = outputs.at[:, mag0_idx + 1 :].set(\n                outputs[:, mag0_idx, None] - outputs[:, mag0_idx + 1 :],\n                indices_are_sorted=True,\n                unique_indices=True,\n            )\n            # return to original ordering\n            outputs = outputs[:, jnp.argsort(new_idx)]\n            # determinant of Jacobian is zero\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.InvSoftplus","title":"<code>InvSoftplus(column_idx, sharpness=1)</code>","text":"<p>Bijector that applies inverse softplus to the specified column(s).</p> <p>Applying the inverse softplus ensures that samples from that column will always be non-negative. This is because samples are the output of the inverse bijection -- so samples will have a softplus applied to them.</p> <p>Parameters:</p> Name Type Description Default <code>column_idx</code> <code>int</code> <p>An index or iterable of indices corresponding to the column(s) you wish to be transformed.</p> required <code>sharpness</code> <code>float; default=1</code> <p>The sharpness(es) of the softplus transformation. If more than one is provided, the list of sharpnesses must be of the same length as column_idx.</p> <code>1</code> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the Softplus Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef InvSoftplus(\n    column_idx: int, sharpness: float = 1\n) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that applies inverse softplus to the specified column(s).\n\n    Applying the inverse softplus ensures that samples from that column will\n    always be non-negative. This is because samples are the output of the\n    inverse bijection -- so samples will have a softplus applied to them.\n\n    Parameters\n    ----------\n    column_idx : int\n        An index or iterable of indices corresponding to the column(s)\n        you wish to be transformed.\n    sharpness : float; default=1\n        The sharpness(es) of the softplus transformation. If more than one\n        is provided, the list of sharpnesses must be of the same length as\n        column_idx.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the Softplus Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    idx = jnp.atleast_1d(column_idx)\n    k = jnp.atleast_1d(sharpness)\n    if len(idx) != len(k) and len(k) != 1:\n        raise ValueError(\n            \"Please provide either a single sharpness or one for each column index.\"\n        )\n\n    bijector_info = (\"InvSoftplus\", (column_idx, sharpness))\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            outputs = inputs.at[:, idx].set(\n                jnp.log(-1 + jnp.exp(k * inputs[:, idx])) / k,\n            )\n            log_det = jnp.log(1 + jnp.exp(-k * outputs[:, idx])).sum(axis=1)\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = inputs.at[:, idx].set(\n                jnp.log(1 + jnp.exp(k * inputs[:, idx])) / k,\n            )\n            log_det = -jnp.log(1 + jnp.exp(-k * inputs[:, idx])).sum(axis=1)\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.NeuralSplineCoupling","title":"<code>NeuralSplineCoupling(K=16, B=5, hidden_layers=2, hidden_dim=128, transformed_dim=None, n_conditions=0, periodic=False)</code>","text":"<p>A coupling layer bijection with rational quadratic splines.</p> <p>This Bijector is a Coupling Layer [1,2], and as such only transforms the second half of input dimensions (or the last N dimensions, where N = transformed_dim). In order to transform all of the dimensions, you need multiple Couplings interspersed with Bijectors that change the order of inputs dimensions, e.g., Reverse, Shuffle, Roll, etc.</p> <p>NeuralSplineCoupling uses piecewise rational quadratic splines, as developed in [3].</p> <p>If periodic=True, then this is a Circular Spline as described in [4].</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>int; default=16</code> <p>Number of bins in the spline (the number of knots is K+1).</p> <code>16</code> <code>B</code> <code>float; default=5</code> <p>Range of the splines. If periodic=False, outside of (-B,B), the transformation is just the identity. If periodic=True, the input is mapped into the appropriate location in the range (-B,B).</p> <code>5</code> <code>hidden_layers</code> <code>int; default=2</code> <p>The number of hidden layers in the neural network used to calculate the positions and derivatives of the spline knots.</p> <code>2</code> <code>hidden_dim</code> <code>int; default=128</code> <p>The width of the hidden layers in the neural network used to calculate the positions and derivatives of the spline knots.</p> <code>128</code> <code>transformed_dim</code> <code>int; optional</code> <p>The number of dimensions transformed by the splines. Default is ceiling(input_dim /2).</p> <code>None</code> <code>n_conditions</code> <code>int; default=0</code> <p>The number of variables to condition the bijection on.</p> <code>0</code> <code>periodic</code> <code>bool; default=False</code> <p>Whether to make this a periodic, Circular Spline [4].</p> <code>False</code> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the NeuralSplineCoupling Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> References <p>[1] Laurent Dinh, David Krueger, Yoshua Bengio. NICE: Non-linear     Independent Components Estimation. arXiv: 1605.08803, 2015.     http://arxiv.org/abs/1605.08803 [2] Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio.     Density Estimation Using Real NVP. arXiv: 1605.08803, 2017.     http://arxiv.org/abs/1605.08803 [3] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios.     Neural Spline Flows. arXiv:1906.04032, 2019.     https://arxiv.org/abs/1906.04032 [4] Rezende, Danilo Jimenez et al.     Normalizing Flows on Tori and Spheres. arxiv:2002.02428, 2020     http://arxiv.org/abs/2002.02428</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef NeuralSplineCoupling(\n    K: int = 16,\n    B: float = 5,\n    hidden_layers: int = 2,\n    hidden_dim: int = 128,\n    transformed_dim: int = None,\n    n_conditions: int = 0,\n    periodic: bool = False,\n) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"A coupling layer bijection with rational quadratic splines.\n\n    This Bijector is a Coupling Layer [1,2], and as such only transforms\n    the second half of input dimensions (or the last N dimensions, where\n    N = transformed_dim). In order to transform all of the dimensions,\n    you need multiple Couplings interspersed with Bijectors that change\n    the order of inputs dimensions, e.g., Reverse, Shuffle, Roll, etc.\n\n    NeuralSplineCoupling uses piecewise rational quadratic splines,\n    as developed in [3].\n\n    If periodic=True, then this is a Circular Spline as described in [4].\n\n    Parameters\n    ----------\n    K : int; default=16\n        Number of bins in the spline (the number of knots is K+1).\n    B : float; default=5\n        Range of the splines.\n        If periodic=False, outside of (-B,B), the transformation is just\n        the identity. If periodic=True, the input is mapped into the\n        appropriate location in the range (-B,B).\n    hidden_layers : int; default=2\n        The number of hidden layers in the neural network used to calculate\n        the positions and derivatives of the spline knots.\n    hidden_dim : int; default=128\n        The width of the hidden layers in the neural network used to\n        calculate the positions and derivatives of the spline knots.\n    transformed_dim : int; optional\n        The number of dimensions transformed by the splines.\n        Default is ceiling(input_dim /2).\n    n_conditions : int; default=0\n        The number of variables to condition the bijection on.\n    periodic : bool; default=False\n        Whether to make this a periodic, Circular Spline [4].\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the NeuralSplineCoupling Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n\n    References\n    ----------\n    [1] Laurent Dinh, David Krueger, Yoshua Bengio. NICE: Non-linear\n        Independent Components Estimation. arXiv: 1605.08803, 2015.\n        http://arxiv.org/abs/1605.08803\n    [2] Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio.\n        Density Estimation Using Real NVP. arXiv: 1605.08803, 2017.\n        http://arxiv.org/abs/1605.08803\n    [3] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios.\n        Neural Spline Flows. arXiv:1906.04032, 2019.\n        https://arxiv.org/abs/1906.04032\n    [4] Rezende, Danilo Jimenez et al.\n        Normalizing Flows on Tori and Spheres. arxiv:2002.02428, 2020\n        http://arxiv.org/abs/2002.02428\n    \"\"\"\n\n    if not isinstance(periodic, bool):\n        raise ValueError(\"`periodic` must be True or False.\")\n\n    bijector_info = (\n        \"NeuralSplineCoupling\",\n        (\n            K,\n            B,\n            hidden_layers,\n            hidden_dim,\n            transformed_dim,\n            n_conditions,\n            periodic,\n        ),\n    )\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        if transformed_dim is None:\n            upper_dim = input_dim // 2  # variables that determine NN params\n            lower_dim = (\n                input_dim - upper_dim\n            )  # variables transformed by the NN\n        else:\n            upper_dim = input_dim - transformed_dim\n            lower_dim = transformed_dim\n\n        # create the neural network that will take in the upper dimensions and\n        # will return the spline parameters to transform the lower dimensions\n        network_init_fun, network_apply_fun = DenseReluNetwork(\n            (3 * K - 1 + int(periodic)) * lower_dim, hidden_layers, hidden_dim\n        )\n        _, network_params = network_init_fun(rng, (upper_dim + n_conditions,))\n\n        # calculate spline parameters as a function of the upper variables\n        def spline_params(params, upper, conditions):\n            key = jnp.hstack((upper, conditions))[\n                :, : upper_dim + n_conditions\n            ]\n            outputs = network_apply_fun(params, key)\n            outputs = jnp.reshape(\n                outputs, [-1, lower_dim, 3 * K - 1 + int(periodic)]\n            )\n            W, H, D = jnp.split(outputs, [K, 2 * K], axis=2)\n            W = 2 * B * softmax(W)\n            H = 2 * B * softmax(H)\n            D = softplus(D)\n            return W, H, D\n\n        @ForwardFunction\n        def forward_fun(params, inputs, conditions, **kwargs):\n            # lower dimensions are transformed as function of upper dimensions\n            upper, lower = inputs[:, :upper_dim], inputs[:, upper_dim:]\n            # widths, heights, derivatives = function(upper dimensions)\n            W, H, D = spline_params(params, upper, conditions)\n            # transform the lower dimensions with the Rational Quadratic Spline\n            lower, log_det = RationalQuadraticSpline(\n                lower, W, H, D, B, periodic, inverse=False\n            )\n            outputs = jnp.hstack((upper, lower))\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, conditions, **kwargs):\n            # lower dimensions are transformed as function of upper dimensions\n            upper, lower = inputs[:, :upper_dim], inputs[:, upper_dim:]\n            # widths, heights, derivatives = function(upper dimensions)\n            W, H, D = spline_params(params, upper, conditions)\n            # transform the lower dimensions with the Rational Quadratic Spline\n            lower, log_det = RationalQuadraticSpline(\n                lower, W, H, D, B, periodic, inverse=True\n            )\n            outputs = jnp.hstack((upper, lower))\n            return outputs, log_det\n\n        return network_params, forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.Reverse","title":"<code>Reverse()</code>","text":"<p>Bijector that reverses the order of inputs.</p> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the the Reverse Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef Reverse() -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that reverses the order of inputs.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the the Reverse Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    bijector_info = (\"Reverse\", ())\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            outputs = inputs[:, ::-1]\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = inputs[:, ::-1]\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.Roll","title":"<code>Roll(shift=1)</code>","text":"<p>Bijector that rolls inputs along their last column using jnp.roll.</p> <p>Parameters:</p> Name Type Description Default <code>shift</code> <code>int; default=1</code> <p>The number of places to roll.</p> <code>1</code> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the the Roll Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef Roll(shift: int = 1) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that rolls inputs along their last column using jnp.roll.\n\n    Parameters\n    ----------\n    shift : int; default=1\n        The number of places to roll.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the the Roll Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    if not isinstance(shift, int):\n        raise ValueError(\"shift must be an integer.\")\n\n    bijector_info = (\"Roll\", (shift,))\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            outputs = jnp.roll(inputs, shift=shift, axis=-1)\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = jnp.roll(inputs, shift=-shift, axis=-1)\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.RollingSplineCoupling","title":"<code>RollingSplineCoupling(nlayers, shift=1, K=16, B=5, hidden_layers=2, hidden_dim=128, transformed_dim=None, n_conditions=0, periodic=False)</code>","text":"<p>Bijector that alternates NeuralSplineCouplings and Roll bijections.</p> <p>Parameters:</p> Name Type Description Default <code>nlayers</code> <code>int</code> <p>The number of (NeuralSplineCoupling(), Roll()) couplets in the chain.</p> required <code>shift</code> <code>int</code> <p>How far the inputs are shifted on each Roll().</p> <code>1</code> <code>K</code> <code>int; default=16</code> <p>Number of bins in the RollingSplineCoupling.</p> <code>16</code> <code>B</code> <code>float; default=5</code> <p>Range of the splines in the RollingSplineCoupling. If periodic=False, outside of (-B,B), the transformation is just the identity. If periodic=True, the input is mapped into the appropriate location in the range (-B,B).</p> <code>5</code> <code>hidden_layers</code> <code>int; default=2</code> <p>The number of hidden layers in the neural network used to calculate the bins and derivatives in the RollingSplineCoupling.</p> <code>2</code> <code>hidden_dim</code> <code>int; default=128</code> <p>The width of the hidden layers in the neural network used to calculate the bins and derivatives in the RollingSplineCoupling.</p> <code>128</code> <code>transformed_dim</code> <code>int; optional</code> <p>The number of dimensions transformed by the splines. Default is ceiling(input_dim /2).</p> <code>None</code> <code>n_conditions</code> <code>int; default=0</code> <p>The number of variables to condition the bijection on.</p> <code>0</code> <code>periodic</code> <code>bool; default=False</code> <p>Whether to make this a periodic, Circular Spline</p> <code>False</code> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the RollingSplineCoupling Bijector.</p> <code>Bijector_Info</code> <p>Nested tuple of the Bijector name and input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef RollingSplineCoupling(\n    nlayers: int,\n    shift: int = 1,\n    K: int = 16,\n    B: float = 5,\n    hidden_layers: int = 2,\n    hidden_dim: int = 128,\n    transformed_dim: int = None,\n    n_conditions: int = 0,\n    periodic: bool = False,\n) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that alternates NeuralSplineCouplings and Roll bijections.\n\n    Parameters\n    ----------\n    nlayers : int\n        The number of (NeuralSplineCoupling(), Roll()) couplets in the chain.\n    shift : int\n        How far the inputs are shifted on each Roll().\n    K : int; default=16\n        Number of bins in the RollingSplineCoupling.\n    B : float; default=5\n        Range of the splines in the RollingSplineCoupling.\n        If periodic=False, outside of (-B,B), the transformation is just\n        the identity. If periodic=True, the input is mapped into the\n        appropriate location in the range (-B,B).\n    hidden_layers : int; default=2\n        The number of hidden layers in the neural network used to calculate\n        the bins and derivatives in the RollingSplineCoupling.\n    hidden_dim : int; default=128\n        The width of the hidden layers in the neural network used to\n        calculate the bins and derivatives in the RollingSplineCoupling.\n    transformed_dim : int; optional\n        The number of dimensions transformed by the splines.\n        Default is ceiling(input_dim /2).\n    n_conditions : int; default=0\n        The number of variables to condition the bijection on.\n    periodic : bool; default=False\n        Whether to make this a periodic, Circular Spline\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the RollingSplineCoupling Bijector.\n    Bijector_Info\n        Nested tuple of the Bijector name and input parameters. This allows\n        it to be recreated later.\n\n    \"\"\"\n    return Chain(\n        *(\n            NeuralSplineCoupling(\n                K=K,\n                B=B,\n                hidden_layers=hidden_layers,\n                hidden_dim=hidden_dim,\n                transformed_dim=transformed_dim,\n                n_conditions=n_conditions,\n                periodic=periodic,\n            ),\n            Roll(shift),\n        )\n        * nlayers\n    )\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.Scale","title":"<code>Scale(scale)</code>","text":"<p>Bijector that multiplies inputs by a scalar.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>Factor by which to scale inputs.</p> required <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the the Scale Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef Scale(scale: float) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that multiplies inputs by a scalar.\n\n    Parameters\n    ----------\n    scale : float\n        Factor by which to scale inputs.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the the Scale Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    if isinstance(scale, jnp.ndarray):\n        if scale.dtype != jnp.float32:\n            raise ValueError(\"scale must be a float or array of floats.\")\n    elif not isinstance(scale, float):\n        raise ValueError(\"scale must be a float or array of floats.\")\n\n    bijector_info = (\"Scale\", (scale,))\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            outputs = scale * inputs\n            log_det = jnp.log(scale ** inputs.shape[-1]) * jnp.ones(\n                inputs.shape[0]\n            )\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = 1 / scale * inputs\n            log_det = -jnp.log(scale ** inputs.shape[-1]) * jnp.ones(\n                inputs.shape[0]\n            )\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.ShiftBounds","title":"<code>ShiftBounds(min, max, B=5)</code>","text":"<p>Bijector shifts the bounds of inputs so the lie in the range (-B, B).</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float</code> <p>The minimum of the input range.</p> required <code>min</code> <code>float</code> <p>The maximum of the input range.</p> required <code>B</code> <code>float; default=5</code> <p>The extent of the output bounds, which will be (-B, B).</p> <code>5</code> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the ShiftBounds Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef ShiftBounds(\n    min: float, max: float, B: float = 5\n) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector shifts the bounds of inputs so the lie in the range (-B, B).\n\n    Parameters\n    ----------\n    min : float\n        The minimum of the input range.\n    min : float\n        The maximum of the input range.\n    B : float; default=5\n        The extent of the output bounds, which will be (-B, B).\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the ShiftBounds Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    min = jnp.atleast_1d(min)\n    max = jnp.atleast_1d(max)\n    if len(min) != len(max):\n        raise ValueError(\n            \"Lengths of min and max do not match. \"\n            + \"Please provide either a single min and max, \"\n            + \"or a min and max for each dimension.\"\n        )\n    if (min &gt; max).any():\n        raise ValueError(\"All mins must be less than maxes.\")\n\n    bijector_info = (\"ShiftBounds\", (min, max, B))\n\n    mean = (max + min) / 2\n    half_range = (max - min) / 2\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            outputs = B * (inputs - mean) / half_range\n            log_det = jnp.log(jnp.prod(B / half_range)) * jnp.ones(\n                inputs.shape[0]\n            )\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = inputs * half_range / B + mean\n            log_det = jnp.log(jnp.prod(half_range / B)) * jnp.ones(\n                inputs.shape[0]\n            )\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.Shuffle","title":"<code>Shuffle()</code>","text":"<p>Bijector that randomly permutes inputs.</p> <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the Shuffle Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef Shuffle() -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that randomly permutes inputs.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the Shuffle Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    bijector_info = (\"Shuffle\", ())\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        perm = random.permutation(rng, jnp.arange(input_dim))\n        inv_perm = jnp.argsort(perm)\n\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            outputs = inputs[:, perm]\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = inputs[:, inv_perm]\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.StandardScaler","title":"<code>StandardScaler(means, stds)</code>","text":"<p>Bijector that applies standard scaling to each input.</p> <p>Each input dimension i has an associated mean u_i and standard dev s_i. Each input is rescaled as (input[i] - u_i)/s_i, so that each input dimension has mean zero and unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>means</code> <code>ndarray</code> <p>The mean of each column.</p> required <code>stds</code> <code>ndarray</code> <p>The standard deviation of each column.</p> required <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the StandardScaler Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef StandardScaler(\n    means: jnp.array, stds: jnp.array\n) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that applies standard scaling to each input.\n\n    Each input dimension i has an associated mean u_i and standard dev s_i.\n    Each input is rescaled as (input[i] - u_i)/s_i, so that each input dimension\n    has mean zero and unit variance.\n\n    Parameters\n    ----------\n    means : jnp.ndarray\n        The mean of each column.\n    stds : jnp.ndarray\n        The standard deviation of each column.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the StandardScaler Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    bijector_info = (\"StandardScaler\", (means, stds))\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            outputs = (inputs - means) / stds\n            log_det = jnp.log(1 / jnp.prod(stds)) * jnp.ones(inputs.shape[0])\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = inputs * stds + means\n            log_det = jnp.log(jnp.prod(stds)) * jnp.ones(inputs.shape[0])\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/bijectors/#pzflow.bijectors.UniformDequantizer","title":"<code>UniformDequantizer(column_idx)</code>","text":"<p>Bijector that dequantizes discrete variables with uniform noise.</p> <p>Dequantizers are necessary for modeling discrete values with a flow. Note that this isn't technically a bijector.</p> <p>Parameters:</p> Name Type Description Default <code>column_idx</code> <code>int</code> <p>An index or iterable of indices corresponding to the column(s) with discrete values.</p> required <p>Returns:</p> Type Description <code>InitFunction</code> <p>The InitFunction of the UniformDequantizer Bijector.</p> <code>Bijector_Info</code> <p>Tuple of the Bijector name and the input parameters. This allows it to be recreated later.</p> Source code in <code>pzflow/bijectors.py</code> <pre><code>@Bijector\ndef UniformDequantizer(column_idx: int) -&gt; Tuple[InitFunction, Bijector_Info]:\n    \"\"\"Bijector that dequantizes discrete variables with uniform noise.\n\n    Dequantizers are necessary for modeling discrete values with a flow.\n    Note that this isn't technically a bijector.\n\n    Parameters\n    ----------\n    column_idx : int\n        An index or iterable of indices corresponding to the column(s) with\n        discrete values.\n\n    Returns\n    -------\n    InitFunction\n        The InitFunction of the UniformDequantizer Bijector.\n    Bijector_Info\n        Tuple of the Bijector name and the input parameters.\n        This allows it to be recreated later.\n    \"\"\"\n\n    bijector_info = (\"UniformDequantizer\", (column_idx,))\n    column_idx = jnp.array(column_idx)\n\n    @InitFunction\n    def init_fun(rng, input_dim, **kwargs):\n        @ForwardFunction\n        def forward_fun(params, inputs, **kwargs):\n            u = random.uniform(\n                random.PRNGKey(0), shape=inputs[:, column_idx].shape\n            )\n            outputs = inputs.astype(float)\n            outputs.at[:, column_idx].set(outputs[:, column_idx] + u)\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        @InverseFunction\n        def inverse_fun(params, inputs, **kwargs):\n            outputs = inputs.at[:, column_idx].set(\n                jnp.floor(inputs[:, column_idx])\n            )\n            log_det = jnp.zeros(inputs.shape[0])\n            return outputs, log_det\n\n        return (), forward_fun, inverse_fun\n\n    return init_fun, bijector_info\n</code></pre>"},{"location":"API/distributions/","title":"Distributions","text":"<p>Define the latent distributions used in the normalizing flows.</p>"},{"location":"API/distributions/#pzflow.distributions.CentBeta","title":"<code>CentBeta</code>","text":"<p>               Bases: <code>LatentDist</code></p> <p>A centered Beta distribution.</p> <p>This distribution is just a regular Beta distribution, scaled and shifted to have support on the domain [-B, B] in each dimension.</p> <p>Alpha and beta parameters for each dimension are learned during training.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>class CentBeta(LatentDist):\n    \"\"\"A centered Beta distribution.\n\n    This distribution is just a regular Beta distribution, scaled and shifted\n    to have support on the domain [-B, B] in each dimension.\n\n    Alpha and beta parameters for each dimension are learned during training.\n    \"\"\"\n\n    def __init__(self, input_dim: int, B: float = 5) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        input_dim : int\n            The dimension of the distribution.\n        B : float; default=5\n            The distribution has support (-B, B) along each dimension.\n        \"\"\"\n        self.input_dim = input_dim\n        self.B = B\n\n        # save dist info\n        self._params = tuple([(0.0, 0.0) for i in range(input_dim)])\n        self.info = (\"CentBeta\", (input_dim, B))\n\n    def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Tuple of ((a1, b1), (a2, b2), ...) where aN,bN are log(alpha),log(beta)\n            for the Nth dimension.\n        inputs : jnp.ndarray\n            Input data for which log probability density is calculated.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0],).\n        \"\"\"\n        log_prob = jnp.hstack(\n            [\n                beta.logpdf(\n                    inputs[:, i],\n                    a=jnp.exp(params[i][0]),\n                    b=jnp.exp(params[i][1]),\n                    loc=-self.B,\n                    scale=2 * self.B,\n                ).reshape(-1, 1)\n                for i in range(self.input_dim)\n            ]\n        ).sum(axis=1)\n\n        return log_prob\n\n    def sample(\n        self, params: Pytree, nsamples: int, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Returns samples from the distribution.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Tuple of ((a1, b1), (a2, b2), ...) where aN,bN are log(alpha),log(beta)\n            for the Nth dimension.\n        nsamples : int\n            The number of samples to be returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (nsamples, self.input_dim).\n        \"\"\"\n        seed = np.random.randint(1e18) if seed is None else seed\n        seeds = random.split(random.PRNGKey(seed), self.input_dim)\n        samples = jnp.hstack(\n            [\n                random.beta(\n                    seeds[i],\n                    jnp.exp(params[i][0]),\n                    jnp.exp(params[i][1]),\n                    shape=(nsamples, 1),\n                )\n                for i in range(self.input_dim)\n            ]\n        )\n        return 2 * self.B * (samples - 0.5)\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.CentBeta.__init__","title":"<code>__init__(input_dim, B=5)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimension of the distribution.</p> required <code>B</code> <code>float; default=5</code> <p>The distribution has support (-B, B) along each dimension.</p> <code>5</code> Source code in <code>pzflow/distributions.py</code> <pre><code>def __init__(self, input_dim: int, B: float = 5) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    input_dim : int\n        The dimension of the distribution.\n    B : float; default=5\n        The distribution has support (-B, B) along each dimension.\n    \"\"\"\n    self.input_dim = input_dim\n    self.B = B\n\n    # save dist info\n    self._params = tuple([(0.0, 0.0) for i in range(input_dim)])\n    self.info = (\"CentBeta\", (input_dim, B))\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.CentBeta.log_prob","title":"<code>log_prob(params, inputs)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Tuple of ((a1, b1), (a2, b2), ...) where aN,bN are log(alpha),log(beta) for the Nth dimension.</p> required <code>inputs</code> <code>ndarray</code> <p>Input data for which log probability density is calculated.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0],).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Tuple of ((a1, b1), (a2, b2), ...) where aN,bN are log(alpha),log(beta)\n        for the Nth dimension.\n    inputs : jnp.ndarray\n        Input data for which log probability density is calculated.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0],).\n    \"\"\"\n    log_prob = jnp.hstack(\n        [\n            beta.logpdf(\n                inputs[:, i],\n                a=jnp.exp(params[i][0]),\n                b=jnp.exp(params[i][1]),\n                loc=-self.B,\n                scale=2 * self.B,\n            ).reshape(-1, 1)\n            for i in range(self.input_dim)\n        ]\n    ).sum(axis=1)\n\n    return log_prob\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.CentBeta.sample","title":"<code>sample(params, nsamples, seed=None)</code>","text":"<p>Returns samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Tuple of ((a1, b1), (a2, b2), ...) where aN,bN are log(alpha),log(beta) for the Nth dimension.</p> required <code>nsamples</code> <code>int</code> <p>The number of samples to be returned.</p> required <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (nsamples, self.input_dim).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def sample(\n    self, params: Pytree, nsamples: int, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Returns samples from the distribution.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Tuple of ((a1, b1), (a2, b2), ...) where aN,bN are log(alpha),log(beta)\n        for the Nth dimension.\n    nsamples : int\n        The number of samples to be returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (nsamples, self.input_dim).\n    \"\"\"\n    seed = np.random.randint(1e18) if seed is None else seed\n    seeds = random.split(random.PRNGKey(seed), self.input_dim)\n    samples = jnp.hstack(\n        [\n            random.beta(\n                seeds[i],\n                jnp.exp(params[i][0]),\n                jnp.exp(params[i][1]),\n                shape=(nsamples, 1),\n            )\n            for i in range(self.input_dim)\n        ]\n    )\n    return 2 * self.B * (samples - 0.5)\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.CentBeta13","title":"<code>CentBeta13</code>","text":"<p>               Bases: <code>LatentDist</code></p> <p>A centered Beta distribution with alpha, beta = 13.</p> <p>This distribution is just a regular Beta distribution, scaled and shifted to have support on the domain [-B, B] in each dimension.</p> <p>Alpha, beta = 13 means that the distribution looks like a Gaussian distribution, but with hard cutoffs at +/- B.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>class CentBeta13(LatentDist):\n    \"\"\"A centered Beta distribution with alpha, beta = 13.\n\n    This distribution is just a regular Beta distribution, scaled and shifted\n    to have support on the domain [-B, B] in each dimension.\n\n    Alpha, beta = 13 means that the distribution looks like a Gaussian\n    distribution, but with hard cutoffs at +/- B.\n    \"\"\"\n\n    def __init__(self, input_dim: int, B: float = 5) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        input_dim : int\n            The dimension of the distribution.\n        B : float; default=5\n            The distribution has support (-B, B) along each dimension.\n        \"\"\"\n        self.input_dim = input_dim\n        self.B = B\n\n        # save dist info\n        self._params = tuple([(0.0, 0.0) for i in range(input_dim)])\n        self.info = (\"CentBeta13\", (input_dim, B))\n        self.a = 13\n        self.b = 13\n\n    def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Empty pytree -- this distribution doesn't have learnable parameters.\n            This parameter is present to ensure a consistent interface.\n        inputs : jnp.ndarray\n            Input data for which log probability density is calculated.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0],).\n        \"\"\"\n        log_prob = jnp.hstack(\n            [\n                beta.logpdf(\n                    inputs[:, i],\n                    a=self.a,\n                    b=self.b,\n                    loc=-self.B,\n                    scale=2 * self.B,\n                ).reshape(-1, 1)\n                for i in range(self.input_dim)\n            ]\n        ).sum(axis=1)\n\n        return log_prob\n\n    def sample(\n        self, params: Pytree, nsamples: int, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Returns samples from the distribution.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Empty pytree -- this distribution doesn't have learnable parameters.\n            This parameter is present to ensure a consistent interface.\n        nsamples : int\n            The number of samples to be returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (nsamples, self.input_dim).\n        \"\"\"\n        seed = np.random.randint(1e18) if seed is None else seed\n        seeds = random.split(random.PRNGKey(seed), self.input_dim)\n        samples = jnp.hstack(\n            [\n                random.beta(\n                    seeds[i],\n                    self.a,\n                    self.b,\n                    shape=(nsamples, 1),\n                )\n                for i in range(self.input_dim)\n            ]\n        )\n        return 2 * self.B * (samples - 0.5)\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.CentBeta13.__init__","title":"<code>__init__(input_dim, B=5)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimension of the distribution.</p> required <code>B</code> <code>float; default=5</code> <p>The distribution has support (-B, B) along each dimension.</p> <code>5</code> Source code in <code>pzflow/distributions.py</code> <pre><code>def __init__(self, input_dim: int, B: float = 5) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    input_dim : int\n        The dimension of the distribution.\n    B : float; default=5\n        The distribution has support (-B, B) along each dimension.\n    \"\"\"\n    self.input_dim = input_dim\n    self.B = B\n\n    # save dist info\n    self._params = tuple([(0.0, 0.0) for i in range(input_dim)])\n    self.info = (\"CentBeta13\", (input_dim, B))\n    self.a = 13\n    self.b = 13\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.CentBeta13.log_prob","title":"<code>log_prob(params, inputs)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Empty pytree -- this distribution doesn't have learnable parameters. This parameter is present to ensure a consistent interface.</p> required <code>inputs</code> <code>ndarray</code> <p>Input data for which log probability density is calculated.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0],).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Empty pytree -- this distribution doesn't have learnable parameters.\n        This parameter is present to ensure a consistent interface.\n    inputs : jnp.ndarray\n        Input data for which log probability density is calculated.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0],).\n    \"\"\"\n    log_prob = jnp.hstack(\n        [\n            beta.logpdf(\n                inputs[:, i],\n                a=self.a,\n                b=self.b,\n                loc=-self.B,\n                scale=2 * self.B,\n            ).reshape(-1, 1)\n            for i in range(self.input_dim)\n        ]\n    ).sum(axis=1)\n\n    return log_prob\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.CentBeta13.sample","title":"<code>sample(params, nsamples, seed=None)</code>","text":"<p>Returns samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Empty pytree -- this distribution doesn't have learnable parameters. This parameter is present to ensure a consistent interface.</p> required <code>nsamples</code> <code>int</code> <p>The number of samples to be returned.</p> required <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (nsamples, self.input_dim).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def sample(\n    self, params: Pytree, nsamples: int, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Returns samples from the distribution.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Empty pytree -- this distribution doesn't have learnable parameters.\n        This parameter is present to ensure a consistent interface.\n    nsamples : int\n        The number of samples to be returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (nsamples, self.input_dim).\n    \"\"\"\n    seed = np.random.randint(1e18) if seed is None else seed\n    seeds = random.split(random.PRNGKey(seed), self.input_dim)\n    samples = jnp.hstack(\n        [\n            random.beta(\n                seeds[i],\n                self.a,\n                self.b,\n                shape=(nsamples, 1),\n            )\n            for i in range(self.input_dim)\n        ]\n    )\n    return 2 * self.B * (samples - 0.5)\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Joint","title":"<code>Joint</code>","text":"<p>               Bases: <code>LatentDist</code></p> <p>A joint distribution built from other distributions.</p> <p>Note that each of the other distributions already have support for multiple dimensions. This is only useful if you want to combine different distributions for different dimensions, e.g. if your first dimension has a Uniform latent space and the second dimension has a CentBeta latent space.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>class Joint(LatentDist):\n    \"\"\"A joint distribution built from other distributions.\n\n    Note that each of the other distributions already have support for\n    multiple dimensions. This is only useful if you want to combine\n    different distributions for different dimensions, e.g. if your first\n    dimension has a Uniform latent space and the second dimension has a\n    CentBeta latent space.\n    \"\"\"\n\n    def __init__(self, *inputs: Union[LatentDist, tuple]) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        inputs: LatentDist or tuple\n            The latent distributions to join together.\n        \"\"\"\n\n        # if Joint info provided, use that for setup\n        if inputs[0] == \"Joint info\":\n            self.dists = [globals()[dist[0]](*dist[1]) for dist in inputs[1]]\n        # otherwise, assume it's a list of distributions\n        else:\n            self.dists = inputs\n\n        # save info\n        self._params = [dist._params for dist in self.dists]\n        self.input_dim = sum([dist.input_dim for dist in self.dists])\n        self.info = (\n            \"Joint\",\n            (\"Joint info\", [dist.info for dist in self.dists]),\n        )\n\n        # save the indices at which inputs will be split for log_prob\n        # they must be concretely saved ahead-of-time so that jax trace\n        # works properly when jitting\n        self._splits = jnp.cumsum(\n            jnp.array([dist.input_dim for dist in self.dists])\n        )[:-1]\n\n    def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Parameters\n        ----------\n        params : Jax Pytree\n            Parameters for the distributions.\n        inputs : jnp.ndarray\n            Input data for which log probability density is calculated.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0],).\n        \"\"\"\n\n        # split inputs for corresponding distribution\n        inputs = jnp.split(inputs, self._splits, axis=1)\n\n        # calculate log_prob with respect to each sub-distribution,\n        # then sum all the log_probs for each input\n        log_prob = jnp.hstack(\n            [\n                self.dists[i].log_prob(params[i], inputs[i]).reshape(-1, 1)\n                for i in range(len(self.dists))\n            ]\n        ).sum(axis=1)\n\n        return log_prob\n\n    def sample(\n        self, params: Pytree, nsamples: int, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Returns samples from the distribution.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Parameters for the distributions.\n        nsamples : int\n            The number of samples to be returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (nsamples, self.input_dim).\n        \"\"\"\n\n        seed = np.random.randint(1e18) if seed is None else seed\n        seeds = random.randint(\n            random.PRNGKey(seed), (len(self.dists),), 0, int(1e9)\n        )\n        samples = jnp.hstack(\n            [\n                self.dists[i]\n                .sample(params[i], nsamples, seeds[i])\n                .reshape(nsamples, -1)\n                for i in range(len(self.dists))\n            ]\n        )\n\n        return samples\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Joint.__init__","title":"<code>__init__(*inputs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[LatentDist, tuple]</code> <p>The latent distributions to join together.</p> <code>()</code> Source code in <code>pzflow/distributions.py</code> <pre><code>def __init__(self, *inputs: Union[LatentDist, tuple]) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    inputs: LatentDist or tuple\n        The latent distributions to join together.\n    \"\"\"\n\n    # if Joint info provided, use that for setup\n    if inputs[0] == \"Joint info\":\n        self.dists = [globals()[dist[0]](*dist[1]) for dist in inputs[1]]\n    # otherwise, assume it's a list of distributions\n    else:\n        self.dists = inputs\n\n    # save info\n    self._params = [dist._params for dist in self.dists]\n    self.input_dim = sum([dist.input_dim for dist in self.dists])\n    self.info = (\n        \"Joint\",\n        (\"Joint info\", [dist.info for dist in self.dists]),\n    )\n\n    # save the indices at which inputs will be split for log_prob\n    # they must be concretely saved ahead-of-time so that jax trace\n    # works properly when jitting\n    self._splits = jnp.cumsum(\n        jnp.array([dist.input_dim for dist in self.dists])\n    )[:-1]\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Joint.log_prob","title":"<code>log_prob(params, inputs)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Jax Pytree</code> <p>Parameters for the distributions.</p> required <code>inputs</code> <code>ndarray</code> <p>Input data for which log probability density is calculated.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0],).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Parameters\n    ----------\n    params : Jax Pytree\n        Parameters for the distributions.\n    inputs : jnp.ndarray\n        Input data for which log probability density is calculated.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0],).\n    \"\"\"\n\n    # split inputs for corresponding distribution\n    inputs = jnp.split(inputs, self._splits, axis=1)\n\n    # calculate log_prob with respect to each sub-distribution,\n    # then sum all the log_probs for each input\n    log_prob = jnp.hstack(\n        [\n            self.dists[i].log_prob(params[i], inputs[i]).reshape(-1, 1)\n            for i in range(len(self.dists))\n        ]\n    ).sum(axis=1)\n\n    return log_prob\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Joint.sample","title":"<code>sample(params, nsamples, seed=None)</code>","text":"<p>Returns samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Parameters for the distributions.</p> required <code>nsamples</code> <code>int</code> <p>The number of samples to be returned.</p> required <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (nsamples, self.input_dim).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def sample(\n    self, params: Pytree, nsamples: int, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Returns samples from the distribution.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Parameters for the distributions.\n    nsamples : int\n        The number of samples to be returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (nsamples, self.input_dim).\n    \"\"\"\n\n    seed = np.random.randint(1e18) if seed is None else seed\n    seeds = random.randint(\n        random.PRNGKey(seed), (len(self.dists),), 0, int(1e9)\n    )\n    samples = jnp.hstack(\n        [\n            self.dists[i]\n            .sample(params[i], nsamples, seeds[i])\n            .reshape(nsamples, -1)\n            for i in range(len(self.dists))\n        ]\n    )\n\n    return samples\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.LatentDist","title":"<code>LatentDist</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for latent distributions.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>class LatentDist(ABC):\n    \"\"\"Base class for latent distributions.\"\"\"\n\n    info = (\"LatentDist\", ())\n\n    @abstractmethod\n    def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Calculate log-probability of the inputs.\"\"\"\n\n    @abstractmethod\n    def sample(\n        self, params: Pytree, nsamples: int, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Sample from the distribution.\"\"\"\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.LatentDist.log_prob","title":"<code>log_prob(params, inputs)</code>  <code>abstractmethod</code>","text":"<p>Calculate log-probability of the inputs.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>@abstractmethod\ndef log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculate log-probability of the inputs.\"\"\"\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.LatentDist.sample","title":"<code>sample(params, nsamples, seed=None)</code>  <code>abstractmethod</code>","text":"<p>Sample from the distribution.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>@abstractmethod\ndef sample(\n    self, params: Pytree, nsamples: int, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Sample from the distribution.\"\"\"\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Normal","title":"<code>Normal</code>","text":"<p>               Bases: <code>LatentDist</code></p> <p>A multivariate Gaussian distribution with mean zero and unit variance.</p> <p>Note this distribution has infinite support, so it is not recommended that you use it with the spline coupling layers, which have compact support. If you do use the two together, you should set the support of the spline layers (using the spline parameter B) to be large enough that you rarely draw Gaussian samples outside the support of the splines.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>class Normal(LatentDist):\n    \"\"\"A multivariate Gaussian distribution with mean zero and unit variance.\n\n    Note this distribution has infinite support, so it is not recommended that\n    you use it with the spline coupling layers, which have compact support.\n    If you do use the two together, you should set the support of the spline\n    layers (using the spline parameter B) to be large enough that you rarely\n    draw Gaussian samples outside the support of the splines.\n    \"\"\"\n\n    def __init__(self, input_dim: int) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        input_dim : int\n            The dimension of the distribution.\n        \"\"\"\n        self.input_dim = input_dim\n\n        # save dist info\n        self._params = ()\n        self.info = (\"Normal\", (input_dim,))\n\n    def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Empty pytree -- this distribution doesn't have learnable parameters.\n            This parameter is present to ensure a consistent interface.\n        inputs : jnp.ndarray\n            Input data for which log probability density is calculated.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0],).\n        \"\"\"\n        return multivariate_normal.logpdf(\n            inputs,\n            mean=jnp.zeros(self.input_dim),\n            cov=jnp.identity(self.input_dim),\n        )\n\n    def sample(\n        self, params: Pytree, nsamples: int, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Returns samples from the distribution.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Empty pytree -- this distribution doesn't have learnable parameters.\n            This parameter is present to ensure a consistent interface.\n        nsamples : int\n            The number of samples to be returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (nsamples, self.input_dim).\n        \"\"\"\n        seed = np.random.randint(1e18) if seed is None else seed\n        return random.multivariate_normal(\n            key=random.PRNGKey(seed),\n            mean=jnp.zeros(self.input_dim),\n            cov=jnp.identity(self.input_dim),\n            shape=(nsamples,),\n        )\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Normal.__init__","title":"<code>__init__(input_dim)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimension of the distribution.</p> required Source code in <code>pzflow/distributions.py</code> <pre><code>def __init__(self, input_dim: int) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    input_dim : int\n        The dimension of the distribution.\n    \"\"\"\n    self.input_dim = input_dim\n\n    # save dist info\n    self._params = ()\n    self.info = (\"Normal\", (input_dim,))\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Normal.log_prob","title":"<code>log_prob(params, inputs)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Empty pytree -- this distribution doesn't have learnable parameters. This parameter is present to ensure a consistent interface.</p> required <code>inputs</code> <code>ndarray</code> <p>Input data for which log probability density is calculated.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0],).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Empty pytree -- this distribution doesn't have learnable parameters.\n        This parameter is present to ensure a consistent interface.\n    inputs : jnp.ndarray\n        Input data for which log probability density is calculated.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0],).\n    \"\"\"\n    return multivariate_normal.logpdf(\n        inputs,\n        mean=jnp.zeros(self.input_dim),\n        cov=jnp.identity(self.input_dim),\n    )\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Normal.sample","title":"<code>sample(params, nsamples, seed=None)</code>","text":"<p>Returns samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Empty pytree -- this distribution doesn't have learnable parameters. This parameter is present to ensure a consistent interface.</p> required <code>nsamples</code> <code>int</code> <p>The number of samples to be returned.</p> required <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (nsamples, self.input_dim).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def sample(\n    self, params: Pytree, nsamples: int, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Returns samples from the distribution.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Empty pytree -- this distribution doesn't have learnable parameters.\n        This parameter is present to ensure a consistent interface.\n    nsamples : int\n        The number of samples to be returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (nsamples, self.input_dim).\n    \"\"\"\n    seed = np.random.randint(1e18) if seed is None else seed\n    return random.multivariate_normal(\n        key=random.PRNGKey(seed),\n        mean=jnp.zeros(self.input_dim),\n        cov=jnp.identity(self.input_dim),\n        shape=(nsamples,),\n    )\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Tdist","title":"<code>Tdist</code>","text":"<p>               Bases: <code>LatentDist</code></p> <p>A multivariate T distribution with mean zero and unit scale matrix.</p> <p>The number of degrees of freedom (i.e. the weight of the tails) is learned during training.</p> <p>Note this distribution has infinite support and potentially large tails, so it is not recommended to use this distribution with the spline coupling layers, which have compact support.</p> Source code in <code>pzflow/distributions.py</code> <pre><code>class Tdist(LatentDist):\n    \"\"\"A multivariate T distribution with mean zero and unit scale matrix.\n\n    The number of degrees of freedom (i.e. the weight of the tails) is learned\n    during training.\n\n    Note this distribution has infinite support and potentially large tails,\n    so it is not recommended to use this distribution with the spline coupling\n    layers, which have compact support.\n    \"\"\"\n\n    def __init__(self, input_dim: int) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        input_dim : int\n            The dimension of the distribution.\n        \"\"\"\n        self.input_dim = input_dim\n\n        # save dist info\n        self._params = jnp.log(30.0)\n        self.info = (\"Tdist\", (input_dim,))\n\n    def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Uses method explained here:\n        http://gregorygundersen.com/blog/2020/01/20/multivariate-t/\n\n        Parameters\n        ----------\n        params : float\n            The degrees of freedom (nu) of the t-distribution.\n        inputs : jnp.ndarray\n            Input data for which log probability density is calculated.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0],).\n        \"\"\"\n        cov = jnp.identity(self.input_dim)\n        nu = jnp.exp(params)\n        maha, log_det = _mahalanobis_and_logdet(inputs, cov)\n        t = 0.5 * (nu + self.input_dim)\n        A = gammaln(t)\n        B = gammaln(0.5 * nu)\n        C = self.input_dim / 2.0 * jnp.log(nu * jnp.pi)\n        D = 0.5 * log_det\n        E = -t * jnp.log(1 + (1.0 / nu) * maha)\n\n        return A - B - C - D + E\n\n    def sample(\n        self, params: Pytree, nsamples: int, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Returns samples from the distribution.\n\n        Parameters\n        ----------\n        params : float\n            The degrees of freedom (nu) of the t-distribution.\n        nsamples : int\n            The number of samples to be returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (nsamples, self.input_dim).\n        \"\"\"\n        mean = jnp.zeros(self.input_dim)\n        nu = jnp.exp(params)\n\n        seed = np.random.randint(1e18) if seed is None else seed\n        rng = np.random.default_rng(int(seed))\n        x = jnp.array(rng.chisquare(nu, nsamples) / nu)\n        z = random.multivariate_normal(\n            key=random.PRNGKey(seed),\n            mean=jnp.zeros(self.input_dim),\n            cov=jnp.identity(self.input_dim),\n            shape=(nsamples,),\n        )\n        samples = mean + z / jnp.sqrt(x)[:, None]\n        return samples\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Tdist.__init__","title":"<code>__init__(input_dim)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimension of the distribution.</p> required Source code in <code>pzflow/distributions.py</code> <pre><code>def __init__(self, input_dim: int) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    input_dim : int\n        The dimension of the distribution.\n    \"\"\"\n    self.input_dim = input_dim\n\n    # save dist info\n    self._params = jnp.log(30.0)\n    self.info = (\"Tdist\", (input_dim,))\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Tdist.log_prob","title":"<code>log_prob(params, inputs)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Uses method explained here: http://gregorygundersen.com/blog/2020/01/20/multivariate-t/</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>float</code> <p>The degrees of freedom (nu) of the t-distribution.</p> required <code>inputs</code> <code>ndarray</code> <p>Input data for which log probability density is calculated.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0],).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Uses method explained here:\n    http://gregorygundersen.com/blog/2020/01/20/multivariate-t/\n\n    Parameters\n    ----------\n    params : float\n        The degrees of freedom (nu) of the t-distribution.\n    inputs : jnp.ndarray\n        Input data for which log probability density is calculated.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0],).\n    \"\"\"\n    cov = jnp.identity(self.input_dim)\n    nu = jnp.exp(params)\n    maha, log_det = _mahalanobis_and_logdet(inputs, cov)\n    t = 0.5 * (nu + self.input_dim)\n    A = gammaln(t)\n    B = gammaln(0.5 * nu)\n    C = self.input_dim / 2.0 * jnp.log(nu * jnp.pi)\n    D = 0.5 * log_det\n    E = -t * jnp.log(1 + (1.0 / nu) * maha)\n\n    return A - B - C - D + E\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Tdist.sample","title":"<code>sample(params, nsamples, seed=None)</code>","text":"<p>Returns samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>float</code> <p>The degrees of freedom (nu) of the t-distribution.</p> required <code>nsamples</code> <code>int</code> <p>The number of samples to be returned.</p> required <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (nsamples, self.input_dim).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def sample(\n    self, params: Pytree, nsamples: int, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Returns samples from the distribution.\n\n    Parameters\n    ----------\n    params : float\n        The degrees of freedom (nu) of the t-distribution.\n    nsamples : int\n        The number of samples to be returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (nsamples, self.input_dim).\n    \"\"\"\n    mean = jnp.zeros(self.input_dim)\n    nu = jnp.exp(params)\n\n    seed = np.random.randint(1e18) if seed is None else seed\n    rng = np.random.default_rng(int(seed))\n    x = jnp.array(rng.chisquare(nu, nsamples) / nu)\n    z = random.multivariate_normal(\n        key=random.PRNGKey(seed),\n        mean=jnp.zeros(self.input_dim),\n        cov=jnp.identity(self.input_dim),\n        shape=(nsamples,),\n    )\n    samples = mean + z / jnp.sqrt(x)[:, None]\n    return samples\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Uniform","title":"<code>Uniform</code>","text":"<p>               Bases: <code>LatentDist</code></p> <p>A multivariate uniform distribution with support [-B, B].</p> Source code in <code>pzflow/distributions.py</code> <pre><code>class Uniform(LatentDist):\n    \"\"\"A multivariate uniform distribution with support [-B, B].\"\"\"\n\n    def __init__(self, input_dim: int, B: float = 5) -&gt; None:\n        \"\"\"\n        Parameters\n        ----------\n        input_dim : int\n            The dimension of the distribution.\n        B : float; default=5\n            The distribution has support (-B, B) along each dimension.\n        \"\"\"\n        self.input_dim = input_dim\n        self.B = B\n\n        # save dist info\n        self._params = ()\n        self.info = (\"Uniform\", (input_dim, B))\n\n    def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Parameters\n        ----------\n        params : Jax Pytree\n            Empty pytree -- this distribution doesn't have learnable parameters.\n            This parameter is present to ensure a consistent interface.\n        inputs : jnp.ndarray\n            Input data for which log probability density is calculated.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0],).\n        \"\"\"\n\n        # which inputs are inside the support of the distribution\n        mask = jnp.prod((inputs &gt;= -self.B) &amp; (inputs &lt;= self.B), axis=-1)\n\n        # calculate log_prob\n        log_prob = jnp.where(\n            mask,\n            -self.input_dim * jnp.log(2 * self.B),\n            -jnp.inf,\n        )\n\n        return log_prob\n\n    def sample(\n        self, params: Pytree, nsamples: int, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Returns samples from the distribution.\n\n        Parameters\n        ----------\n        params : a Jax pytree\n            Empty pytree -- this distribution doesn't have learnable parameters.\n            This parameter is present to ensure a consistent interface.\n        nsamples : int\n            The number of samples to be returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (nsamples, self.input_dim).\n        \"\"\"\n        seed = np.random.randint(1e18) if seed is None else seed\n        samples = random.uniform(\n            random.PRNGKey(seed),\n            shape=(nsamples, self.input_dim),\n            minval=-self.B,\n            maxval=self.B,\n        )\n        return jnp.array(samples)\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Uniform.__init__","title":"<code>__init__(input_dim, B=5)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The dimension of the distribution.</p> required <code>B</code> <code>float; default=5</code> <p>The distribution has support (-B, B) along each dimension.</p> <code>5</code> Source code in <code>pzflow/distributions.py</code> <pre><code>def __init__(self, input_dim: int, B: float = 5) -&gt; None:\n    \"\"\"\n    Parameters\n    ----------\n    input_dim : int\n        The dimension of the distribution.\n    B : float; default=5\n        The distribution has support (-B, B) along each dimension.\n    \"\"\"\n    self.input_dim = input_dim\n    self.B = B\n\n    # save dist info\n    self._params = ()\n    self.info = (\"Uniform\", (input_dim, B))\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Uniform.log_prob","title":"<code>log_prob(params, inputs)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Jax Pytree</code> <p>Empty pytree -- this distribution doesn't have learnable parameters. This parameter is present to ensure a consistent interface.</p> required <code>inputs</code> <code>ndarray</code> <p>Input data for which log probability density is calculated.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0],).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def log_prob(self, params: Pytree, inputs: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Parameters\n    ----------\n    params : Jax Pytree\n        Empty pytree -- this distribution doesn't have learnable parameters.\n        This parameter is present to ensure a consistent interface.\n    inputs : jnp.ndarray\n        Input data for which log probability density is calculated.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0],).\n    \"\"\"\n\n    # which inputs are inside the support of the distribution\n    mask = jnp.prod((inputs &gt;= -self.B) &amp; (inputs &lt;= self.B), axis=-1)\n\n    # calculate log_prob\n    log_prob = jnp.where(\n        mask,\n        -self.input_dim * jnp.log(2 * self.B),\n        -jnp.inf,\n    )\n\n    return log_prob\n</code></pre>"},{"location":"API/distributions/#pzflow.distributions.Uniform.sample","title":"<code>sample(params, nsamples, seed=None)</code>","text":"<p>Returns samples from the distribution.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>a Jax pytree</code> <p>Empty pytree -- this distribution doesn't have learnable parameters. This parameter is present to ensure a consistent interface.</p> required <code>nsamples</code> <code>int</code> <p>The number of samples to be returned.</p> required <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (nsamples, self.input_dim).</p> Source code in <code>pzflow/distributions.py</code> <pre><code>def sample(\n    self, params: Pytree, nsamples: int, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Returns samples from the distribution.\n\n    Parameters\n    ----------\n    params : a Jax pytree\n        Empty pytree -- this distribution doesn't have learnable parameters.\n        This parameter is present to ensure a consistent interface.\n    nsamples : int\n        The number of samples to be returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (nsamples, self.input_dim).\n    \"\"\"\n    seed = np.random.randint(1e18) if seed is None else seed\n    samples = random.uniform(\n        random.PRNGKey(seed),\n        shape=(nsamples, self.input_dim),\n        minval=-self.B,\n        maxval=self.B,\n    )\n    return jnp.array(samples)\n</code></pre>"},{"location":"API/examples/","title":"Examples","text":"<p>Functions that return example data and a example flow trained on galaxy data. To see these examples in action, see the tutorial notebooks.</p>"},{"location":"API/examples/#pzflow.examples.get_checkerboard_data","title":"<code>get_checkerboard_data()</code>","text":"<p>Return DataFrame with discrete checkerboard data.</p> Source code in <code>pzflow/examples.py</code> <pre><code>def get_checkerboard_data() -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with discrete checkerboard data.\"\"\"\n    return _load_example_data(\"checkerboard-data\")\n</code></pre>"},{"location":"API/examples/#pzflow.examples.get_city_data","title":"<code>get_city_data()</code>","text":"<p>Return DataFrame with example city data.</p> <p>The countries, names, population, and coordinates of 47,966 cities.</p> <p>Subset of the Kaggle world cities database. https://www.kaggle.com/max-mind/world-cities-database This database was downloaded from MaxMind. The license follows:</p> <pre><code>OPEN DATA LICENSE for MaxMind WorldCities and Postal Code Databases\n\nCopyright (c) 2008 MaxMind Inc.  All Rights Reserved.\n\nThe database uses toponymic information, based on the Geographic Names\nData Base, containing official standard names approved by the United States\nBoard on Geographic Names and maintained by the National\nGeospatial-Intelligence Agency. More information is available at the Maps\nand Geodata link at www.nga.mil. The National Geospatial-Intelligence Agency\nname, initials, and seal are protected by 10 United States Code Section 445.\n\nIt also uses free population data from Stefan Helders www.world-gazetteer.com.\nVisit his website to download the free population data.  Our database\ncombines Stefan's population data with the list of all cities in the world.\n\nAll advertising materials and documentation mentioning features or use of\nthis database must display the following acknowledgment:\n\"This product includes data created by MaxMind, available from\nhttp://www.maxmind.com/\"\n\nRedistribution and use with or without modification, are permitted provided\nthat the following conditions are met:\n1. Redistributions must retain the above copyright notice, this list of\nconditions and the following disclaimer in the documentation and/or other\nmaterials provided with the distribution.\n2. All advertising materials and documentation mentioning features or use of\nthis database must display the following acknowledgement:\n\"This product includes data created by MaxMind, available from\nhttp://www.maxmind.com/\"\n3. \"MaxMind\" may not be used to endorse or promote products derived from this\ndatabase without specific prior written permission.\n\nTHIS DATABASE IS PROVIDED BY MAXMIND.COM ``AS IS'' AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL MAXMIND.COM BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nDATABASE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre> Source code in <code>pzflow/examples.py</code> <pre><code>def get_city_data() -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with example city data.\n\n    The countries, names, population, and coordinates of 47,966 cities.\n\n    Subset of the Kaggle world cities database.\n    https://www.kaggle.com/max-mind/world-cities-database\n    This database was downloaded from MaxMind. The license follows:\n\n        OPEN DATA LICENSE for MaxMind WorldCities and Postal Code Databases\n\n        Copyright (c) 2008 MaxMind Inc.  All Rights Reserved.\n\n        The database uses toponymic information, based on the Geographic Names\n        Data Base, containing official standard names approved by the United States\n        Board on Geographic Names and maintained by the National\n        Geospatial-Intelligence Agency. More information is available at the Maps\n        and Geodata link at www.nga.mil. The National Geospatial-Intelligence Agency\n        name, initials, and seal are protected by 10 United States Code Section 445.\n\n        It also uses free population data from Stefan Helders www.world-gazetteer.com.\n        Visit his website to download the free population data.  Our database\n        combines Stefan's population data with the list of all cities in the world.\n\n        All advertising materials and documentation mentioning features or use of\n        this database must display the following acknowledgment:\n        \"This product includes data created by MaxMind, available from\n        http://www.maxmind.com/\"\n\n        Redistribution and use with or without modification, are permitted provided\n        that the following conditions are met:\n        1. Redistributions must retain the above copyright notice, this list of\n        conditions and the following disclaimer in the documentation and/or other\n        materials provided with the distribution.\n        2. All advertising materials and documentation mentioning features or use of\n        this database must display the following acknowledgement:\n        \"This product includes data created by MaxMind, available from\n        http://www.maxmind.com/\"\n        3. \"MaxMind\" may not be used to endorse or promote products derived from this\n        database without specific prior written permission.\n\n        THIS DATABASE IS PROVIDED BY MAXMIND.COM ``AS IS'' AND ANY\n        EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n        WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n        DISCLAIMED. IN NO EVENT SHALL MAXMIND.COM BE LIABLE FOR ANY\n        DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n        (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n        LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n        ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n        (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n        DATABASE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n    \"\"\"\n    return _load_example_data(\"city-data\")\n</code></pre>"},{"location":"API/examples/#pzflow.examples.get_example_flow","title":"<code>get_example_flow()</code>","text":"<p>Return a normalizing flow that was trained on galaxy data.</p> <p>This flow was trained in the <code>redshift_example.ipynb</code> Jupyter notebook, on the example data available in <code>pzflow.examples.galaxy_data</code>. For more info: <code>print(example_flow().info)</code>.</p> Source code in <code>pzflow/examples.py</code> <pre><code>def get_example_flow() -&gt; Flow:\n    \"\"\"Return a normalizing flow that was trained on galaxy data.\n\n    This flow was trained in the `redshift_example.ipynb` Jupyter notebook,\n    on the example data available in `pzflow.examples.galaxy_data`.\n    For more info: `print(example_flow().info)`.\n    \"\"\"\n    this_dir, _ = os.path.split(__file__)\n    flow_path = os.path.join(\n        this_dir, f\"{EXAMPLE_FILE_DIR}/example-flow.pzflow.pkl\"\n    )\n    flow = Flow(file=flow_path)\n    return flow\n</code></pre>"},{"location":"API/examples/#pzflow.examples.get_galaxy_data","title":"<code>get_galaxy_data()</code>","text":"<p>Return DataFrame with example galaxy data.</p> <p>100,000 galaxies from the Buzzard simulation [1], with redshifts in the range (0,2.3) and photometry in the LSST ugrizy bands.</p> References <p>[1] Joseph DeRose et al. The Buzzard Flock: Dark Energy Survey Synthetic Sky Catalogs. arXiv:1901.02401, 2019. https://arxiv.org/abs/1901.02401</p> Source code in <code>pzflow/examples.py</code> <pre><code>def get_galaxy_data() -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with example galaxy data.\n\n    100,000 galaxies from the Buzzard simulation [1], with redshifts\n    in the range (0,2.3) and photometry in the LSST ugrizy bands.\n\n    References\n    ----------\n    [1] Joseph DeRose et al. The Buzzard Flock: Dark Energy Survey\n    Synthetic Sky Catalogs. arXiv:1901.02401, 2019.\n    https://arxiv.org/abs/1901.02401\n    \"\"\"\n    return _load_example_data(\"galaxy-data\")\n</code></pre>"},{"location":"API/examples/#pzflow.examples.get_twomoons_data","title":"<code>get_twomoons_data()</code>","text":"<p>Return DataFrame with two moons example data.</p> <p>Two moons data originally from scikit-learn, i.e., <code>sklearn.datasets.make_moons</code>.</p> Source code in <code>pzflow/examples.py</code> <pre><code>def get_twomoons_data() -&gt; pd.DataFrame:\n    \"\"\"Return DataFrame with two moons example data.\n\n    Two moons data originally from scikit-learn,\n    i.e., `sklearn.datasets.make_moons`.\n    \"\"\"\n    return _load_example_data(\"two-moons-data\")\n</code></pre>"},{"location":"API/flow/","title":"Flow","text":"<p>Define the Flow object that defines the normalizing flow.</p>"},{"location":"API/flow/#pzflow.flow.Flow","title":"<code>Flow</code>","text":"<p>A normalizing flow that models tabular data.</p> <p>Attributes:</p> Name Type Description <code>data_columns</code> <code>tuple</code> <p>List of DataFrame columns that the flow expects/produces.</p> <code>conditional_columns</code> <code>tuple</code> <p>List of DataFrame columns on which the flow is conditioned.</p> <code>latent</code> <code>LatentDist</code> <p>The latent distribution of the normalizing flow. Has it's own sample and log_prob methods.</p> <code>data_error_model</code> <code>Callable</code> <p>The error model for the data variables. See the docstring of init for more details.</p> <code>condition_error_model</code> <code>Callable</code> <p>The error model for the conditional variables. See the docstring of init for more details.</p> <code>info</code> <code>Any</code> <p>Object containing any kind of info included with the flow. Often describes the data the flow is trained on.</p> Source code in <code>pzflow/flow.py</code> <pre><code>class Flow:\n    \"\"\"A normalizing flow that models tabular data.\n\n    Attributes\n    ----------\n    data_columns : tuple\n        List of DataFrame columns that the flow expects/produces.\n    conditional_columns : tuple\n        List of DataFrame columns on which the flow is conditioned.\n    latent : distributions.LatentDist\n        The latent distribution of the normalizing flow.\n        Has it's own sample and log_prob methods.\n    data_error_model : Callable\n        The error model for the data variables. See the docstring of\n        __init__ for more details.\n    condition_error_model : Callable\n        The error model for the conditional variables. See the docstring\n        of __init__ for more details.\n    info : Any\n        Object containing any kind of info included with the flow.\n        Often describes the data the flow is trained on.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_columns: Sequence[str] = None,\n        bijector: Tuple[InitFunction, Bijector_Info] = None,\n        latent: distributions.LatentDist = None,\n        conditional_columns: Sequence[str] = None,\n        data_error_model: Callable = None,\n        condition_error_model: Callable = None,\n        autoscale_conditions: bool = True,\n        seed: int = 0,\n        info: Any = None,\n        file: str = None,\n        _dictionary: dict = None,\n    ) -&gt; None:\n        \"\"\"Instantiate a normalizing flow.\n\n        Note that while all of the init parameters are technically optional,\n        you must provide either data_columns OR file.\n        In addition, if a file is provided, all other parameters must be None.\n\n        Parameters\n        ----------\n        data_columns : Sequence[str]; optional\n            Tuple, list, or other container of column names.\n            These are the columns the flow expects/produces in DataFrames.\n        bijector : Bijector Call; optional\n            A Bijector call that consists of the bijector InitFunction that\n            initializes the bijector and the tuple of Bijector Info.\n            Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc.\n            If not provided, the bijector can be set later using\n            flow.set_bijector, or by calling flow.train, in which case the\n            default bijector will be used. The default bijector is\n            ShiftBounds -&gt; RollingSplineCoupling, where the range of shift\n            bounds is learned from the training data, and the dimensions of\n            RollingSplineCoupling is inferred. The default bijector assumes\n            that the latent has support [-5, 5] for every dimension.\n        latent : distributions.LatentDist; optional\n            The latent distribution for the normalizing flow. Can be any of\n            the distributions from pzflow.distributions. If not provided,\n            CentBeta13 is used with input_dim = len(data_columns) and B=5.\n        conditional_columns : Sequence[str]; optional\n            Names of columns on which to condition the normalizing flow.\n        data_error_model : Callable; optional\n            A callable that defines the error model for data variables.\n            data_error_model must take key, X, Xerr, nsamples as arguments:\n                - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n                - X is 2D array of data variables, where the order of variables\n                    matches the order of the columns in data_columns\n                - Xerr is the corresponding 2D array of errors\n                - nsamples is number of samples to draw from error distribution\n            data_error_model must return an array of samples with the shape\n            (X.shape[0], nsamples, X.shape[1]).\n            If data_error_model is not provided, Gaussian error model assumed.\n        condition_error_model : Callable; optional\n            A callable that defines the error model for conditional variables.\n            condition_error_model must take key, X, Xerr, nsamples, where:\n                - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n                - X is 2D array of conditional variables, where the order of\n                    variables matches order of columns in conditional_columns\n                - Xerr is the corresponding 2D array of errors\n                - nsamples is number of samples to draw from error distribution\n            condition_error_model must return array of samples with shape\n            (X.shape[0], nsamples, X.shape[1]).\n            If condition_error_model is not provided, Gaussian error model\n            assumed.\n        autoscale_conditions : bool; default=True\n            Sets whether or not conditions are automatically standard scaled\n            when passed to a conditional flow. I recommend you leave as True.\n        seed : int; default=0\n            The random seed for initial parameters\n        info : Any; optional\n            An object to attach to the info attribute.\n        file : str; optional\n            Path to file from which to load a pretrained flow.\n            If a file is provided, all other parameters must be None.\n        \"\"\"\n\n        # validate parameters\n        if data_columns is None and file is None and _dictionary is None:\n            raise ValueError(\"You must provide data_columns OR file.\")\n        if any(\n            (\n                data_columns is not None,\n                bijector is not None,\n                conditional_columns is not None,\n                latent is not None,\n                data_error_model is not None,\n                condition_error_model is not None,\n                info is not None,\n            )\n        ):\n            if file is not None:\n                raise ValueError(\n                    \"If providing a file, please do not provide any other parameters.\"\n                )\n            if _dictionary is not None:\n                raise ValueError(\n                    \"If providing a dictionary, please do not provide any other parameters.\"\n                )\n        if file is not None and _dictionary is not None:\n            raise ValueError(\"Only provide file or _dictionary, not both.\")\n\n        # if file or dictionary is provided, load everything from it\n        if file is not None or _dictionary is not None:\n            save_dict = self._save_dict()\n            if file is not None:\n                with open(file, \"rb\") as handle:\n                    save_dict.update(pickle.load(handle))\n            else:\n                save_dict.update(_dictionary)\n\n            if save_dict[\"class\"] != self.__class__.__name__:\n                raise TypeError(\n                    f\"This save file isn't a {self.__class__.__name__}. \"\n                    f\"It is a {save_dict['class']}\"\n                )\n\n            # load columns and dimensions\n            self.data_columns = save_dict[\"data_columns\"]\n            self.conditional_columns = save_dict[\"conditional_columns\"]\n            self._input_dim = len(self.data_columns)\n            self.info = save_dict[\"info\"]\n\n            # load the latent distribution\n            self._latent_info = save_dict[\"latent_info\"]\n            self.latent = getattr(distributions, self._latent_info[0])(\n                *self._latent_info[1]\n            )\n\n            # load the error models\n            self.data_error_model = save_dict[\"data_error_model\"]\n            self.condition_error_model = save_dict[\"condition_error_model\"]\n\n            # load the bijector\n            self._bijector_info = save_dict[\"bijector_info\"]\n            if self._bijector_info is not None:\n                init_fun, _ = build_bijector_from_info(self._bijector_info)\n                _, self._forward, self._inverse = init_fun(\n                    random.PRNGKey(0), self._input_dim\n                )\n            self._params = save_dict[\"params\"]\n\n            # load the conditional means and stds\n            self._condition_means = save_dict[\"condition_means\"]\n            self._condition_stds = save_dict[\"condition_stds\"]\n\n            # set whether or not to automatically standard scale any\n            # conditions passed to the normalizing flow\n            self._autoscale_conditions = save_dict[\"autoscale_conditions\"]\n\n        # if no file is provided, use provided parameters\n        else:\n            self.data_columns = tuple(data_columns)\n            self._input_dim = len(self.data_columns)\n            self.info = info\n\n            if conditional_columns is None:\n                self.conditional_columns = None\n                self._condition_means = None\n                self._condition_stds = None\n            else:\n                self.conditional_columns = tuple(conditional_columns)\n                self._condition_means = jnp.zeros(\n                    len(self.conditional_columns)\n                )\n                self._condition_stds = jnp.ones(len(self.conditional_columns))\n\n            # set whether or not to automatically standard scale any\n            # conditions passed to the normalizing flow\n            self._autoscale_conditions = autoscale_conditions\n\n            # set up the latent distribution\n            if latent is None:\n                self.latent = distributions.CentBeta13(self._input_dim, 5)\n            else:\n                self.latent = latent\n            self._latent_info = self.latent.info\n\n            # make sure the latent distribution and data_columns have the\n            # same number of dimensions\n            if self.latent.input_dim != len(data_columns):\n                raise ValueError(\n                    f\"The latent distribution has {self.latent.input_dim} \"\n                    f\"dimensions, but data_columns has {len(data_columns)} \"\n                    \"dimensions. They must match!\"\n                )\n\n            # set up the error models\n            if data_error_model is None:\n                self.data_error_model = gaussian_error_model\n            else:\n                self.data_error_model = data_error_model\n            if condition_error_model is None:\n                self.condition_error_model = gaussian_error_model\n            else:\n                self.condition_error_model = condition_error_model\n\n            # set up the bijector\n            if bijector is not None:\n                self.set_bijector(bijector, seed=seed)\n            # if no bijector was provided, set bijector_info to None\n            else:\n                self._bijector_info = None\n\n    def _check_bijector(self) -&gt; None:\n        if self._bijector_info is None:\n            raise ValueError(\n                \"The bijector has not been set up yet! \"\n                \"You can do this by calling \"\n                \"flow.set_bijector(bijector, params), \"\n                \"or by calling train, in which case the default \"\n                \"bijector will be used.\"\n            )\n\n    def set_bijector(\n        self,\n        bijector: Tuple[InitFunction, Bijector_Info],\n        params: Pytree = None,\n        seed: int = 0,\n    ) -&gt; None:\n        \"\"\"Set the bijector.\n\n        Parameters\n        ----------\n        bijector : Bijector Call\n            A Bijector call that consists of the bijector InitFunction that\n            initializes the bijector and the tuple of Bijector Info.\n            Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc.\n        params : Pytree; optional\n            A Pytree of bijector parameters. If not provided, the bijector\n            will be initialized with random parameters.\n        seed: int; default=0\n            A random seed for initializing the bijector with random parameters.\n        \"\"\"\n\n        # set up the bijector\n        init_fun, self._bijector_info = bijector\n        bijector_params, self._forward, self._inverse = init_fun(\n            random.PRNGKey(seed), self._input_dim\n        )\n\n        # check if params were passed\n        bijector_params = params if params is not None else bijector_params\n\n        # save the bijector params along with the latent params\n        self._params = (self.latent._params, bijector_params)\n\n    def _set_default_bijector(\n        self, inputs: pd.DataFrame, seed: int = 0\n    ) -&gt; None:\n        # Set the default bijector\n        # which is ShiftBounds -&gt; RollingSplineCoupling\n\n        # get the min/max for each data column\n        data = inputs[list(self.data_columns)].to_numpy()\n        mins = data.min(axis=0)\n        maxs = data.max(axis=0)\n\n        # determine how many conditional columns we have\n        n_conditions = (\n            0\n            if self.conditional_columns is None\n            else len(self.conditional_columns)\n        )\n\n        self.set_bijector(\n            Chain(\n                ShiftBounds(mins, maxs, 4.0),\n                RollingSplineCoupling(\n                    len(self.data_columns), n_conditions=n_conditions\n                ),\n            ),\n            seed=seed,\n        )\n\n    def _get_conditions(self, inputs: pd.DataFrame) -&gt; jnp.ndarray:\n        # Return an array of the bijector conditions.\n\n        # if this isn't a conditional flow, just return empty conditions\n        if self.conditional_columns is None:\n            conditions = jnp.zeros((inputs.shape[0], 1))\n        # if this a conditional flow, return an array of the conditions\n        else:\n            columns = list(self.conditional_columns)\n            conditions = jnp.array(inputs[columns].to_numpy())\n            conditions = (\n                conditions - self._condition_means\n            ) / self._condition_stds\n        return conditions\n\n    def _get_err_samples(\n        self,\n        key,\n        inputs: pd.DataFrame,\n        err_samples: int,\n        type: str = \"data\",\n        skip: str = None,\n    ) -&gt; jnp.ndarray:\n        # Draw error samples for each row of inputs.\n\n        X = inputs.copy()\n\n        # get list of columns\n        if type == \"data\":\n            columns = list(self.data_columns)\n            error_model = self.data_error_model\n        elif type == \"conditions\":\n            if self.conditional_columns is None:\n                return jnp.zeros((err_samples * X.shape[0], 1))\n            else:\n                columns = list(self.conditional_columns)\n                error_model = self.condition_error_model\n        else:\n            raise ValueError(\"type must be `data` or `conditions`.\")\n\n        # make sure all relevant variables have error columns\n        for col in columns:\n            # if errors not provided for the column, fill in zeros\n            if f\"{col}_err\" not in inputs.columns and col != skip:\n                X[f\"{col}_err\"] = jnp.zeros(X.shape[0])\n            # if we are skipping this column, fill in nan's\n            elif col == skip:\n                X[col] = jnp.nan * jnp.zeros(X.shape[0])\n                X[f\"{col}_err\"] = jnp.nan * jnp.zeros(X.shape[0])\n\n        # pull out relevant columns\n        err_columns = [col + \"_err\" for col in columns]\n        X, Xerr = jnp.array(X[columns].to_numpy()), jnp.array(\n            X[err_columns].to_numpy()\n        )\n\n        # generate samples\n        Xsamples = error_model(key, X, Xerr, err_samples)\n        Xsamples = Xsamples.reshape(X.shape[0] * err_samples, X.shape[1])\n\n        # delete the column corresponding to skip\n        if skip is not None:\n            idx = columns.index(skip)\n            Xsamples = jnp.delete(Xsamples, idx, axis=1)\n\n        # if these are samples of conditions, standard scale them!\n        if type == \"conditions\":\n            Xsamples = (\n                Xsamples - self._condition_means\n            ) / self._condition_stds\n\n        return Xsamples\n\n    def _log_prob(\n        self, params: Pytree, inputs: jnp.ndarray, conditions: jnp.ndarray\n    ) -&gt; jnp.ndarray:\n        # Log prob for arrays.\n\n        # calculate log_prob\n        u, log_det = self._forward(params[1], inputs, conditions=conditions)\n        log_prob = self.latent.log_prob(params[0], u) + log_det\n        # set NaN's to negative infinity (i.e. zero probability)\n        log_prob = jnp.nan_to_num(log_prob, nan=-jnp.inf)\n        return log_prob\n\n    def log_prob(\n        self, inputs: pd.DataFrame, err_samples: int = None, seed: int = None\n    ) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Parameters\n        ----------\n        inputs : pd.DataFrame\n            Input data for which log probability density is calculated.\n            Every column in self.data_columns must be present.\n            If self.conditional_columns is not None, those must be present\n            as well. If other columns are present, they are ignored.\n        err_samples : int; default=None\n            Number of samples from the error distribution to average over for\n            the log_prob calculation. If provided, Gaussian errors are assumed,\n            and method will look for error columns in `inputs`. Error columns\n            must end in `_err`. E.g. the error column for the variable `u` must\n            be `u_err`. Zero error assumed for any missing error columns.\n        seed : int; default=None\n            Random seed for drawing the samples with Gaussian errors.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0],).\n        \"\"\"\n\n        # check that the bijector exists\n        self._check_bijector()\n\n        if err_samples is None:\n            # convert data to an array with columns ordered\n            columns = list(self.data_columns)\n            X = jnp.array(inputs[columns].to_numpy())\n            # get conditions\n            conditions = self._get_conditions(inputs)\n            # calculate log_prob\n            return self._log_prob(self._params, X, conditions)\n\n        else:\n            # validate nsamples\n            assert isinstance(\n                err_samples, int\n            ), \"err_samples must be a positive integer.\"\n            assert err_samples &gt; 0, \"err_samples must be a positive integer.\"\n            # get Gaussian samples\n            seed = np.random.randint(1e18) if seed is None else seed\n            key = random.PRNGKey(seed)\n            X = self._get_err_samples(key, inputs, err_samples, type=\"data\")\n            C = self._get_err_samples(\n                key, inputs, err_samples, type=\"conditions\"\n            )\n            # calculate log_probs\n            log_probs = self._log_prob(self._params, X, C)\n            probs = jnp.exp(log_probs.reshape(-1, err_samples))\n            return jnp.log(probs.mean(axis=1))\n\n    def posterior(\n        self,\n        inputs: pd.DataFrame,\n        column: str,\n        grid: jnp.ndarray,\n        marg_rules: dict = None,\n        normalize: bool = True,\n        err_samples: int = None,\n        seed: int = None,\n        batch_size: int = None,\n        nan_to_zero: bool = True,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Calculates posterior distributions for the provided column.\n\n        Calculates the conditional posterior distribution, assuming the\n        data values in the other columns of the DataFrame.\n\n        Parameters\n        ----------\n        inputs : pd.DataFrame\n            Data on which the posterior distributions are conditioned.\n            Must have columns matching self.data_columns, *except*\n            for the column specified for the posterior (see below).\n        column : str\n            Name of the column for which the posterior distribution\n            is calculated. Must be one of the columns in self.data_columns.\n            However, whether or not this column is one of the columns in\n            `inputs` is irrelevant.\n        grid : jnp.ndarray\n            Grid on which to calculate the posterior.\n        marg_rules : dict; optional\n            Dictionary with rules for marginalizing over missing variables.\n            The dictionary must contain the key \"flag\", which gives the flag\n            that indicates a missing value. E.g. if missing values are given\n            the value 99, the dictionary should contain {\"flag\": 99}.\n            The dictionary must also contain {\"name\": callable} for any\n            variables that will need to be marginalized over, where name is\n            the name of the variable, and callable is a callable that takes\n            the row of variables nad returns a grid over which to marginalize\n            the variable. E.g. {\"y\": lambda row: jnp.linspace(0, row[\"x\"], 10)}.\n            Note: the callable for a given name must *always* return an array\n            of the same length, regardless of the input row.\n        err_samples : int; default=None\n            Number of samples from the error distribution to average over for\n            the posterior calculation. If provided, Gaussian errors are assumed,\n            and method will look for error columns in `inputs`. Error columns\n            must end in `_err`. E.g. the error column for the variable `u` must\n            be `u_err`. Zero error assumed for any missing error columns.\n        seed : int; default=None\n            Random seed for drawing the samples with Gaussian errors.\n        batch_size : int; default=None\n            Size of batches in which to calculate posteriors. If None, all\n            posteriors are calculated simultaneously. Simultaneous calculation\n            is faster, but memory intensive for large data sets.\n        normalize : boolean; default=True\n            Whether to normalize the posterior so that it integrates to 1.\n        nan_to_zero : bool; default=True\n            Whether to convert NaN's to zero probability in the final pdfs.\n\n        Returns\n        -------\n        jnp.ndarray\n            Device array of shape (inputs.shape[0], grid.size).\n        \"\"\"\n\n        # check that the bijector exists\n        self._check_bijector()\n\n        # get the index of the provided column, and remove it from the list\n        columns = list(self.data_columns)\n        idx = columns.index(column)\n        columns.remove(column)\n\n        nrows = inputs.shape[0]\n        batch_size = nrows if batch_size is None else batch_size\n\n        # make sure indices run 0 -&gt; nrows\n        inputs = inputs.reset_index(drop=True)\n\n        if err_samples is not None:\n            # validate nsamples\n            assert isinstance(\n                err_samples, int\n            ), \"err_samples must be a positive integer.\"\n            assert err_samples &gt; 0, \"err_samples must be a positive integer.\"\n            # set the seed\n            seed = np.random.randint(1e18) if seed is None else seed\n            key = random.PRNGKey(seed)\n\n        # empty array to hold pdfs\n        pdfs = jnp.zeros((nrows, len(grid)))\n\n        # if marginalization rules were passed, we will loop over the rules\n        # and repeatedly call this method\n        if marg_rules is not None:\n            # if the flag is NaN, we must use jnp.isnan to check for flags\n            if np.isnan(marg_rules[\"flag\"]):\n\n                def check_flags(data):\n                    return np.isnan(data)\n\n            # else we use jnp.isclose to check for flags\n            else:\n\n                def check_flags(data):\n                    return np.isclose(data, marg_rules[\"flag\"])\n\n            # first calculate pdfs for unflagged rows\n            unflagged_idx = inputs[\n                ~check_flags(inputs[columns]).any(axis=1)\n            ].index.tolist()\n            unflagged_pdfs = self.posterior(\n                inputs=inputs.iloc[unflagged_idx],\n                column=column,\n                grid=grid,\n                err_samples=err_samples,\n                seed=seed,\n                batch_size=batch_size,\n                normalize=False,\n                nan_to_zero=nan_to_zero,\n            )\n\n            # save these pdfs in the big array\n            pdfs = pdfs.at[unflagged_idx, :].set(\n                unflagged_pdfs,\n                indices_are_sorted=True,\n                unique_indices=True,\n            )\n\n            # we will keep track of all the rows we've already calculated\n            # posteriors for\n            already_done = unflagged_idx\n\n            # now we will loop over the rules in marg_rules\n            for name, rule in marg_rules.items():\n                # ignore the flag, because that's not a column in the data\n                if name == \"flag\":\n                    continue\n\n                # get the list of new rows for which we need to calculate posteriors\n                flagged_idx = inputs[check_flags(inputs[name])].index.tolist()\n                flagged_idx = list(set(flagged_idx).difference(already_done))\n\n                # if flagged_idx is empty, move on!\n                if len(flagged_idx) == 0:\n                    continue\n\n                # get the marginalization grid for each row\n                marg_grids = (\n                    inputs.iloc[flagged_idx]\n                    .apply(rule, axis=1, result_type=\"expand\")\n                    .to_numpy()\n                )\n\n                # make a new data frame with the marginalization grids replacing\n                # the values of the flag in the column\n                marg_inputs = pd.DataFrame(\n                    np.repeat(\n                        inputs.iloc[flagged_idx].to_numpy(),\n                        marg_grids.shape[1],\n                        axis=0,\n                    ),\n                    columns=inputs.columns,\n                )\n                marg_inputs[name] = marg_grids.reshape(marg_inputs.shape[0], 1)\n\n                # remove the error column if it's present\n                marg_inputs.drop(\n                    f\"{name}_err\", axis=1, inplace=True, errors=\"ignore\"\n                )\n\n                # calculate posteriors for these\n                marg_pdfs = self.posterior(\n                    inputs=marg_inputs,\n                    column=column,\n                    grid=grid,\n                    marg_rules=marg_rules,\n                    err_samples=err_samples,\n                    seed=seed,\n                    batch_size=batch_size,\n                    normalize=False,\n                    nan_to_zero=nan_to_zero,\n                )\n\n                # sum over the marginalized dimension\n                marg_pdfs = marg_pdfs.reshape(\n                    len(flagged_idx), marg_grids.shape[1], grid.size\n                )\n                marg_pdfs = marg_pdfs.sum(axis=1)\n\n                # save the new pdfs in the big array\n                pdfs = pdfs.at[flagged_idx, :].set(\n                    marg_pdfs,\n                    indices_are_sorted=True,\n                    unique_indices=True,\n                )\n\n                # add these flagged indices to the list of rows already done\n                already_done += flagged_idx\n\n        # now for the main posterior calculation loop\n        else:\n            # loop through batches\n            for batch_idx in range(0, nrows, batch_size):\n                # get the data batch\n                # and, if this is a conditional flow, the correpsonding conditions\n                batch = inputs.iloc[batch_idx : batch_idx + batch_size]\n\n                # if not drawing samples, just grab batch and conditions\n                if err_samples is None:\n                    conditions = self._get_conditions(batch)\n                    batch = jnp.array(batch[columns].to_numpy())\n                # if only drawing condition samples...\n                elif len(self.data_columns) == 1:\n                    conditions = self._get_err_samples(\n                        key, batch, err_samples, type=\"conditions\"\n                    )\n                    batch = jnp.repeat(\n                        batch[columns].to_numpy(), err_samples, axis=0\n                    )\n                # if drawing data and condition samples...\n                else:\n                    conditions = self._get_err_samples(\n                        key, batch, err_samples, type=\"conditions\"\n                    )\n                    batch = self._get_err_samples(\n                        key, batch, err_samples, skip=column, type=\"data\"\n                    )\n\n                # make a new copy of each row for each value of the column\n                # for which we are calculating the posterior\n                batch = jnp.hstack(\n                    (\n                        jnp.repeat(\n                            batch[:, :idx],\n                            len(grid),\n                            axis=0,\n                        ),\n                        jnp.tile(grid, len(batch))[:, None],\n                        jnp.repeat(\n                            batch[:, idx:],\n                            len(grid),\n                            axis=0,\n                        ),\n                    )\n                )\n\n                # make similar copies of the conditions\n                conditions = jnp.repeat(conditions, len(grid), axis=0)\n\n                # calculate probability densities\n                log_prob = self._log_prob(\n                    self._params, batch, conditions\n                ).reshape((-1, len(grid)))\n                prob = jnp.exp(log_prob)\n                # if we were Gaussian sampling, average over the samples\n                if err_samples is not None:\n                    prob = prob.reshape(-1, err_samples, len(grid))\n                    prob = prob.mean(axis=1)\n                # add the pdfs to the bigger list\n                pdfs = pdfs.at[batch_idx : batch_idx + batch_size, :].set(\n                    prob,\n                    indices_are_sorted=True,\n                    unique_indices=True,\n                )\n\n        if normalize:\n            # normalize so they integrate to one\n            pdfs = pdfs / trapezoid(y=pdfs, x=grid).reshape(-1, 1)\n        if nan_to_zero:\n            # set NaN's equal to zero probability\n            pdfs = jnp.nan_to_num(pdfs, nan=0.0)\n        return pdfs\n\n    def sample(\n        self,\n        nsamples: int = 1,\n        conditions: pd.DataFrame = None,\n        save_conditions: bool = True,\n        seed: int = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Returns samples from the normalizing flow.\n\n        Parameters\n        ----------\n        nsamples : int; default=1\n            The number of samples to be returned.\n        conditions : pd.DataFrame; optional\n            If this is a conditional flow, you must pass conditions for\n            each sample. nsamples will be drawn for each row in conditions.\n        save_conditions : bool; default=True\n            If true, conditions will be saved in the DataFrame of samples\n            that is returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n\n        Returns\n        -------\n        pd.DataFrame\n            Pandas DataFrame of samples.\n        \"\"\"\n\n        # check that the bijector exists\n        self._check_bijector()\n\n        # validate nsamples\n        assert isinstance(\n            nsamples, int\n        ), \"nsamples must be a positive integer.\"\n        assert nsamples &gt; 0, \"nsamples must be a positive integer.\"\n\n        if self.conditional_columns is not None and conditions is None:\n            raise ValueError(\n                f\"Must provide the following conditions\\n{self.conditional_columns}\"\n            )\n\n        # if this isn't a conditional flow, get empty conditions\n        if self.conditional_columns is None:\n            conditions = jnp.zeros((nsamples, 1))\n        # otherwise get conditions and make `nsamples` copies of each\n        else:\n            conditions_idx = list(conditions.index)\n            conditions = self._get_conditions(conditions)\n            conditions_idx = np.repeat(conditions_idx, nsamples)\n            conditions = jnp.repeat(conditions, nsamples, axis=0)\n\n        # draw from latent distribution\n        u = self.latent.sample(self._params[0], conditions.shape[0], seed)\n        # take the inverse back to the data distribution\n        x = self._inverse(self._params[1], u, conditions=conditions)[0]\n        # if not conditional, this is all we need\n        if self.conditional_columns is None:\n            x = pd.DataFrame(np.array(x), columns=self.data_columns)\n        # but if conditional\n        else:\n            if save_conditions:\n                # unscale the conditions\n                conditions = (\n                    conditions * self._condition_stds + self._condition_means\n                )\n                x = pd.DataFrame(\n                    np.array(jnp.hstack((x, conditions))),\n                    columns=self.data_columns + self.conditional_columns,\n                ).set_index(conditions_idx)\n            else:\n                # reindex according to the conditions\n                x = pd.DataFrame(\n                    np.array(x), columns=self.data_columns\n                ).set_index(conditions_idx)\n\n        # return the samples!\n        return x\n\n    def _save_dict(self) -&gt; None:\n        ### Returns the dictionary of all flow params to be saved.\n        save_dict = {\"class\": self.__class__.__name__}\n        keys = [\n            \"data_columns\",\n            \"conditional_columns\",\n            \"condition_means\",\n            \"condition_stds\",\n            \"data_error_model\",\n            \"condition_error_model\",\n            \"autoscale_conditions\",\n            \"info\",\n            \"latent_info\",\n            \"bijector_info\",\n            \"params\",\n        ]\n        for key in keys:\n            try:\n                save_dict[key] = getattr(self, key)\n            except AttributeError:\n                try:\n                    save_dict[key] = getattr(self, \"_\" + key)\n                except AttributeError:\n                    save_dict[key] = None\n\n        return save_dict\n\n    def save(self, file: str) -&gt; None:\n        \"\"\"Saves the flow to a file.\n\n        Pickles the flow and saves it to a file that can be passed as\n        the `file` argument during flow instantiation.\n\n        WARNING: Currently, this method only works for bijectors that are\n        implemented in the `bijectors` module. If you want to save a flow\n        with a custom bijector, you either need to add the bijector to that\n        module, or handle the saving and loading on your end.\n\n        Parameters\n        ----------\n        file : str\n            Path to where the flow will be saved.\n            Extension `.pkl` will be appended if not already present.\n        \"\"\"\n        save_dict = self._save_dict()\n\n        with open(file, \"wb\") as handle:\n            pickle.dump(save_dict, handle, recurse=True)\n\n    def train(\n        self,\n        inputs: pd.DataFrame,\n        val_set: pd.DataFrame = None,\n        train_weight: np.ndarray = None,\n        val_weight: np.ndarray = None,\n        epochs: int = 100,\n        batch_size: int = 1024,\n        optimizer: Callable = None,\n        loss_fn: Callable = None,\n        convolve_errs: bool = False,\n        patience: int = None,\n        best_params: bool = True,\n        seed: int = 0,\n        verbose: bool = False,\n        progress_bar: bool = False,\n        initial_loss: bool = True,\n    ) -&gt; list:\n        \"\"\"Trains the normalizing flow on the provided inputs.\n\n        Parameters\n        ----------\n        inputs : pd.DataFrame\n            Data on which to train the normalizing flow.\n            Must have columns matching `self.data_columns`.\n            If training a conditional flow, must also have columns\n            matching `self.conditional_columns`.\n        val_set : pd.DataFrame; default=None\n            Validation set, of same format as inputs. If provided,\n            validation loss will be calculated at the end of each epoch.\n        train_weight: np.ndarray; default=None\n            Array of weights for each sample in the training set.\n        val_weight: np.ndarray; default=None\n            Array of weights for each sample in the validation set.\n        epochs : int; default=100\n            Number of epochs to train.\n        batch_size : int; default=1024\n            Batch size for training.\n        optimizer : optax optimizer\n            An optimizer from Optax. default = optax.adam(learning_rate=1e-3)\n            see https://optax.readthedocs.io/en/latest/index.html for more.\n        loss_fn : Callable; optional\n            A function to calculate the loss: `loss = loss_fn(params, x, c, w)`.\n            If not provided, will be `-mean(log_prob)`.\n        convolve_errs : bool; default=False\n            Whether to draw new data from the error distributions during\n            each epoch of training. Method will look for error columns in\n            `inputs`. Error columns must end in `_err`. E.g. the error column\n            for the variable `u` must be `u_err`. Zero error assumed for\n            any missing error columns. The error distribution is set during\n            flow instantiation.\n        patience : int; optional\n            Factor that controls early stopping. Training will stop if the\n            loss doesn't decrease for this number of epochs. Note if a\n            validation set is provided, the validation loss is used.\n        best_params : bool; default=True\n            Whether to use the params from the epoch with the lowest loss.\n            Note if a validation set is provided, the epoch with the lowest\n            validation loss is chosen. If False, the params from the final\n            epoch are saved.\n        seed : int; default=0\n            A random seed to control the batching and the (optional)\n            error sampling and creating the default bijector (the latter\n            only happens if you didn't set up the bijector during Flow\n            instantiation).\n        verbose : bool; default=False\n            If true, print the training loss every 5% of epochs.\n        progress_bar : bool; default=False\n            If true, display a tqdm progress bar during training.\n        initial_loss : bool; default=True\n            If true, start by calculating the initial loss.\n\n        Returns\n        -------\n        list\n            List of training losses from every epoch. If no val_set provided,\n            these are just training losses. If val_set is provided, then the\n            first element is the list of training losses, while the second is\n            the list of validation losses.\n        \"\"\"\n        # split the seed\n        rng = np.random.default_rng(seed)\n        batch_seed, bijector_seed = rng.integers(1e9, size=2)\n\n        # if the bijector is None, set the default bijector\n        if self._bijector_info is None:\n            self._set_default_bijector(inputs, seed=bijector_seed)\n\n        # validate epochs\n        if not isinstance(epochs, int) or epochs &lt;= 0:\n            raise ValueError(\"epochs must be a positive integer.\")\n\n        # if no loss_fn is provided, use the default loss function\n        if loss_fn is None:\n\n            @jit\n            def loss_fn(params, x, c, w):\n                return -jnp.mean(w * self._log_prob(params, x, c))\n\n        # initialize the optimizer\n        optimizer = (\n            optax.adam(learning_rate=1e-3) if optimizer is None else optimizer\n        )\n        opt_state = optimizer.init(self._params)\n\n        # pull out the model parameters\n        model_params = self._params\n\n        # define the training step function\n        @jit\n        def step(params, opt_state, x, c, w):\n            gradients = grad(loss_fn)(params, x, c, w)\n            updates, opt_state = optimizer.update(gradients, opt_state, params)\n            params = optax.apply_updates(params, updates)\n            return params, opt_state\n\n        # get list of data columns\n        columns = list(self.data_columns)\n\n        # if this is a conditional flow, and autoscale_conditions == True\n        # save the means and stds of the conditional columns\n        if self.conditional_columns is not None and self._autoscale_conditions:\n            self._condition_means = jnp.array(\n                inputs[list(self.conditional_columns)].to_numpy().mean(axis=0)\n            )\n            condition_stds = jnp.array(\n                inputs[list(self.conditional_columns)].to_numpy().std(axis=0)\n            )\n            self._condition_stds = jnp.where(\n                condition_stds != 0, condition_stds, 1\n            )\n\n        # define a function to return batches\n        if convolve_errs:\n\n            def get_batch(sample_key, x, type):\n                return self._get_err_samples(sample_key, x, 1, type=type)\n\n        else:\n\n            def get_batch(sample_key, x, type):\n                if type == \"conditions\":\n                    return self._get_conditions(x)\n                else:\n                    return jnp.array(x[columns].to_numpy())\n\n        # get random seed for training loop\n        key = random.PRNGKey(batch_seed)\n\n        if verbose:\n            print(f\"Training {epochs} epochs \\nLoss:\")\n\n        # save the initial loss\n        W = jnp.ones(len(inputs)) if train_weight is None else train_weight\n        W /= W.mean()\n        if initial_loss:\n            X = jnp.array(inputs[columns].to_numpy())\n            C = self._get_conditions(inputs)\n            losses = [loss_fn(model_params, X, C, W).item()]\n        else:\n            losses = []\n\n        if val_set is not None:\n            Xval = jnp.array(val_set[columns].to_numpy())\n            Cval = self._get_conditions(val_set)\n            Wval = jnp.ones(len(val_set)) if val_weight is None else val_weight\n            Wval /= Wval.mean()\n            if initial_loss:\n                val_losses = [loss_fn(model_params, Xval, Cval, Wval).item()]\n            else:\n                val_losses = []\n\n        if verbose and initial_loss:\n            if val_set is None:\n                print(f\"(0) {losses[-1]:.4f}\")\n            else:\n                print(f\"(0) {losses[-1]:.4f}  {val_losses[-1]:.4f}\")\n\n        # initialize variables for early stopping\n        best_loss = jnp.inf\n        best_param_vals = model_params\n        early_stopping_counter = 0\n\n        # loop through training\n        loop = tqdm(range(epochs)) if progress_bar else range(epochs)\n        for epoch in loop:\n            # new permutation of batches\n            permute_key, sample_key, key = random.split(key, num=3)\n            idx = random.permutation(permute_key, inputs.shape[0])\n            X = inputs.iloc[idx]\n\n            # loop through batches and step optimizer\n            for batch_idx in range(0, len(X), batch_size):\n                # if sampling from the error distribution, this returns a\n                # Gaussian sample of the batch. Else just returns batch as a\n                # jax array\n                batch = get_batch(\n                    sample_key,\n                    X.iloc[batch_idx : batch_idx + batch_size],\n                    type=\"data\",\n                )\n                batch_conditions = get_batch(\n                    sample_key,\n                    X.iloc[batch_idx : batch_idx + batch_size],\n                    type=\"conditions\",\n                )\n                batch_weights = jnp.asarray(\n                    W[idx][batch_idx : batch_idx + batch_size]\n                )\n\n                model_params, opt_state = step(\n                    model_params,\n                    opt_state,\n                    batch,\n                    batch_conditions,\n                    batch_weights\n                )\n\n            # save end-of-epoch training loss\n            losses.append(\n                loss_fn(\n                    model_params,\n                    jnp.array(X[columns].to_numpy()),\n                    self._get_conditions(X),\n                    jnp.asarray(W),\n                ).item()\n            )\n\n            # and validation loss\n            if val_set is not None:\n                val_losses.append(loss_fn(model_params, Xval, Cval, Wval).item())\n\n            # if verbose, print current loss\n            if verbose and (\n                epoch % max(int(0.05 * epochs), 1) == 0\n                or (epoch + 1) == epochs\n            ):\n                if val_set is None:\n                    print(f\"({epoch+1}) {losses[-1]:.4f}\")\n                else:\n                    print(\n                        f\"({epoch+1}) {losses[-1]:.4f}  {val_losses[-1]:.4f}\"\n                    )\n\n            # if patience provided, we need to check for early stopping\n            if patience is not None or best_loss:\n                if val_set is None:\n                    tracked_losses = losses\n                else:\n                    tracked_losses = val_losses\n\n                # if loss didn't improve, increase counter\n                # and check early stopping criterion\n                if tracked_losses[-1] &gt;= best_loss or jnp.isclose(\n                    tracked_losses[-1], best_loss\n                ):\n                    early_stopping_counter += 1\n\n                    # check if the early stopping criterion is met\n                    if (\n                        patience is not None\n                        and early_stopping_counter &gt;= patience\n                    ):\n                        print(\n                            \"Early stopping criterion is met.\",\n                            f\"Training stopping after epoch {epoch+1}.\",\n                        )\n                        break\n                # if this is the best loss, reset the counter\n                else:\n                    best_loss = tracked_losses[-1]\n                    best_param_vals = model_params\n                    early_stopping_counter = 0\n\n            # break if the training loss is NaN\n            if not np.isfinite(losses[-1]):\n                print(\n                    f\"Training stopping after epoch {epoch+1}\",\n                    \"because training loss diverged.\",\n                )\n                break\n\n        # update the flow parameters with the final training state\n        if best_params:\n            self._params = best_param_vals\n        else:\n            self._params = model_params\n\n        if val_set is None:\n            return losses\n        else:\n            return [losses, val_losses]\n</code></pre>"},{"location":"API/flow/#pzflow.flow.Flow.__init__","title":"<code>__init__(data_columns=None, bijector=None, latent=None, conditional_columns=None, data_error_model=None, condition_error_model=None, autoscale_conditions=True, seed=0, info=None, file=None, _dictionary=None)</code>","text":"<p>Instantiate a normalizing flow.</p> <p>Note that while all of the init parameters are technically optional, you must provide either data_columns OR file. In addition, if a file is provided, all other parameters must be None.</p> <p>Parameters:</p> Name Type Description Default <code>data_columns</code> <code>Sequence[str]; optional</code> <p>Tuple, list, or other container of column names. These are the columns the flow expects/produces in DataFrames.</p> <code>None</code> <code>bijector</code> <code>Bijector Call; optional</code> <p>A Bijector call that consists of the bijector InitFunction that initializes the bijector and the tuple of Bijector Info. Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc. If not provided, the bijector can be set later using flow.set_bijector, or by calling flow.train, in which case the default bijector will be used. The default bijector is ShiftBounds -&gt; RollingSplineCoupling, where the range of shift bounds is learned from the training data, and the dimensions of RollingSplineCoupling is inferred. The default bijector assumes that the latent has support [-5, 5] for every dimension.</p> <code>None</code> <code>latent</code> <code>distributions.LatentDist; optional</code> <p>The latent distribution for the normalizing flow. Can be any of the distributions from pzflow.distributions. If not provided, CentBeta13 is used with input_dim = len(data_columns) and B=5.</p> <code>None</code> <code>conditional_columns</code> <code>Sequence[str]; optional</code> <p>Names of columns on which to condition the normalizing flow.</p> <code>None</code> <code>data_error_model</code> <code>Callable; optional</code> <p>A callable that defines the error model for data variables. data_error_model must take key, X, Xerr, nsamples as arguments:     - key is a jax rng key, e.g. jax.random.PRNGKey(0)     - X is 2D array of data variables, where the order of variables         matches the order of the columns in data_columns     - Xerr is the corresponding 2D array of errors     - nsamples is number of samples to draw from error distribution data_error_model must return an array of samples with the shape (X.shape[0], nsamples, X.shape[1]). If data_error_model is not provided, Gaussian error model assumed.</p> <code>None</code> <code>condition_error_model</code> <code>Callable; optional</code> <p>A callable that defines the error model for conditional variables. condition_error_model must take key, X, Xerr, nsamples, where:     - key is a jax rng key, e.g. jax.random.PRNGKey(0)     - X is 2D array of conditional variables, where the order of         variables matches order of columns in conditional_columns     - Xerr is the corresponding 2D array of errors     - nsamples is number of samples to draw from error distribution condition_error_model must return array of samples with shape (X.shape[0], nsamples, X.shape[1]). If condition_error_model is not provided, Gaussian error model assumed.</p> <code>None</code> <code>autoscale_conditions</code> <code>bool; default=True</code> <p>Sets whether or not conditions are automatically standard scaled when passed to a conditional flow. I recommend you leave as True.</p> <code>True</code> <code>seed</code> <code>int; default=0</code> <p>The random seed for initial parameters</p> <code>0</code> <code>info</code> <code>Any; optional</code> <p>An object to attach to the info attribute.</p> <code>None</code> <code>file</code> <code>str; optional</code> <p>Path to file from which to load a pretrained flow. If a file is provided, all other parameters must be None.</p> <code>None</code> Source code in <code>pzflow/flow.py</code> <pre><code>def __init__(\n    self,\n    data_columns: Sequence[str] = None,\n    bijector: Tuple[InitFunction, Bijector_Info] = None,\n    latent: distributions.LatentDist = None,\n    conditional_columns: Sequence[str] = None,\n    data_error_model: Callable = None,\n    condition_error_model: Callable = None,\n    autoscale_conditions: bool = True,\n    seed: int = 0,\n    info: Any = None,\n    file: str = None,\n    _dictionary: dict = None,\n) -&gt; None:\n    \"\"\"Instantiate a normalizing flow.\n\n    Note that while all of the init parameters are technically optional,\n    you must provide either data_columns OR file.\n    In addition, if a file is provided, all other parameters must be None.\n\n    Parameters\n    ----------\n    data_columns : Sequence[str]; optional\n        Tuple, list, or other container of column names.\n        These are the columns the flow expects/produces in DataFrames.\n    bijector : Bijector Call; optional\n        A Bijector call that consists of the bijector InitFunction that\n        initializes the bijector and the tuple of Bijector Info.\n        Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc.\n        If not provided, the bijector can be set later using\n        flow.set_bijector, or by calling flow.train, in which case the\n        default bijector will be used. The default bijector is\n        ShiftBounds -&gt; RollingSplineCoupling, where the range of shift\n        bounds is learned from the training data, and the dimensions of\n        RollingSplineCoupling is inferred. The default bijector assumes\n        that the latent has support [-5, 5] for every dimension.\n    latent : distributions.LatentDist; optional\n        The latent distribution for the normalizing flow. Can be any of\n        the distributions from pzflow.distributions. If not provided,\n        CentBeta13 is used with input_dim = len(data_columns) and B=5.\n    conditional_columns : Sequence[str]; optional\n        Names of columns on which to condition the normalizing flow.\n    data_error_model : Callable; optional\n        A callable that defines the error model for data variables.\n        data_error_model must take key, X, Xerr, nsamples as arguments:\n            - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n            - X is 2D array of data variables, where the order of variables\n                matches the order of the columns in data_columns\n            - Xerr is the corresponding 2D array of errors\n            - nsamples is number of samples to draw from error distribution\n        data_error_model must return an array of samples with the shape\n        (X.shape[0], nsamples, X.shape[1]).\n        If data_error_model is not provided, Gaussian error model assumed.\n    condition_error_model : Callable; optional\n        A callable that defines the error model for conditional variables.\n        condition_error_model must take key, X, Xerr, nsamples, where:\n            - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n            - X is 2D array of conditional variables, where the order of\n                variables matches order of columns in conditional_columns\n            - Xerr is the corresponding 2D array of errors\n            - nsamples is number of samples to draw from error distribution\n        condition_error_model must return array of samples with shape\n        (X.shape[0], nsamples, X.shape[1]).\n        If condition_error_model is not provided, Gaussian error model\n        assumed.\n    autoscale_conditions : bool; default=True\n        Sets whether or not conditions are automatically standard scaled\n        when passed to a conditional flow. I recommend you leave as True.\n    seed : int; default=0\n        The random seed for initial parameters\n    info : Any; optional\n        An object to attach to the info attribute.\n    file : str; optional\n        Path to file from which to load a pretrained flow.\n        If a file is provided, all other parameters must be None.\n    \"\"\"\n\n    # validate parameters\n    if data_columns is None and file is None and _dictionary is None:\n        raise ValueError(\"You must provide data_columns OR file.\")\n    if any(\n        (\n            data_columns is not None,\n            bijector is not None,\n            conditional_columns is not None,\n            latent is not None,\n            data_error_model is not None,\n            condition_error_model is not None,\n            info is not None,\n        )\n    ):\n        if file is not None:\n            raise ValueError(\n                \"If providing a file, please do not provide any other parameters.\"\n            )\n        if _dictionary is not None:\n            raise ValueError(\n                \"If providing a dictionary, please do not provide any other parameters.\"\n            )\n    if file is not None and _dictionary is not None:\n        raise ValueError(\"Only provide file or _dictionary, not both.\")\n\n    # if file or dictionary is provided, load everything from it\n    if file is not None or _dictionary is not None:\n        save_dict = self._save_dict()\n        if file is not None:\n            with open(file, \"rb\") as handle:\n                save_dict.update(pickle.load(handle))\n        else:\n            save_dict.update(_dictionary)\n\n        if save_dict[\"class\"] != self.__class__.__name__:\n            raise TypeError(\n                f\"This save file isn't a {self.__class__.__name__}. \"\n                f\"It is a {save_dict['class']}\"\n            )\n\n        # load columns and dimensions\n        self.data_columns = save_dict[\"data_columns\"]\n        self.conditional_columns = save_dict[\"conditional_columns\"]\n        self._input_dim = len(self.data_columns)\n        self.info = save_dict[\"info\"]\n\n        # load the latent distribution\n        self._latent_info = save_dict[\"latent_info\"]\n        self.latent = getattr(distributions, self._latent_info[0])(\n            *self._latent_info[1]\n        )\n\n        # load the error models\n        self.data_error_model = save_dict[\"data_error_model\"]\n        self.condition_error_model = save_dict[\"condition_error_model\"]\n\n        # load the bijector\n        self._bijector_info = save_dict[\"bijector_info\"]\n        if self._bijector_info is not None:\n            init_fun, _ = build_bijector_from_info(self._bijector_info)\n            _, self._forward, self._inverse = init_fun(\n                random.PRNGKey(0), self._input_dim\n            )\n        self._params = save_dict[\"params\"]\n\n        # load the conditional means and stds\n        self._condition_means = save_dict[\"condition_means\"]\n        self._condition_stds = save_dict[\"condition_stds\"]\n\n        # set whether or not to automatically standard scale any\n        # conditions passed to the normalizing flow\n        self._autoscale_conditions = save_dict[\"autoscale_conditions\"]\n\n    # if no file is provided, use provided parameters\n    else:\n        self.data_columns = tuple(data_columns)\n        self._input_dim = len(self.data_columns)\n        self.info = info\n\n        if conditional_columns is None:\n            self.conditional_columns = None\n            self._condition_means = None\n            self._condition_stds = None\n        else:\n            self.conditional_columns = tuple(conditional_columns)\n            self._condition_means = jnp.zeros(\n                len(self.conditional_columns)\n            )\n            self._condition_stds = jnp.ones(len(self.conditional_columns))\n\n        # set whether or not to automatically standard scale any\n        # conditions passed to the normalizing flow\n        self._autoscale_conditions = autoscale_conditions\n\n        # set up the latent distribution\n        if latent is None:\n            self.latent = distributions.CentBeta13(self._input_dim, 5)\n        else:\n            self.latent = latent\n        self._latent_info = self.latent.info\n\n        # make sure the latent distribution and data_columns have the\n        # same number of dimensions\n        if self.latent.input_dim != len(data_columns):\n            raise ValueError(\n                f\"The latent distribution has {self.latent.input_dim} \"\n                f\"dimensions, but data_columns has {len(data_columns)} \"\n                \"dimensions. They must match!\"\n            )\n\n        # set up the error models\n        if data_error_model is None:\n            self.data_error_model = gaussian_error_model\n        else:\n            self.data_error_model = data_error_model\n        if condition_error_model is None:\n            self.condition_error_model = gaussian_error_model\n        else:\n            self.condition_error_model = condition_error_model\n\n        # set up the bijector\n        if bijector is not None:\n            self.set_bijector(bijector, seed=seed)\n        # if no bijector was provided, set bijector_info to None\n        else:\n            self._bijector_info = None\n</code></pre>"},{"location":"API/flow/#pzflow.flow.Flow.log_prob","title":"<code>log_prob(inputs, err_samples=None, seed=None)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataFrame</code> <p>Input data for which log probability density is calculated. Every column in self.data_columns must be present. If self.conditional_columns is not None, those must be present as well. If other columns are present, they are ignored.</p> required <code>err_samples</code> <code>int; default=None</code> <p>Number of samples from the error distribution to average over for the log_prob calculation. If provided, Gaussian errors are assumed, and method will look for error columns in <code>inputs</code>. Error columns must end in <code>_err</code>. E.g. the error column for the variable <code>u</code> must be <code>u_err</code>. Zero error assumed for any missing error columns.</p> <code>None</code> <code>seed</code> <code>int; default=None</code> <p>Random seed for drawing the samples with Gaussian errors.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0],).</p> Source code in <code>pzflow/flow.py</code> <pre><code>def log_prob(\n    self, inputs: pd.DataFrame, err_samples: int = None, seed: int = None\n) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Parameters\n    ----------\n    inputs : pd.DataFrame\n        Input data for which log probability density is calculated.\n        Every column in self.data_columns must be present.\n        If self.conditional_columns is not None, those must be present\n        as well. If other columns are present, they are ignored.\n    err_samples : int; default=None\n        Number of samples from the error distribution to average over for\n        the log_prob calculation. If provided, Gaussian errors are assumed,\n        and method will look for error columns in `inputs`. Error columns\n        must end in `_err`. E.g. the error column for the variable `u` must\n        be `u_err`. Zero error assumed for any missing error columns.\n    seed : int; default=None\n        Random seed for drawing the samples with Gaussian errors.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0],).\n    \"\"\"\n\n    # check that the bijector exists\n    self._check_bijector()\n\n    if err_samples is None:\n        # convert data to an array with columns ordered\n        columns = list(self.data_columns)\n        X = jnp.array(inputs[columns].to_numpy())\n        # get conditions\n        conditions = self._get_conditions(inputs)\n        # calculate log_prob\n        return self._log_prob(self._params, X, conditions)\n\n    else:\n        # validate nsamples\n        assert isinstance(\n            err_samples, int\n        ), \"err_samples must be a positive integer.\"\n        assert err_samples &gt; 0, \"err_samples must be a positive integer.\"\n        # get Gaussian samples\n        seed = np.random.randint(1e18) if seed is None else seed\n        key = random.PRNGKey(seed)\n        X = self._get_err_samples(key, inputs, err_samples, type=\"data\")\n        C = self._get_err_samples(\n            key, inputs, err_samples, type=\"conditions\"\n        )\n        # calculate log_probs\n        log_probs = self._log_prob(self._params, X, C)\n        probs = jnp.exp(log_probs.reshape(-1, err_samples))\n        return jnp.log(probs.mean(axis=1))\n</code></pre>"},{"location":"API/flow/#pzflow.flow.Flow.posterior","title":"<code>posterior(inputs, column, grid, marg_rules=None, normalize=True, err_samples=None, seed=None, batch_size=None, nan_to_zero=True)</code>","text":"<p>Calculates posterior distributions for the provided column.</p> <p>Calculates the conditional posterior distribution, assuming the data values in the other columns of the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataFrame</code> <p>Data on which the posterior distributions are conditioned. Must have columns matching self.data_columns, except for the column specified for the posterior (see below).</p> required <code>column</code> <code>str</code> <p>Name of the column for which the posterior distribution is calculated. Must be one of the columns in self.data_columns. However, whether or not this column is one of the columns in <code>inputs</code> is irrelevant.</p> required <code>grid</code> <code>ndarray</code> <p>Grid on which to calculate the posterior.</p> required <code>marg_rules</code> <code>dict; optional</code> <p>Dictionary with rules for marginalizing over missing variables. The dictionary must contain the key \"flag\", which gives the flag that indicates a missing value. E.g. if missing values are given the value 99, the dictionary should contain {\"flag\": 99}. The dictionary must also contain {\"name\": callable} for any variables that will need to be marginalized over, where name is the name of the variable, and callable is a callable that takes the row of variables nad returns a grid over which to marginalize the variable. E.g. {\"y\": lambda row: jnp.linspace(0, row[\"x\"], 10)}. Note: the callable for a given name must always return an array of the same length, regardless of the input row.</p> <code>None</code> <code>err_samples</code> <code>int; default=None</code> <p>Number of samples from the error distribution to average over for the posterior calculation. If provided, Gaussian errors are assumed, and method will look for error columns in <code>inputs</code>. Error columns must end in <code>_err</code>. E.g. the error column for the variable <code>u</code> must be <code>u_err</code>. Zero error assumed for any missing error columns.</p> <code>None</code> <code>seed</code> <code>int; default=None</code> <p>Random seed for drawing the samples with Gaussian errors.</p> <code>None</code> <code>batch_size</code> <code>int; default=None</code> <p>Size of batches in which to calculate posteriors. If None, all posteriors are calculated simultaneously. Simultaneous calculation is faster, but memory intensive for large data sets.</p> <code>None</code> <code>normalize</code> <code>boolean; default=True</code> <p>Whether to normalize the posterior so that it integrates to 1.</p> <code>True</code> <code>nan_to_zero</code> <code>bool; default=True</code> <p>Whether to convert NaN's to zero probability in the final pdfs.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Device array of shape (inputs.shape[0], grid.size).</p> Source code in <code>pzflow/flow.py</code> <pre><code>def posterior(\n    self,\n    inputs: pd.DataFrame,\n    column: str,\n    grid: jnp.ndarray,\n    marg_rules: dict = None,\n    normalize: bool = True,\n    err_samples: int = None,\n    seed: int = None,\n    batch_size: int = None,\n    nan_to_zero: bool = True,\n) -&gt; jnp.ndarray:\n    \"\"\"Calculates posterior distributions for the provided column.\n\n    Calculates the conditional posterior distribution, assuming the\n    data values in the other columns of the DataFrame.\n\n    Parameters\n    ----------\n    inputs : pd.DataFrame\n        Data on which the posterior distributions are conditioned.\n        Must have columns matching self.data_columns, *except*\n        for the column specified for the posterior (see below).\n    column : str\n        Name of the column for which the posterior distribution\n        is calculated. Must be one of the columns in self.data_columns.\n        However, whether or not this column is one of the columns in\n        `inputs` is irrelevant.\n    grid : jnp.ndarray\n        Grid on which to calculate the posterior.\n    marg_rules : dict; optional\n        Dictionary with rules for marginalizing over missing variables.\n        The dictionary must contain the key \"flag\", which gives the flag\n        that indicates a missing value. E.g. if missing values are given\n        the value 99, the dictionary should contain {\"flag\": 99}.\n        The dictionary must also contain {\"name\": callable} for any\n        variables that will need to be marginalized over, where name is\n        the name of the variable, and callable is a callable that takes\n        the row of variables nad returns a grid over which to marginalize\n        the variable. E.g. {\"y\": lambda row: jnp.linspace(0, row[\"x\"], 10)}.\n        Note: the callable for a given name must *always* return an array\n        of the same length, regardless of the input row.\n    err_samples : int; default=None\n        Number of samples from the error distribution to average over for\n        the posterior calculation. If provided, Gaussian errors are assumed,\n        and method will look for error columns in `inputs`. Error columns\n        must end in `_err`. E.g. the error column for the variable `u` must\n        be `u_err`. Zero error assumed for any missing error columns.\n    seed : int; default=None\n        Random seed for drawing the samples with Gaussian errors.\n    batch_size : int; default=None\n        Size of batches in which to calculate posteriors. If None, all\n        posteriors are calculated simultaneously. Simultaneous calculation\n        is faster, but memory intensive for large data sets.\n    normalize : boolean; default=True\n        Whether to normalize the posterior so that it integrates to 1.\n    nan_to_zero : bool; default=True\n        Whether to convert NaN's to zero probability in the final pdfs.\n\n    Returns\n    -------\n    jnp.ndarray\n        Device array of shape (inputs.shape[0], grid.size).\n    \"\"\"\n\n    # check that the bijector exists\n    self._check_bijector()\n\n    # get the index of the provided column, and remove it from the list\n    columns = list(self.data_columns)\n    idx = columns.index(column)\n    columns.remove(column)\n\n    nrows = inputs.shape[0]\n    batch_size = nrows if batch_size is None else batch_size\n\n    # make sure indices run 0 -&gt; nrows\n    inputs = inputs.reset_index(drop=True)\n\n    if err_samples is not None:\n        # validate nsamples\n        assert isinstance(\n            err_samples, int\n        ), \"err_samples must be a positive integer.\"\n        assert err_samples &gt; 0, \"err_samples must be a positive integer.\"\n        # set the seed\n        seed = np.random.randint(1e18) if seed is None else seed\n        key = random.PRNGKey(seed)\n\n    # empty array to hold pdfs\n    pdfs = jnp.zeros((nrows, len(grid)))\n\n    # if marginalization rules were passed, we will loop over the rules\n    # and repeatedly call this method\n    if marg_rules is not None:\n        # if the flag is NaN, we must use jnp.isnan to check for flags\n        if np.isnan(marg_rules[\"flag\"]):\n\n            def check_flags(data):\n                return np.isnan(data)\n\n        # else we use jnp.isclose to check for flags\n        else:\n\n            def check_flags(data):\n                return np.isclose(data, marg_rules[\"flag\"])\n\n        # first calculate pdfs for unflagged rows\n        unflagged_idx = inputs[\n            ~check_flags(inputs[columns]).any(axis=1)\n        ].index.tolist()\n        unflagged_pdfs = self.posterior(\n            inputs=inputs.iloc[unflagged_idx],\n            column=column,\n            grid=grid,\n            err_samples=err_samples,\n            seed=seed,\n            batch_size=batch_size,\n            normalize=False,\n            nan_to_zero=nan_to_zero,\n        )\n\n        # save these pdfs in the big array\n        pdfs = pdfs.at[unflagged_idx, :].set(\n            unflagged_pdfs,\n            indices_are_sorted=True,\n            unique_indices=True,\n        )\n\n        # we will keep track of all the rows we've already calculated\n        # posteriors for\n        already_done = unflagged_idx\n\n        # now we will loop over the rules in marg_rules\n        for name, rule in marg_rules.items():\n            # ignore the flag, because that's not a column in the data\n            if name == \"flag\":\n                continue\n\n            # get the list of new rows for which we need to calculate posteriors\n            flagged_idx = inputs[check_flags(inputs[name])].index.tolist()\n            flagged_idx = list(set(flagged_idx).difference(already_done))\n\n            # if flagged_idx is empty, move on!\n            if len(flagged_idx) == 0:\n                continue\n\n            # get the marginalization grid for each row\n            marg_grids = (\n                inputs.iloc[flagged_idx]\n                .apply(rule, axis=1, result_type=\"expand\")\n                .to_numpy()\n            )\n\n            # make a new data frame with the marginalization grids replacing\n            # the values of the flag in the column\n            marg_inputs = pd.DataFrame(\n                np.repeat(\n                    inputs.iloc[flagged_idx].to_numpy(),\n                    marg_grids.shape[1],\n                    axis=0,\n                ),\n                columns=inputs.columns,\n            )\n            marg_inputs[name] = marg_grids.reshape(marg_inputs.shape[0], 1)\n\n            # remove the error column if it's present\n            marg_inputs.drop(\n                f\"{name}_err\", axis=1, inplace=True, errors=\"ignore\"\n            )\n\n            # calculate posteriors for these\n            marg_pdfs = self.posterior(\n                inputs=marg_inputs,\n                column=column,\n                grid=grid,\n                marg_rules=marg_rules,\n                err_samples=err_samples,\n                seed=seed,\n                batch_size=batch_size,\n                normalize=False,\n                nan_to_zero=nan_to_zero,\n            )\n\n            # sum over the marginalized dimension\n            marg_pdfs = marg_pdfs.reshape(\n                len(flagged_idx), marg_grids.shape[1], grid.size\n            )\n            marg_pdfs = marg_pdfs.sum(axis=1)\n\n            # save the new pdfs in the big array\n            pdfs = pdfs.at[flagged_idx, :].set(\n                marg_pdfs,\n                indices_are_sorted=True,\n                unique_indices=True,\n            )\n\n            # add these flagged indices to the list of rows already done\n            already_done += flagged_idx\n\n    # now for the main posterior calculation loop\n    else:\n        # loop through batches\n        for batch_idx in range(0, nrows, batch_size):\n            # get the data batch\n            # and, if this is a conditional flow, the correpsonding conditions\n            batch = inputs.iloc[batch_idx : batch_idx + batch_size]\n\n            # if not drawing samples, just grab batch and conditions\n            if err_samples is None:\n                conditions = self._get_conditions(batch)\n                batch = jnp.array(batch[columns].to_numpy())\n            # if only drawing condition samples...\n            elif len(self.data_columns) == 1:\n                conditions = self._get_err_samples(\n                    key, batch, err_samples, type=\"conditions\"\n                )\n                batch = jnp.repeat(\n                    batch[columns].to_numpy(), err_samples, axis=0\n                )\n            # if drawing data and condition samples...\n            else:\n                conditions = self._get_err_samples(\n                    key, batch, err_samples, type=\"conditions\"\n                )\n                batch = self._get_err_samples(\n                    key, batch, err_samples, skip=column, type=\"data\"\n                )\n\n            # make a new copy of each row for each value of the column\n            # for which we are calculating the posterior\n            batch = jnp.hstack(\n                (\n                    jnp.repeat(\n                        batch[:, :idx],\n                        len(grid),\n                        axis=0,\n                    ),\n                    jnp.tile(grid, len(batch))[:, None],\n                    jnp.repeat(\n                        batch[:, idx:],\n                        len(grid),\n                        axis=0,\n                    ),\n                )\n            )\n\n            # make similar copies of the conditions\n            conditions = jnp.repeat(conditions, len(grid), axis=0)\n\n            # calculate probability densities\n            log_prob = self._log_prob(\n                self._params, batch, conditions\n            ).reshape((-1, len(grid)))\n            prob = jnp.exp(log_prob)\n            # if we were Gaussian sampling, average over the samples\n            if err_samples is not None:\n                prob = prob.reshape(-1, err_samples, len(grid))\n                prob = prob.mean(axis=1)\n            # add the pdfs to the bigger list\n            pdfs = pdfs.at[batch_idx : batch_idx + batch_size, :].set(\n                prob,\n                indices_are_sorted=True,\n                unique_indices=True,\n            )\n\n    if normalize:\n        # normalize so they integrate to one\n        pdfs = pdfs / trapezoid(y=pdfs, x=grid).reshape(-1, 1)\n    if nan_to_zero:\n        # set NaN's equal to zero probability\n        pdfs = jnp.nan_to_num(pdfs, nan=0.0)\n    return pdfs\n</code></pre>"},{"location":"API/flow/#pzflow.flow.Flow.sample","title":"<code>sample(nsamples=1, conditions=None, save_conditions=True, seed=None)</code>","text":"<p>Returns samples from the normalizing flow.</p> <p>Parameters:</p> Name Type Description Default <code>nsamples</code> <code>int; default=1</code> <p>The number of samples to be returned.</p> <code>1</code> <code>conditions</code> <code>pd.DataFrame; optional</code> <p>If this is a conditional flow, you must pass conditions for each sample. nsamples will be drawn for each row in conditions.</p> <code>None</code> <code>save_conditions</code> <code>bool; default=True</code> <p>If true, conditions will be saved in the DataFrame of samples that is returned.</p> <code>True</code> <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame of samples.</p> Source code in <code>pzflow/flow.py</code> <pre><code>def sample(\n    self,\n    nsamples: int = 1,\n    conditions: pd.DataFrame = None,\n    save_conditions: bool = True,\n    seed: int = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Returns samples from the normalizing flow.\n\n    Parameters\n    ----------\n    nsamples : int; default=1\n        The number of samples to be returned.\n    conditions : pd.DataFrame; optional\n        If this is a conditional flow, you must pass conditions for\n        each sample. nsamples will be drawn for each row in conditions.\n    save_conditions : bool; default=True\n        If true, conditions will be saved in the DataFrame of samples\n        that is returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n\n    Returns\n    -------\n    pd.DataFrame\n        Pandas DataFrame of samples.\n    \"\"\"\n\n    # check that the bijector exists\n    self._check_bijector()\n\n    # validate nsamples\n    assert isinstance(\n        nsamples, int\n    ), \"nsamples must be a positive integer.\"\n    assert nsamples &gt; 0, \"nsamples must be a positive integer.\"\n\n    if self.conditional_columns is not None and conditions is None:\n        raise ValueError(\n            f\"Must provide the following conditions\\n{self.conditional_columns}\"\n        )\n\n    # if this isn't a conditional flow, get empty conditions\n    if self.conditional_columns is None:\n        conditions = jnp.zeros((nsamples, 1))\n    # otherwise get conditions and make `nsamples` copies of each\n    else:\n        conditions_idx = list(conditions.index)\n        conditions = self._get_conditions(conditions)\n        conditions_idx = np.repeat(conditions_idx, nsamples)\n        conditions = jnp.repeat(conditions, nsamples, axis=0)\n\n    # draw from latent distribution\n    u = self.latent.sample(self._params[0], conditions.shape[0], seed)\n    # take the inverse back to the data distribution\n    x = self._inverse(self._params[1], u, conditions=conditions)[0]\n    # if not conditional, this is all we need\n    if self.conditional_columns is None:\n        x = pd.DataFrame(np.array(x), columns=self.data_columns)\n    # but if conditional\n    else:\n        if save_conditions:\n            # unscale the conditions\n            conditions = (\n                conditions * self._condition_stds + self._condition_means\n            )\n            x = pd.DataFrame(\n                np.array(jnp.hstack((x, conditions))),\n                columns=self.data_columns + self.conditional_columns,\n            ).set_index(conditions_idx)\n        else:\n            # reindex according to the conditions\n            x = pd.DataFrame(\n                np.array(x), columns=self.data_columns\n            ).set_index(conditions_idx)\n\n    # return the samples!\n    return x\n</code></pre>"},{"location":"API/flow/#pzflow.flow.Flow.save","title":"<code>save(file)</code>","text":"<p>Saves the flow to a file.</p> <p>Pickles the flow and saves it to a file that can be passed as the <code>file</code> argument during flow instantiation.</p> <p>WARNING: Currently, this method only works for bijectors that are implemented in the <code>bijectors</code> module. If you want to save a flow with a custom bijector, you either need to add the bijector to that module, or handle the saving and loading on your end.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to where the flow will be saved. Extension <code>.pkl</code> will be appended if not already present.</p> required Source code in <code>pzflow/flow.py</code> <pre><code>def save(self, file: str) -&gt; None:\n    \"\"\"Saves the flow to a file.\n\n    Pickles the flow and saves it to a file that can be passed as\n    the `file` argument during flow instantiation.\n\n    WARNING: Currently, this method only works for bijectors that are\n    implemented in the `bijectors` module. If you want to save a flow\n    with a custom bijector, you either need to add the bijector to that\n    module, or handle the saving and loading on your end.\n\n    Parameters\n    ----------\n    file : str\n        Path to where the flow will be saved.\n        Extension `.pkl` will be appended if not already present.\n    \"\"\"\n    save_dict = self._save_dict()\n\n    with open(file, \"wb\") as handle:\n        pickle.dump(save_dict, handle, recurse=True)\n</code></pre>"},{"location":"API/flow/#pzflow.flow.Flow.set_bijector","title":"<code>set_bijector(bijector, params=None, seed=0)</code>","text":"<p>Set the bijector.</p> <p>Parameters:</p> Name Type Description Default <code>bijector</code> <code>Bijector Call</code> <p>A Bijector call that consists of the bijector InitFunction that initializes the bijector and the tuple of Bijector Info. Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc.</p> required <code>params</code> <code>Pytree; optional</code> <p>A Pytree of bijector parameters. If not provided, the bijector will be initialized with random parameters.</p> <code>None</code> <code>seed</code> <code>int</code> <p>A random seed for initializing the bijector with random parameters.</p> <code>0</code> Source code in <code>pzflow/flow.py</code> <pre><code>def set_bijector(\n    self,\n    bijector: Tuple[InitFunction, Bijector_Info],\n    params: Pytree = None,\n    seed: int = 0,\n) -&gt; None:\n    \"\"\"Set the bijector.\n\n    Parameters\n    ----------\n    bijector : Bijector Call\n        A Bijector call that consists of the bijector InitFunction that\n        initializes the bijector and the tuple of Bijector Info.\n        Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc.\n    params : Pytree; optional\n        A Pytree of bijector parameters. If not provided, the bijector\n        will be initialized with random parameters.\n    seed: int; default=0\n        A random seed for initializing the bijector with random parameters.\n    \"\"\"\n\n    # set up the bijector\n    init_fun, self._bijector_info = bijector\n    bijector_params, self._forward, self._inverse = init_fun(\n        random.PRNGKey(seed), self._input_dim\n    )\n\n    # check if params were passed\n    bijector_params = params if params is not None else bijector_params\n\n    # save the bijector params along with the latent params\n    self._params = (self.latent._params, bijector_params)\n</code></pre>"},{"location":"API/flow/#pzflow.flow.Flow.train","title":"<code>train(inputs, val_set=None, train_weight=None, val_weight=None, epochs=100, batch_size=1024, optimizer=None, loss_fn=None, convolve_errs=False, patience=None, best_params=True, seed=0, verbose=False, progress_bar=False, initial_loss=True)</code>","text":"<p>Trains the normalizing flow on the provided inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataFrame</code> <p>Data on which to train the normalizing flow. Must have columns matching <code>self.data_columns</code>. If training a conditional flow, must also have columns matching <code>self.conditional_columns</code>.</p> required <code>val_set</code> <code>pd.DataFrame; default=None</code> <p>Validation set, of same format as inputs. If provided, validation loss will be calculated at the end of each epoch.</p> <code>None</code> <code>train_weight</code> <code>ndarray</code> <p>Array of weights for each sample in the training set.</p> <code>None</code> <code>val_weight</code> <code>ndarray</code> <p>Array of weights for each sample in the validation set.</p> <code>None</code> <code>epochs</code> <code>int; default=100</code> <p>Number of epochs to train.</p> <code>100</code> <code>batch_size</code> <code>int; default=1024</code> <p>Batch size for training.</p> <code>1024</code> <code>optimizer</code> <code>optax optimizer</code> <p>An optimizer from Optax. default = optax.adam(learning_rate=1e-3) see https://optax.readthedocs.io/en/latest/index.html for more.</p> <code>None</code> <code>loss_fn</code> <code>Callable; optional</code> <p>A function to calculate the loss: <code>loss = loss_fn(params, x, c, w)</code>. If not provided, will be <code>-mean(log_prob)</code>.</p> <code>None</code> <code>convolve_errs</code> <code>bool; default=False</code> <p>Whether to draw new data from the error distributions during each epoch of training. Method will look for error columns in <code>inputs</code>. Error columns must end in <code>_err</code>. E.g. the error column for the variable <code>u</code> must be <code>u_err</code>. Zero error assumed for any missing error columns. The error distribution is set during flow instantiation.</p> <code>False</code> <code>patience</code> <code>int; optional</code> <p>Factor that controls early stopping. Training will stop if the loss doesn't decrease for this number of epochs. Note if a validation set is provided, the validation loss is used.</p> <code>None</code> <code>best_params</code> <code>bool; default=True</code> <p>Whether to use the params from the epoch with the lowest loss. Note if a validation set is provided, the epoch with the lowest validation loss is chosen. If False, the params from the final epoch are saved.</p> <code>True</code> <code>seed</code> <code>int; default=0</code> <p>A random seed to control the batching and the (optional) error sampling and creating the default bijector (the latter only happens if you didn't set up the bijector during Flow instantiation).</p> <code>0</code> <code>verbose</code> <code>bool; default=False</code> <p>If true, print the training loss every 5% of epochs.</p> <code>False</code> <code>progress_bar</code> <code>bool; default=False</code> <p>If true, display a tqdm progress bar during training.</p> <code>False</code> <code>initial_loss</code> <code>bool; default=True</code> <p>If true, start by calculating the initial loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>List of training losses from every epoch. If no val_set provided, these are just training losses. If val_set is provided, then the first element is the list of training losses, while the second is the list of validation losses.</p> Source code in <code>pzflow/flow.py</code> <pre><code>def train(\n    self,\n    inputs: pd.DataFrame,\n    val_set: pd.DataFrame = None,\n    train_weight: np.ndarray = None,\n    val_weight: np.ndarray = None,\n    epochs: int = 100,\n    batch_size: int = 1024,\n    optimizer: Callable = None,\n    loss_fn: Callable = None,\n    convolve_errs: bool = False,\n    patience: int = None,\n    best_params: bool = True,\n    seed: int = 0,\n    verbose: bool = False,\n    progress_bar: bool = False,\n    initial_loss: bool = True,\n) -&gt; list:\n    \"\"\"Trains the normalizing flow on the provided inputs.\n\n    Parameters\n    ----------\n    inputs : pd.DataFrame\n        Data on which to train the normalizing flow.\n        Must have columns matching `self.data_columns`.\n        If training a conditional flow, must also have columns\n        matching `self.conditional_columns`.\n    val_set : pd.DataFrame; default=None\n        Validation set, of same format as inputs. If provided,\n        validation loss will be calculated at the end of each epoch.\n    train_weight: np.ndarray; default=None\n        Array of weights for each sample in the training set.\n    val_weight: np.ndarray; default=None\n        Array of weights for each sample in the validation set.\n    epochs : int; default=100\n        Number of epochs to train.\n    batch_size : int; default=1024\n        Batch size for training.\n    optimizer : optax optimizer\n        An optimizer from Optax. default = optax.adam(learning_rate=1e-3)\n        see https://optax.readthedocs.io/en/latest/index.html for more.\n    loss_fn : Callable; optional\n        A function to calculate the loss: `loss = loss_fn(params, x, c, w)`.\n        If not provided, will be `-mean(log_prob)`.\n    convolve_errs : bool; default=False\n        Whether to draw new data from the error distributions during\n        each epoch of training. Method will look for error columns in\n        `inputs`. Error columns must end in `_err`. E.g. the error column\n        for the variable `u` must be `u_err`. Zero error assumed for\n        any missing error columns. The error distribution is set during\n        flow instantiation.\n    patience : int; optional\n        Factor that controls early stopping. Training will stop if the\n        loss doesn't decrease for this number of epochs. Note if a\n        validation set is provided, the validation loss is used.\n    best_params : bool; default=True\n        Whether to use the params from the epoch with the lowest loss.\n        Note if a validation set is provided, the epoch with the lowest\n        validation loss is chosen. If False, the params from the final\n        epoch are saved.\n    seed : int; default=0\n        A random seed to control the batching and the (optional)\n        error sampling and creating the default bijector (the latter\n        only happens if you didn't set up the bijector during Flow\n        instantiation).\n    verbose : bool; default=False\n        If true, print the training loss every 5% of epochs.\n    progress_bar : bool; default=False\n        If true, display a tqdm progress bar during training.\n    initial_loss : bool; default=True\n        If true, start by calculating the initial loss.\n\n    Returns\n    -------\n    list\n        List of training losses from every epoch. If no val_set provided,\n        these are just training losses. If val_set is provided, then the\n        first element is the list of training losses, while the second is\n        the list of validation losses.\n    \"\"\"\n    # split the seed\n    rng = np.random.default_rng(seed)\n    batch_seed, bijector_seed = rng.integers(1e9, size=2)\n\n    # if the bijector is None, set the default bijector\n    if self._bijector_info is None:\n        self._set_default_bijector(inputs, seed=bijector_seed)\n\n    # validate epochs\n    if not isinstance(epochs, int) or epochs &lt;= 0:\n        raise ValueError(\"epochs must be a positive integer.\")\n\n    # if no loss_fn is provided, use the default loss function\n    if loss_fn is None:\n\n        @jit\n        def loss_fn(params, x, c, w):\n            return -jnp.mean(w * self._log_prob(params, x, c))\n\n    # initialize the optimizer\n    optimizer = (\n        optax.adam(learning_rate=1e-3) if optimizer is None else optimizer\n    )\n    opt_state = optimizer.init(self._params)\n\n    # pull out the model parameters\n    model_params = self._params\n\n    # define the training step function\n    @jit\n    def step(params, opt_state, x, c, w):\n        gradients = grad(loss_fn)(params, x, c, w)\n        updates, opt_state = optimizer.update(gradients, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state\n\n    # get list of data columns\n    columns = list(self.data_columns)\n\n    # if this is a conditional flow, and autoscale_conditions == True\n    # save the means and stds of the conditional columns\n    if self.conditional_columns is not None and self._autoscale_conditions:\n        self._condition_means = jnp.array(\n            inputs[list(self.conditional_columns)].to_numpy().mean(axis=0)\n        )\n        condition_stds = jnp.array(\n            inputs[list(self.conditional_columns)].to_numpy().std(axis=0)\n        )\n        self._condition_stds = jnp.where(\n            condition_stds != 0, condition_stds, 1\n        )\n\n    # define a function to return batches\n    if convolve_errs:\n\n        def get_batch(sample_key, x, type):\n            return self._get_err_samples(sample_key, x, 1, type=type)\n\n    else:\n\n        def get_batch(sample_key, x, type):\n            if type == \"conditions\":\n                return self._get_conditions(x)\n            else:\n                return jnp.array(x[columns].to_numpy())\n\n    # get random seed for training loop\n    key = random.PRNGKey(batch_seed)\n\n    if verbose:\n        print(f\"Training {epochs} epochs \\nLoss:\")\n\n    # save the initial loss\n    W = jnp.ones(len(inputs)) if train_weight is None else train_weight\n    W /= W.mean()\n    if initial_loss:\n        X = jnp.array(inputs[columns].to_numpy())\n        C = self._get_conditions(inputs)\n        losses = [loss_fn(model_params, X, C, W).item()]\n    else:\n        losses = []\n\n    if val_set is not None:\n        Xval = jnp.array(val_set[columns].to_numpy())\n        Cval = self._get_conditions(val_set)\n        Wval = jnp.ones(len(val_set)) if val_weight is None else val_weight\n        Wval /= Wval.mean()\n        if initial_loss:\n            val_losses = [loss_fn(model_params, Xval, Cval, Wval).item()]\n        else:\n            val_losses = []\n\n    if verbose and initial_loss:\n        if val_set is None:\n            print(f\"(0) {losses[-1]:.4f}\")\n        else:\n            print(f\"(0) {losses[-1]:.4f}  {val_losses[-1]:.4f}\")\n\n    # initialize variables for early stopping\n    best_loss = jnp.inf\n    best_param_vals = model_params\n    early_stopping_counter = 0\n\n    # loop through training\n    loop = tqdm(range(epochs)) if progress_bar else range(epochs)\n    for epoch in loop:\n        # new permutation of batches\n        permute_key, sample_key, key = random.split(key, num=3)\n        idx = random.permutation(permute_key, inputs.shape[0])\n        X = inputs.iloc[idx]\n\n        # loop through batches and step optimizer\n        for batch_idx in range(0, len(X), batch_size):\n            # if sampling from the error distribution, this returns a\n            # Gaussian sample of the batch. Else just returns batch as a\n            # jax array\n            batch = get_batch(\n                sample_key,\n                X.iloc[batch_idx : batch_idx + batch_size],\n                type=\"data\",\n            )\n            batch_conditions = get_batch(\n                sample_key,\n                X.iloc[batch_idx : batch_idx + batch_size],\n                type=\"conditions\",\n            )\n            batch_weights = jnp.asarray(\n                W[idx][batch_idx : batch_idx + batch_size]\n            )\n\n            model_params, opt_state = step(\n                model_params,\n                opt_state,\n                batch,\n                batch_conditions,\n                batch_weights\n            )\n\n        # save end-of-epoch training loss\n        losses.append(\n            loss_fn(\n                model_params,\n                jnp.array(X[columns].to_numpy()),\n                self._get_conditions(X),\n                jnp.asarray(W),\n            ).item()\n        )\n\n        # and validation loss\n        if val_set is not None:\n            val_losses.append(loss_fn(model_params, Xval, Cval, Wval).item())\n\n        # if verbose, print current loss\n        if verbose and (\n            epoch % max(int(0.05 * epochs), 1) == 0\n            or (epoch + 1) == epochs\n        ):\n            if val_set is None:\n                print(f\"({epoch+1}) {losses[-1]:.4f}\")\n            else:\n                print(\n                    f\"({epoch+1}) {losses[-1]:.4f}  {val_losses[-1]:.4f}\"\n                )\n\n        # if patience provided, we need to check for early stopping\n        if patience is not None or best_loss:\n            if val_set is None:\n                tracked_losses = losses\n            else:\n                tracked_losses = val_losses\n\n            # if loss didn't improve, increase counter\n            # and check early stopping criterion\n            if tracked_losses[-1] &gt;= best_loss or jnp.isclose(\n                tracked_losses[-1], best_loss\n            ):\n                early_stopping_counter += 1\n\n                # check if the early stopping criterion is met\n                if (\n                    patience is not None\n                    and early_stopping_counter &gt;= patience\n                ):\n                    print(\n                        \"Early stopping criterion is met.\",\n                        f\"Training stopping after epoch {epoch+1}.\",\n                    )\n                    break\n            # if this is the best loss, reset the counter\n            else:\n                best_loss = tracked_losses[-1]\n                best_param_vals = model_params\n                early_stopping_counter = 0\n\n        # break if the training loss is NaN\n        if not np.isfinite(losses[-1]):\n            print(\n                f\"Training stopping after epoch {epoch+1}\",\n                \"because training loss diverged.\",\n            )\n            break\n\n    # update the flow parameters with the final training state\n    if best_params:\n        self._params = best_param_vals\n    else:\n        self._params = model_params\n\n    if val_set is None:\n        return losses\n    else:\n        return [losses, val_losses]\n</code></pre>"},{"location":"API/flowEnsemble/","title":"flowEnsemble","text":"<p>Define FlowEnsemble object that holds an ensemble of normalizing flows.</p>"},{"location":"API/flowEnsemble/#pzflow.flowEnsemble.FlowEnsemble","title":"<code>FlowEnsemble</code>","text":"<p>An ensemble of normalizing flows.</p> <p>Attributes:</p> Name Type Description <code>data_columns</code> <code>tuple</code> <p>List of DataFrame columns that the flows expect/produce.</p> <code>conditional_columns</code> <code>tuple</code> <p>List of DataFrame columns on which the flows are conditioned.</p> <code>latent</code> <code>LatentDist</code> <p>The latent distribution of the normalizing flows. Has it's own sample and log_prob methods.</p> <code>data_error_model</code> <code>Callable</code> <p>The error model for the data variables. See the docstring of init for more details.</p> <code>condition_error_model</code> <code>Callable</code> <p>The error model for the conditional variables. See the docstring of init for more details.</p> <code>info</code> <code>Any</code> <p>Object containing any kind of info included with the ensemble. Often Reverse the data the flows are trained on.</p> Source code in <code>pzflow/flowEnsemble.py</code> <pre><code>class FlowEnsemble:\n    \"\"\"An ensemble of normalizing flows.\n\n    Attributes\n    ----------\n    data_columns : tuple\n        List of DataFrame columns that the flows expect/produce.\n    conditional_columns : tuple\n        List of DataFrame columns on which the flows are conditioned.\n    latent: distributions.LatentDist\n        The latent distribution of the normalizing flows.\n        Has it's own sample and log_prob methods.\n    data_error_model : Callable\n        The error model for the data variables. See the docstring of\n        __init__ for more details.\n    condition_error_model : Callable\n        The error model for the conditional variables. See the docstring\n        of __init__ for more details.\n    info : Any\n        Object containing any kind of info included with the ensemble.\n        Often Reverse the data the flows are trained on.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_columns: Sequence[str] = None,\n        bijector: Tuple[InitFunction, Bijector_Info] = None,\n        latent: distributions.LatentDist = None,\n        conditional_columns: Sequence[str] = None,\n        data_error_model: Callable = None,\n        condition_error_model: Callable = None,\n        autoscale_conditions: bool = True,\n        N: int = 1,\n        info: Any = None,\n        file: str = None,\n    ) -&gt; None:\n        \"\"\"Instantiate an ensemble of normalizing flows.\n\n        Note that while all of the init parameters are technically optional,\n        you must provide either data_columns and bijector OR file.\n        In addition, if a file is provided, all other parameters must be None.\n\n        Parameters\n        ----------\n        data_columns : Sequence[str]; optional\n            Tuple, list, or other container of column names.\n            These are the columns the flows expect/produce in DataFrames.\n        bijector : Bijector Call; optional\n            A Bijector call that consists of the bijector InitFunction that\n            initializes the bijector and the tuple of Bijector Info.\n            Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc.\n            If not provided, the bijector can be set later using\n            flow.set_bijector, or by calling flow.train, in which case the\n            default bijector will be used. The default bijector is\n            ShiftBounds -&gt; RollingSplineCoupling, where the range of shift\n            bounds is learned from the training data, and the dimensions of\n            RollingSplineCoupling is inferred. The default bijector assumes\n            that the latent has support [-5, 5] for every dimension.\n        latent : distributions.LatentDist; optional\n            The latent distribution for the normalizing flow. Can be any of\n            the distributions from pzflow.distributions. If not provided,\n            a uniform distribution is used with input_dim = len(data_columns),\n            and B=5.\n        conditional_columns : Sequence[str]; optional\n            Names of columns on which to condition the normalizing flows.\n        data_error_model : Callable; optional\n            A callable that defines the error model for data variables.\n            data_error_model must take key, X, Xerr, nsamples as arguments:\n                - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n                - X is 2D array of data variables, where the order of variables\n                    matches the order of the columns in data_columns\n                - Xerr is the corresponding 2D array of errors\n                - nsamples is number of samples to draw from error distribution\n            data_error_model must return an array of samples with the shape\n            (X.shape[0], nsamples, X.shape[1]).\n            If data_error_model is not provided, Gaussian error model assumed.\n        condition_error_model : Callable; optional\n            A callable that defines the error model for conditional variables.\n            condition_error_model must take key, X, Xerr, nsamples, where:\n                - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n                - X is 2D array of conditional variables, where the order of\n                    variables matches order of columns in conditional_columns\n                - Xerr is the corresponding 2D array of errors\n                - nsamples is number of samples to draw from error distribution\n            condition_error_model must return array of samples with shape\n            (X.shape[0], nsamples, X.shape[1]).\n            If condition_error_model is not provided, Gaussian error model\n            assumed.\n        autoscale_conditions : bool; default=True\n            Sets whether or not conditions are automatically standard scaled\n            when passed to a conditional flow. I recommend you leave as True.\n        N : int; default=1\n            The number of flows in the ensemble.\n        info : Any; optional\n            An object to attach to the info attribute.\n        file : str; optional\n            Path to file from which to load a pretrained flow ensemble.\n            If a file is provided, all other parameters must be None.\n        \"\"\"\n\n        # validate parameters\n        if data_columns is None and file is None:\n            raise ValueError(\"You must provide data_columns OR file.\")\n        if file is not None and any(\n            (\n                data_columns is not None,\n                bijector is not None,\n                conditional_columns is not None,\n                latent is not None,\n                data_error_model is not None,\n                condition_error_model is not None,\n                info is not None,\n            )\n        ):\n            raise ValueError(\n                \"If providing a file, please do not provide any other parameters.\"\n            )\n\n        # if file is provided, load everything from the file\n        if file is not None:\n            # load the file\n            with open(file, \"rb\") as handle:\n                save_dict = pickle.load(handle)\n\n            # make sure the saved file is for this class\n            c = save_dict.pop(\"class\")\n            if c != self.__class__.__name__:\n                raise TypeError(\n                    f\"This save file isn't a {self.__class__.__name__}. It is a {c}.\"\n                )\n\n            # load the ensemble from the dictionary\n            self._ensemble = {\n                name: Flow(_dictionary=flow_dict)\n                for name, flow_dict in save_dict[\"ensemble\"].items()\n            }\n            # load the metadata\n            self.data_columns = save_dict[\"data_columns\"]\n            self.conditional_columns = save_dict[\"conditional_columns\"]\n            self.data_error_model = save_dict[\"data_error_model\"]\n            self.condition_error_model = save_dict[\"condition_error_model\"]\n            self.info = save_dict[\"info\"]\n\n            self._latent_info = save_dict[\"latent_info\"]\n            self.latent = getattr(distributions, self._latent_info[0])(\n                *self._latent_info[1]\n            )\n\n        # otherwise create a new ensemble from the provided parameters\n        else:\n            # save the ensemble of flows\n            self._ensemble = {\n                f\"Flow {i}\": Flow(\n                    data_columns=data_columns,\n                    bijector=bijector,\n                    conditional_columns=conditional_columns,\n                    latent=latent,\n                    data_error_model=data_error_model,\n                    condition_error_model=condition_error_model,\n                    autoscale_conditions=autoscale_conditions,\n                    seed=i,\n                    info=f\"Flow {i}\",\n                )\n                for i in range(N)\n            }\n            # save the metadata\n            self.data_columns = data_columns\n            self.conditional_columns = conditional_columns\n            self.latent = self._ensemble[\"Flow 0\"].latent\n            self.data_error_model = data_error_model\n            self.condition_error_model = condition_error_model\n            self.info = info\n\n    def log_prob(\n        self,\n        inputs: pd.DataFrame,\n        err_samples: int = None,\n        seed: int = None,\n        returnEnsemble: bool = False,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Calculates log probability density of inputs.\n\n        Parameters\n        ----------\n        inputs : pd.DataFrame\n            Input data for which log probability density is calculated.\n            Every column in self.data_columns must be present.\n            If self.conditional_columns is not None, those must be present\n            as well. If other columns are present, they are ignored.\n        err_samples : int; default=None\n            Number of samples from the error distribution to average over for\n            the log_prob calculation. If provided, Gaussian errors are assumed,\n            and method will look for error columns in `inputs`. Error columns\n            must end in `_err`. E.g. the error column for the variable `u` must\n            be `u_err`. Zero error assumed for any missing error columns.\n        seed : int; default=None\n            Random seed for drawing the samples with Gaussian errors.\n        returnEnsemble : bool; default=False\n            If True, returns log_prob for each flow in the ensemble as an\n            array of shape (inputs.shape[0], N flows in ensemble).\n            If False, the prob is averaged over the flows in the ensemble,\n            and the log of this average is returned as an array of shape\n            (inputs.shape[0],)\n\n        Returns\n        -------\n        jnp.ndarray\n            For shape, see returnEnsemble description above.\n        \"\"\"\n\n        # calculate log_prob for each flow in the ensemble\n        ensemble = jnp.array(\n            [\n                flow.log_prob(inputs, err_samples, seed)\n                for flow in self._ensemble.values()\n            ]\n        )\n\n        # re-arrange so that (axis 0, axis 1) = (inputs, flows in ensemble)\n        ensemble = jnp.rollaxis(ensemble, axis=1)\n\n        if returnEnsemble:\n            # return the ensemble of log_probs\n            return ensemble\n        else:\n            # return mean over ensemble\n            # note we return log(mean prob) instead of just mean log_prob\n            return jnp.log(jnp.exp(ensemble).mean(axis=1))\n\n    def posterior(\n        self,\n        inputs: pd.DataFrame,\n        column: str,\n        grid: jnp.ndarray,\n        marg_rules: dict = None,\n        normalize: bool = True,\n        err_samples: int = None,\n        seed: int = None,\n        batch_size: int = None,\n        returnEnsemble: bool = False,\n        nan_to_zero: bool = True,\n    ) -&gt; jnp.ndarray:\n        \"\"\"Calculates posterior distributions for the provided column.\n\n        Calculates the conditional posterior distribution, assuming the\n        data values in the other columns of the DataFrame.\n\n        Parameters\n        ----------\n        inputs : pd.DataFrame\n            Data on which the posterior distributions are conditioned.\n            Must have columns matching self.data_columns, *except*\n            for the column specified for the posterior (see below).\n        column : str\n            Name of the column for which the posterior distribution\n            is calculated. Must be one of the columns in self.data_columns.\n            However, whether or not this column is one of the columns in\n            `inputs` is irrelevant.\n        grid : jnp.ndarray\n            Grid on which to calculate the posterior.\n        marg_rules : dict; optional\n            Dictionary with rules for marginalizing over missing variables.\n            The dictionary must contain the key \"flag\", which gives the flag\n            that indicates a missing value. E.g. if missing values are given\n            the value 99, the dictionary should contain {\"flag\": 99}.\n            The dictionary must also contain {\"name\": callable} for any\n            variables that will need to be marginalized over, where name is\n            the name of the variable, and callable is a callable that takes\n            the row of variables nad returns a grid over which to marginalize\n            the variable. E.g. {\"y\": lambda row: jnp.linspace(0, row[\"x\"], 10)}.\n            Note: the callable for a given name must *always* return an array\n            of the same length, regardless of the input row.\n        normalize : boolean; default=True\n            Whether to normalize the posterior so that it integrates to 1.\n        err_samples : int; default=None\n            Number of samples from the error distribution to average over for\n            the posterior calculation. If provided, Gaussian errors are assumed,\n            and method will look for error columns in `inputs`. Error columns\n            must end in `_err`. E.g. the error column for the variable `u` must\n            be `u_err`. Zero error assumed for any missing error columns.\n        seed : int; default=None\n            Random seed for drawing the samples with Gaussian errors.\n        batch_size : int; default=None\n            Size of batches in which to calculate posteriors. If None, all\n            posteriors are calculated simultaneously. Simultaneous calculation\n            is faster, but memory intensive for large data sets.\n        returnEnsemble : bool; default=False\n            If True, returns posterior for each flow in the ensemble as an\n            array of shape (inputs.shape[0], N flows in ensemble, grid.size).\n            If False, the posterior is averaged over the flows in the ensemble,\n            and returned as an array of shape (inputs.shape[0], grid.size)\n        nan_to_zero : bool; default=True\n            Whether to convert NaN's to zero probability in the final pdfs.\n\n        Returns\n        -------\n        jnp.ndarray\n            For shape, see returnEnsemble description above.\n        \"\"\"\n\n        # calculate posterior for each flow in the ensemble\n        ensemble = jnp.array(\n            [\n                flow.posterior(\n                    inputs=inputs,\n                    column=column,\n                    grid=grid,\n                    marg_rules=marg_rules,\n                    err_samples=err_samples,\n                    seed=seed,\n                    batch_size=batch_size,\n                    normalize=False,\n                    nan_to_zero=nan_to_zero,\n                )\n                for flow in self._ensemble.values()\n            ]\n        )\n\n        # re-arrange so that (axis 0, axis 1) = (inputs, flows in ensemble)\n        ensemble = jnp.rollaxis(ensemble, axis=1)\n\n        if returnEnsemble:\n            # return the ensemble of posteriors\n            if normalize:\n                ensemble = ensemble.reshape(-1, grid.size)\n                ensemble = ensemble / trapezoid(y=ensemble, x=grid).reshape(\n                    -1, 1\n                )\n                ensemble = ensemble.reshape(inputs.shape[0], -1, grid.size)\n            return ensemble\n        else:\n            # return mean over ensemble\n            pdfs = ensemble.mean(axis=1)\n            if normalize:\n                pdfs = pdfs / trapezoid(y=pdfs, x=grid).reshape(-1, 1)\n            return pdfs\n\n    def sample(\n        self,\n        nsamples: int = 1,\n        conditions: pd.DataFrame = None,\n        save_conditions: bool = True,\n        seed: int = None,\n        returnEnsemble: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Returns samples from the ensemble.\n\n        Parameters\n        ----------\n        nsamples : int; default=1\n            The number of samples to be returned, either overall or per flow\n            in the ensemble (see returnEnsemble below).\n        conditions : pd.DataFrame; optional\n            If this is a conditional flow, you must pass conditions for\n            each sample. nsamples will be drawn for each row in conditions.\n        save_conditions : bool; default=True\n            If true, conditions will be saved in the DataFrame of samples\n            that is returned.\n        seed : int; optional\n            Sets the random seed for the samples.\n        returnEnsemble : bool; default=False\n            If True, nsamples is drawn from each flow in the ensemble.\n            If False, nsamples are drawn uniformly from the flows in the ensemble.\n\n        Returns\n        -------\n        pd.DataFrame\n            Pandas DataFrame of samples.\n        \"\"\"\n\n        if returnEnsemble:\n            # return nsamples for each flow in the ensemble\n            return pd.concat(\n                [\n                    flow.sample(nsamples, conditions, save_conditions, seed)\n                    for flow in self._ensemble.values()\n                ],\n                keys=self._ensemble.keys(),\n            )\n        else:\n            # if this isn't a conditional flow, sampling is straightforward\n            if conditions is None:\n                # return nsamples drawn uniformly from the flows in the ensemble\n                N = int(jnp.ceil(nsamples / len(self._ensemble)))\n                samples = pd.concat(\n                    [\n                        flow.sample(N, conditions, save_conditions, seed)\n                        for flow in self._ensemble.values()\n                    ]\n                )\n                return samples.sample(nsamples, random_state=seed).reset_index(\n                    drop=True\n                )\n            # if this is a conditional flow, it's a little more complicated...\n            else:\n                # if nsamples &gt; 1, we duplicate the rows of the conditions\n                if nsamples &gt; 1:\n                    conditions = pd.concat([conditions] * nsamples)\n\n                # now the main sampling algorithm\n                seed = np.random.randint(1e18) if seed is None else seed\n                # if we are drawing more samples than the number of flows in\n                # the ensemble, then we will shuffle the conditions and randomly\n                # assign them to one of the constituent flows\n                if conditions.shape[0] &gt; len(self._ensemble):\n                    # shuffle the conditions\n                    conditions_shuffled = conditions.sample(\n                        frac=1.0, random_state=int(seed / 1e9)\n                    )\n                    # split conditions into ~equal sized chunks\n                    chunks = np.array_split(\n                        conditions_shuffled, len(self._ensemble)\n                    )\n                    # shuffle the chunks\n                    chunks = [\n                        chunks[i]\n                        for i in random.permutation(\n                            random.PRNGKey(seed), jnp.arange(len(chunks))\n                        )\n                    ]\n                    # sample from each flow, and return all the samples\n                    return pd.concat(\n                        [\n                            flow.sample(\n                                1, chunk, save_conditions, seed\n                            ).set_index(chunk.index)\n                            for flow, chunk in zip(\n                                self._ensemble.values(), chunks\n                            )\n                        ]\n                    ).sort_index()\n                # however, if there are more flows in the ensemble than samples\n                # being drawn, then we will randomly select flows for each condition\n                else:\n                    rng = np.random.default_rng(seed)\n                    # randomly select a flow to sample from for each condition\n                    flows = rng.choice(\n                        list(self._ensemble.values()),\n                        size=conditions.shape[0],\n                        replace=True,\n                    )\n                    # sample from each flow and return all the samples together\n                    seeds = rng.integers(1e18, size=conditions.shape[0])\n                    return pd.concat(\n                        [\n                            flow.sample(\n                                1,\n                                conditions[i : i + 1],\n                                save_conditions,\n                                new_seed,\n                            )\n                            for i, (flow, new_seed) in enumerate(\n                                zip(flows, seeds)\n                            )\n                        ],\n                    ).set_index(conditions.index)\n\n    def save(self, file: str) -&gt; None:\n        \"\"\"Saves the ensemble to a file.\n\n        Pickles the ensemble and saves it to a file that can be passed as\n        the `file` argument during flow instantiation.\n\n        WARNING: Currently, this method only works for bijectors that are\n        implemented in the `bijectors` module. If you want to save a flow\n        with a custom bijector, you either need to add the bijector to that\n        module, or handle the saving and loading on your end.\n\n        Parameters\n        ----------\n        file : str\n            Path to where the ensemble will be saved.\n            Extension `.pkl` will be appended if not already present.\n        \"\"\"\n        save_dict = {\n            \"data_columns\": self.data_columns,\n            \"conditional_columns\": self.conditional_columns,\n            \"latent_info\": self.latent.info,\n            \"data_error_model\": self.data_error_model,\n            \"condition_error_model\": self.condition_error_model,\n            \"info\": self.info,\n            \"class\": self.__class__.__name__,\n            \"ensemble\": {\n                name: flow._save_dict()\n                for name, flow in self._ensemble.items()\n            },\n        }\n\n        with open(file, \"wb\") as handle:\n            pickle.dump(save_dict, handle, recurse=True)\n\n    def train(\n        self,\n        inputs: pd.DataFrame,\n        val_set: pd.DataFrame = None,\n        train_weight: np.ndarray = None,\n        val_weight: np.ndarray = None,\n        epochs: int = 50,\n        batch_size: int = 1024,\n        optimizer: Callable = None,\n        loss_fn: Callable = None,\n        convolve_errs: bool = False,\n        patience: int = None,\n        best_params: bool = True,\n        seed: int = 0,\n        verbose: bool = False,\n        progress_bar: bool = False,\n        initial_loss: bool = True,\n    ) -&gt; dict:\n        \"\"\"Trains the normalizing flows on the provided inputs.\n\n        Parameters\n        ----------\n        inputs : pd.DataFrame\n            Data on which to train the normalizing flows.\n            Must have columns matching self.data_columns.\n        val_set : pd.DataFrame; default=None\n            Validation set, of same format as inputs. If provided,\n            validation loss will be calculated at the end of each epoch.\n        train_weight: np.ndarray; default=None\n            Array of weights for each sample in the training set.\n        val_weight: np.ndarray; default=None\n            Array of weights for each sample in the validation set.\n        epochs : int; default=50\n            Number of epochs to train.\n        batch_size : int; default=1024\n            Batch size for training.\n        optimizer : optax optimizer\n            An optimizer from Optax. default = optax.adam(learning_rate=1e-3)\n            see https://optax.readthedocs.io/en/latest/index.html for more.\n        loss_fn : Callable; optional\n            A function to calculate the loss: loss = loss_fn(params, x).\n            If not provided, will be -mean(log_prob).\n        convolve_errs : bool; default=False\n            Whether to draw new data from the error distributions during\n            each epoch of training. Method will look for error columns in\n            `inputs`. Error columns must end in `_err`. E.g. the error column\n            for the variable `u` must be `u_err`. Zero error assumed for\n            any missing error columns. The error distribution is set during\n            ensemble instantiation.\n        patience : int; optional\n            Factor that controls early stopping. Training will stop if the\n            loss doesn't decrease for this number of epochs.\n        best_params : bool; default=True\n            Whether to use the params from the epoch with the lowest loss.\n            Note if a validation set is provided, the epoch with the lowest\n            validation loss is chosen. If False, the params from the final\n            epoch are saved.\n        seed : int; default=0\n            A random seed to control the batching and the (optional)\n            error sampling.\n        verbose : bool; default=False\n            If true, print the training loss every 5% of epochs.\n        progress_bar : bool; default=False\n            If true, display a tqdm progress bar during training.\n        initial_loss : bool; default=True\n            If true, start by calculating the initial loss.\n\n        Returns\n        -------\n        dict\n            Dictionary of training losses from every epoch for each flow\n            in the ensemble.\n        \"\"\"\n\n        # generate random seeds for each flow\n        rng = np.random.default_rng(seed)\n        seeds = rng.integers(1e9, size=len(self._ensemble))\n\n        loss_dict = dict()\n\n        for i, (name, flow) in enumerate(self._ensemble.items()):\n            if verbose:\n                print(name)\n\n            loss_dict[name] = flow.train(\n                inputs=inputs,\n                val_set=val_set,\n                train_weight=train_weight,\n                val_weight=val_weight,\n                epochs=epochs,\n                batch_size=batch_size,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                convolve_errs=convolve_errs,\n                patience=patience,\n                best_params=best_params,\n                seed=seeds[i],\n                verbose=verbose,\n                progress_bar=progress_bar,\n                initial_loss=initial_loss,\n            )\n\n        return loss_dict\n</code></pre>"},{"location":"API/flowEnsemble/#pzflow.flowEnsemble.FlowEnsemble.__init__","title":"<code>__init__(data_columns=None, bijector=None, latent=None, conditional_columns=None, data_error_model=None, condition_error_model=None, autoscale_conditions=True, N=1, info=None, file=None)</code>","text":"<p>Instantiate an ensemble of normalizing flows.</p> <p>Note that while all of the init parameters are technically optional, you must provide either data_columns and bijector OR file. In addition, if a file is provided, all other parameters must be None.</p> <p>Parameters:</p> Name Type Description Default <code>data_columns</code> <code>Sequence[str]; optional</code> <p>Tuple, list, or other container of column names. These are the columns the flows expect/produce in DataFrames.</p> <code>None</code> <code>bijector</code> <code>Bijector Call; optional</code> <p>A Bijector call that consists of the bijector InitFunction that initializes the bijector and the tuple of Bijector Info. Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc. If not provided, the bijector can be set later using flow.set_bijector, or by calling flow.train, in which case the default bijector will be used. The default bijector is ShiftBounds -&gt; RollingSplineCoupling, where the range of shift bounds is learned from the training data, and the dimensions of RollingSplineCoupling is inferred. The default bijector assumes that the latent has support [-5, 5] for every dimension.</p> <code>None</code> <code>latent</code> <code>distributions.LatentDist; optional</code> <p>The latent distribution for the normalizing flow. Can be any of the distributions from pzflow.distributions. If not provided, a uniform distribution is used with input_dim = len(data_columns), and B=5.</p> <code>None</code> <code>conditional_columns</code> <code>Sequence[str]; optional</code> <p>Names of columns on which to condition the normalizing flows.</p> <code>None</code> <code>data_error_model</code> <code>Callable; optional</code> <p>A callable that defines the error model for data variables. data_error_model must take key, X, Xerr, nsamples as arguments:     - key is a jax rng key, e.g. jax.random.PRNGKey(0)     - X is 2D array of data variables, where the order of variables         matches the order of the columns in data_columns     - Xerr is the corresponding 2D array of errors     - nsamples is number of samples to draw from error distribution data_error_model must return an array of samples with the shape (X.shape[0], nsamples, X.shape[1]). If data_error_model is not provided, Gaussian error model assumed.</p> <code>None</code> <code>condition_error_model</code> <code>Callable; optional</code> <p>A callable that defines the error model for conditional variables. condition_error_model must take key, X, Xerr, nsamples, where:     - key is a jax rng key, e.g. jax.random.PRNGKey(0)     - X is 2D array of conditional variables, where the order of         variables matches order of columns in conditional_columns     - Xerr is the corresponding 2D array of errors     - nsamples is number of samples to draw from error distribution condition_error_model must return array of samples with shape (X.shape[0], nsamples, X.shape[1]). If condition_error_model is not provided, Gaussian error model assumed.</p> <code>None</code> <code>autoscale_conditions</code> <code>bool; default=True</code> <p>Sets whether or not conditions are automatically standard scaled when passed to a conditional flow. I recommend you leave as True.</p> <code>True</code> <code>N</code> <code>int; default=1</code> <p>The number of flows in the ensemble.</p> <code>1</code> <code>info</code> <code>Any; optional</code> <p>An object to attach to the info attribute.</p> <code>None</code> <code>file</code> <code>str; optional</code> <p>Path to file from which to load a pretrained flow ensemble. If a file is provided, all other parameters must be None.</p> <code>None</code> Source code in <code>pzflow/flowEnsemble.py</code> <pre><code>def __init__(\n    self,\n    data_columns: Sequence[str] = None,\n    bijector: Tuple[InitFunction, Bijector_Info] = None,\n    latent: distributions.LatentDist = None,\n    conditional_columns: Sequence[str] = None,\n    data_error_model: Callable = None,\n    condition_error_model: Callable = None,\n    autoscale_conditions: bool = True,\n    N: int = 1,\n    info: Any = None,\n    file: str = None,\n) -&gt; None:\n    \"\"\"Instantiate an ensemble of normalizing flows.\n\n    Note that while all of the init parameters are technically optional,\n    you must provide either data_columns and bijector OR file.\n    In addition, if a file is provided, all other parameters must be None.\n\n    Parameters\n    ----------\n    data_columns : Sequence[str]; optional\n        Tuple, list, or other container of column names.\n        These are the columns the flows expect/produce in DataFrames.\n    bijector : Bijector Call; optional\n        A Bijector call that consists of the bijector InitFunction that\n        initializes the bijector and the tuple of Bijector Info.\n        Can be the output of any Bijector, e.g. Reverse(), Chain(...), etc.\n        If not provided, the bijector can be set later using\n        flow.set_bijector, or by calling flow.train, in which case the\n        default bijector will be used. The default bijector is\n        ShiftBounds -&gt; RollingSplineCoupling, where the range of shift\n        bounds is learned from the training data, and the dimensions of\n        RollingSplineCoupling is inferred. The default bijector assumes\n        that the latent has support [-5, 5] for every dimension.\n    latent : distributions.LatentDist; optional\n        The latent distribution for the normalizing flow. Can be any of\n        the distributions from pzflow.distributions. If not provided,\n        a uniform distribution is used with input_dim = len(data_columns),\n        and B=5.\n    conditional_columns : Sequence[str]; optional\n        Names of columns on which to condition the normalizing flows.\n    data_error_model : Callable; optional\n        A callable that defines the error model for data variables.\n        data_error_model must take key, X, Xerr, nsamples as arguments:\n            - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n            - X is 2D array of data variables, where the order of variables\n                matches the order of the columns in data_columns\n            - Xerr is the corresponding 2D array of errors\n            - nsamples is number of samples to draw from error distribution\n        data_error_model must return an array of samples with the shape\n        (X.shape[0], nsamples, X.shape[1]).\n        If data_error_model is not provided, Gaussian error model assumed.\n    condition_error_model : Callable; optional\n        A callable that defines the error model for conditional variables.\n        condition_error_model must take key, X, Xerr, nsamples, where:\n            - key is a jax rng key, e.g. jax.random.PRNGKey(0)\n            - X is 2D array of conditional variables, where the order of\n                variables matches order of columns in conditional_columns\n            - Xerr is the corresponding 2D array of errors\n            - nsamples is number of samples to draw from error distribution\n        condition_error_model must return array of samples with shape\n        (X.shape[0], nsamples, X.shape[1]).\n        If condition_error_model is not provided, Gaussian error model\n        assumed.\n    autoscale_conditions : bool; default=True\n        Sets whether or not conditions are automatically standard scaled\n        when passed to a conditional flow. I recommend you leave as True.\n    N : int; default=1\n        The number of flows in the ensemble.\n    info : Any; optional\n        An object to attach to the info attribute.\n    file : str; optional\n        Path to file from which to load a pretrained flow ensemble.\n        If a file is provided, all other parameters must be None.\n    \"\"\"\n\n    # validate parameters\n    if data_columns is None and file is None:\n        raise ValueError(\"You must provide data_columns OR file.\")\n    if file is not None and any(\n        (\n            data_columns is not None,\n            bijector is not None,\n            conditional_columns is not None,\n            latent is not None,\n            data_error_model is not None,\n            condition_error_model is not None,\n            info is not None,\n        )\n    ):\n        raise ValueError(\n            \"If providing a file, please do not provide any other parameters.\"\n        )\n\n    # if file is provided, load everything from the file\n    if file is not None:\n        # load the file\n        with open(file, \"rb\") as handle:\n            save_dict = pickle.load(handle)\n\n        # make sure the saved file is for this class\n        c = save_dict.pop(\"class\")\n        if c != self.__class__.__name__:\n            raise TypeError(\n                f\"This save file isn't a {self.__class__.__name__}. It is a {c}.\"\n            )\n\n        # load the ensemble from the dictionary\n        self._ensemble = {\n            name: Flow(_dictionary=flow_dict)\n            for name, flow_dict in save_dict[\"ensemble\"].items()\n        }\n        # load the metadata\n        self.data_columns = save_dict[\"data_columns\"]\n        self.conditional_columns = save_dict[\"conditional_columns\"]\n        self.data_error_model = save_dict[\"data_error_model\"]\n        self.condition_error_model = save_dict[\"condition_error_model\"]\n        self.info = save_dict[\"info\"]\n\n        self._latent_info = save_dict[\"latent_info\"]\n        self.latent = getattr(distributions, self._latent_info[0])(\n            *self._latent_info[1]\n        )\n\n    # otherwise create a new ensemble from the provided parameters\n    else:\n        # save the ensemble of flows\n        self._ensemble = {\n            f\"Flow {i}\": Flow(\n                data_columns=data_columns,\n                bijector=bijector,\n                conditional_columns=conditional_columns,\n                latent=latent,\n                data_error_model=data_error_model,\n                condition_error_model=condition_error_model,\n                autoscale_conditions=autoscale_conditions,\n                seed=i,\n                info=f\"Flow {i}\",\n            )\n            for i in range(N)\n        }\n        # save the metadata\n        self.data_columns = data_columns\n        self.conditional_columns = conditional_columns\n        self.latent = self._ensemble[\"Flow 0\"].latent\n        self.data_error_model = data_error_model\n        self.condition_error_model = condition_error_model\n        self.info = info\n</code></pre>"},{"location":"API/flowEnsemble/#pzflow.flowEnsemble.FlowEnsemble.log_prob","title":"<code>log_prob(inputs, err_samples=None, seed=None, returnEnsemble=False)</code>","text":"<p>Calculates log probability density of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataFrame</code> <p>Input data for which log probability density is calculated. Every column in self.data_columns must be present. If self.conditional_columns is not None, those must be present as well. If other columns are present, they are ignored.</p> required <code>err_samples</code> <code>int; default=None</code> <p>Number of samples from the error distribution to average over for the log_prob calculation. If provided, Gaussian errors are assumed, and method will look for error columns in <code>inputs</code>. Error columns must end in <code>_err</code>. E.g. the error column for the variable <code>u</code> must be <code>u_err</code>. Zero error assumed for any missing error columns.</p> <code>None</code> <code>seed</code> <code>int; default=None</code> <p>Random seed for drawing the samples with Gaussian errors.</p> <code>None</code> <code>returnEnsemble</code> <code>bool; default=False</code> <p>If True, returns log_prob for each flow in the ensemble as an array of shape (inputs.shape[0], N flows in ensemble). If False, the prob is averaged over the flows in the ensemble, and the log of this average is returned as an array of shape (inputs.shape[0],)</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>For shape, see returnEnsemble description above.</p> Source code in <code>pzflow/flowEnsemble.py</code> <pre><code>def log_prob(\n    self,\n    inputs: pd.DataFrame,\n    err_samples: int = None,\n    seed: int = None,\n    returnEnsemble: bool = False,\n) -&gt; jnp.ndarray:\n    \"\"\"Calculates log probability density of inputs.\n\n    Parameters\n    ----------\n    inputs : pd.DataFrame\n        Input data for which log probability density is calculated.\n        Every column in self.data_columns must be present.\n        If self.conditional_columns is not None, those must be present\n        as well. If other columns are present, they are ignored.\n    err_samples : int; default=None\n        Number of samples from the error distribution to average over for\n        the log_prob calculation. If provided, Gaussian errors are assumed,\n        and method will look for error columns in `inputs`. Error columns\n        must end in `_err`. E.g. the error column for the variable `u` must\n        be `u_err`. Zero error assumed for any missing error columns.\n    seed : int; default=None\n        Random seed for drawing the samples with Gaussian errors.\n    returnEnsemble : bool; default=False\n        If True, returns log_prob for each flow in the ensemble as an\n        array of shape (inputs.shape[0], N flows in ensemble).\n        If False, the prob is averaged over the flows in the ensemble,\n        and the log of this average is returned as an array of shape\n        (inputs.shape[0],)\n\n    Returns\n    -------\n    jnp.ndarray\n        For shape, see returnEnsemble description above.\n    \"\"\"\n\n    # calculate log_prob for each flow in the ensemble\n    ensemble = jnp.array(\n        [\n            flow.log_prob(inputs, err_samples, seed)\n            for flow in self._ensemble.values()\n        ]\n    )\n\n    # re-arrange so that (axis 0, axis 1) = (inputs, flows in ensemble)\n    ensemble = jnp.rollaxis(ensemble, axis=1)\n\n    if returnEnsemble:\n        # return the ensemble of log_probs\n        return ensemble\n    else:\n        # return mean over ensemble\n        # note we return log(mean prob) instead of just mean log_prob\n        return jnp.log(jnp.exp(ensemble).mean(axis=1))\n</code></pre>"},{"location":"API/flowEnsemble/#pzflow.flowEnsemble.FlowEnsemble.posterior","title":"<code>posterior(inputs, column, grid, marg_rules=None, normalize=True, err_samples=None, seed=None, batch_size=None, returnEnsemble=False, nan_to_zero=True)</code>","text":"<p>Calculates posterior distributions for the provided column.</p> <p>Calculates the conditional posterior distribution, assuming the data values in the other columns of the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataFrame</code> <p>Data on which the posterior distributions are conditioned. Must have columns matching self.data_columns, except for the column specified for the posterior (see below).</p> required <code>column</code> <code>str</code> <p>Name of the column for which the posterior distribution is calculated. Must be one of the columns in self.data_columns. However, whether or not this column is one of the columns in <code>inputs</code> is irrelevant.</p> required <code>grid</code> <code>ndarray</code> <p>Grid on which to calculate the posterior.</p> required <code>marg_rules</code> <code>dict; optional</code> <p>Dictionary with rules for marginalizing over missing variables. The dictionary must contain the key \"flag\", which gives the flag that indicates a missing value. E.g. if missing values are given the value 99, the dictionary should contain {\"flag\": 99}. The dictionary must also contain {\"name\": callable} for any variables that will need to be marginalized over, where name is the name of the variable, and callable is a callable that takes the row of variables nad returns a grid over which to marginalize the variable. E.g. {\"y\": lambda row: jnp.linspace(0, row[\"x\"], 10)}. Note: the callable for a given name must always return an array of the same length, regardless of the input row.</p> <code>None</code> <code>normalize</code> <code>boolean; default=True</code> <p>Whether to normalize the posterior so that it integrates to 1.</p> <code>True</code> <code>err_samples</code> <code>int; default=None</code> <p>Number of samples from the error distribution to average over for the posterior calculation. If provided, Gaussian errors are assumed, and method will look for error columns in <code>inputs</code>. Error columns must end in <code>_err</code>. E.g. the error column for the variable <code>u</code> must be <code>u_err</code>. Zero error assumed for any missing error columns.</p> <code>None</code> <code>seed</code> <code>int; default=None</code> <p>Random seed for drawing the samples with Gaussian errors.</p> <code>None</code> <code>batch_size</code> <code>int; default=None</code> <p>Size of batches in which to calculate posteriors. If None, all posteriors are calculated simultaneously. Simultaneous calculation is faster, but memory intensive for large data sets.</p> <code>None</code> <code>returnEnsemble</code> <code>bool; default=False</code> <p>If True, returns posterior for each flow in the ensemble as an array of shape (inputs.shape[0], N flows in ensemble, grid.size). If False, the posterior is averaged over the flows in the ensemble, and returned as an array of shape (inputs.shape[0], grid.size)</p> <code>False</code> <code>nan_to_zero</code> <code>bool; default=True</code> <p>Whether to convert NaN's to zero probability in the final pdfs.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>For shape, see returnEnsemble description above.</p> Source code in <code>pzflow/flowEnsemble.py</code> <pre><code>def posterior(\n    self,\n    inputs: pd.DataFrame,\n    column: str,\n    grid: jnp.ndarray,\n    marg_rules: dict = None,\n    normalize: bool = True,\n    err_samples: int = None,\n    seed: int = None,\n    batch_size: int = None,\n    returnEnsemble: bool = False,\n    nan_to_zero: bool = True,\n) -&gt; jnp.ndarray:\n    \"\"\"Calculates posterior distributions for the provided column.\n\n    Calculates the conditional posterior distribution, assuming the\n    data values in the other columns of the DataFrame.\n\n    Parameters\n    ----------\n    inputs : pd.DataFrame\n        Data on which the posterior distributions are conditioned.\n        Must have columns matching self.data_columns, *except*\n        for the column specified for the posterior (see below).\n    column : str\n        Name of the column for which the posterior distribution\n        is calculated. Must be one of the columns in self.data_columns.\n        However, whether or not this column is one of the columns in\n        `inputs` is irrelevant.\n    grid : jnp.ndarray\n        Grid on which to calculate the posterior.\n    marg_rules : dict; optional\n        Dictionary with rules for marginalizing over missing variables.\n        The dictionary must contain the key \"flag\", which gives the flag\n        that indicates a missing value. E.g. if missing values are given\n        the value 99, the dictionary should contain {\"flag\": 99}.\n        The dictionary must also contain {\"name\": callable} for any\n        variables that will need to be marginalized over, where name is\n        the name of the variable, and callable is a callable that takes\n        the row of variables nad returns a grid over which to marginalize\n        the variable. E.g. {\"y\": lambda row: jnp.linspace(0, row[\"x\"], 10)}.\n        Note: the callable for a given name must *always* return an array\n        of the same length, regardless of the input row.\n    normalize : boolean; default=True\n        Whether to normalize the posterior so that it integrates to 1.\n    err_samples : int; default=None\n        Number of samples from the error distribution to average over for\n        the posterior calculation. If provided, Gaussian errors are assumed,\n        and method will look for error columns in `inputs`. Error columns\n        must end in `_err`. E.g. the error column for the variable `u` must\n        be `u_err`. Zero error assumed for any missing error columns.\n    seed : int; default=None\n        Random seed for drawing the samples with Gaussian errors.\n    batch_size : int; default=None\n        Size of batches in which to calculate posteriors. If None, all\n        posteriors are calculated simultaneously. Simultaneous calculation\n        is faster, but memory intensive for large data sets.\n    returnEnsemble : bool; default=False\n        If True, returns posterior for each flow in the ensemble as an\n        array of shape (inputs.shape[0], N flows in ensemble, grid.size).\n        If False, the posterior is averaged over the flows in the ensemble,\n        and returned as an array of shape (inputs.shape[0], grid.size)\n    nan_to_zero : bool; default=True\n        Whether to convert NaN's to zero probability in the final pdfs.\n\n    Returns\n    -------\n    jnp.ndarray\n        For shape, see returnEnsemble description above.\n    \"\"\"\n\n    # calculate posterior for each flow in the ensemble\n    ensemble = jnp.array(\n        [\n            flow.posterior(\n                inputs=inputs,\n                column=column,\n                grid=grid,\n                marg_rules=marg_rules,\n                err_samples=err_samples,\n                seed=seed,\n                batch_size=batch_size,\n                normalize=False,\n                nan_to_zero=nan_to_zero,\n            )\n            for flow in self._ensemble.values()\n        ]\n    )\n\n    # re-arrange so that (axis 0, axis 1) = (inputs, flows in ensemble)\n    ensemble = jnp.rollaxis(ensemble, axis=1)\n\n    if returnEnsemble:\n        # return the ensemble of posteriors\n        if normalize:\n            ensemble = ensemble.reshape(-1, grid.size)\n            ensemble = ensemble / trapezoid(y=ensemble, x=grid).reshape(\n                -1, 1\n            )\n            ensemble = ensemble.reshape(inputs.shape[0], -1, grid.size)\n        return ensemble\n    else:\n        # return mean over ensemble\n        pdfs = ensemble.mean(axis=1)\n        if normalize:\n            pdfs = pdfs / trapezoid(y=pdfs, x=grid).reshape(-1, 1)\n        return pdfs\n</code></pre>"},{"location":"API/flowEnsemble/#pzflow.flowEnsemble.FlowEnsemble.sample","title":"<code>sample(nsamples=1, conditions=None, save_conditions=True, seed=None, returnEnsemble=False)</code>","text":"<p>Returns samples from the ensemble.</p> <p>Parameters:</p> Name Type Description Default <code>nsamples</code> <code>int; default=1</code> <p>The number of samples to be returned, either overall or per flow in the ensemble (see returnEnsemble below).</p> <code>1</code> <code>conditions</code> <code>pd.DataFrame; optional</code> <p>If this is a conditional flow, you must pass conditions for each sample. nsamples will be drawn for each row in conditions.</p> <code>None</code> <code>save_conditions</code> <code>bool; default=True</code> <p>If true, conditions will be saved in the DataFrame of samples that is returned.</p> <code>True</code> <code>seed</code> <code>int; optional</code> <p>Sets the random seed for the samples.</p> <code>None</code> <code>returnEnsemble</code> <code>bool; default=False</code> <p>If True, nsamples is drawn from each flow in the ensemble. If False, nsamples are drawn uniformly from the flows in the ensemble.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame of samples.</p> Source code in <code>pzflow/flowEnsemble.py</code> <pre><code>def sample(\n    self,\n    nsamples: int = 1,\n    conditions: pd.DataFrame = None,\n    save_conditions: bool = True,\n    seed: int = None,\n    returnEnsemble: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Returns samples from the ensemble.\n\n    Parameters\n    ----------\n    nsamples : int; default=1\n        The number of samples to be returned, either overall or per flow\n        in the ensemble (see returnEnsemble below).\n    conditions : pd.DataFrame; optional\n        If this is a conditional flow, you must pass conditions for\n        each sample. nsamples will be drawn for each row in conditions.\n    save_conditions : bool; default=True\n        If true, conditions will be saved in the DataFrame of samples\n        that is returned.\n    seed : int; optional\n        Sets the random seed for the samples.\n    returnEnsemble : bool; default=False\n        If True, nsamples is drawn from each flow in the ensemble.\n        If False, nsamples are drawn uniformly from the flows in the ensemble.\n\n    Returns\n    -------\n    pd.DataFrame\n        Pandas DataFrame of samples.\n    \"\"\"\n\n    if returnEnsemble:\n        # return nsamples for each flow in the ensemble\n        return pd.concat(\n            [\n                flow.sample(nsamples, conditions, save_conditions, seed)\n                for flow in self._ensemble.values()\n            ],\n            keys=self._ensemble.keys(),\n        )\n    else:\n        # if this isn't a conditional flow, sampling is straightforward\n        if conditions is None:\n            # return nsamples drawn uniformly from the flows in the ensemble\n            N = int(jnp.ceil(nsamples / len(self._ensemble)))\n            samples = pd.concat(\n                [\n                    flow.sample(N, conditions, save_conditions, seed)\n                    for flow in self._ensemble.values()\n                ]\n            )\n            return samples.sample(nsamples, random_state=seed).reset_index(\n                drop=True\n            )\n        # if this is a conditional flow, it's a little more complicated...\n        else:\n            # if nsamples &gt; 1, we duplicate the rows of the conditions\n            if nsamples &gt; 1:\n                conditions = pd.concat([conditions] * nsamples)\n\n            # now the main sampling algorithm\n            seed = np.random.randint(1e18) if seed is None else seed\n            # if we are drawing more samples than the number of flows in\n            # the ensemble, then we will shuffle the conditions and randomly\n            # assign them to one of the constituent flows\n            if conditions.shape[0] &gt; len(self._ensemble):\n                # shuffle the conditions\n                conditions_shuffled = conditions.sample(\n                    frac=1.0, random_state=int(seed / 1e9)\n                )\n                # split conditions into ~equal sized chunks\n                chunks = np.array_split(\n                    conditions_shuffled, len(self._ensemble)\n                )\n                # shuffle the chunks\n                chunks = [\n                    chunks[i]\n                    for i in random.permutation(\n                        random.PRNGKey(seed), jnp.arange(len(chunks))\n                    )\n                ]\n                # sample from each flow, and return all the samples\n                return pd.concat(\n                    [\n                        flow.sample(\n                            1, chunk, save_conditions, seed\n                        ).set_index(chunk.index)\n                        for flow, chunk in zip(\n                            self._ensemble.values(), chunks\n                        )\n                    ]\n                ).sort_index()\n            # however, if there are more flows in the ensemble than samples\n            # being drawn, then we will randomly select flows for each condition\n            else:\n                rng = np.random.default_rng(seed)\n                # randomly select a flow to sample from for each condition\n                flows = rng.choice(\n                    list(self._ensemble.values()),\n                    size=conditions.shape[0],\n                    replace=True,\n                )\n                # sample from each flow and return all the samples together\n                seeds = rng.integers(1e18, size=conditions.shape[0])\n                return pd.concat(\n                    [\n                        flow.sample(\n                            1,\n                            conditions[i : i + 1],\n                            save_conditions,\n                            new_seed,\n                        )\n                        for i, (flow, new_seed) in enumerate(\n                            zip(flows, seeds)\n                        )\n                    ],\n                ).set_index(conditions.index)\n</code></pre>"},{"location":"API/flowEnsemble/#pzflow.flowEnsemble.FlowEnsemble.save","title":"<code>save(file)</code>","text":"<p>Saves the ensemble to a file.</p> <p>Pickles the ensemble and saves it to a file that can be passed as the <code>file</code> argument during flow instantiation.</p> <p>WARNING: Currently, this method only works for bijectors that are implemented in the <code>bijectors</code> module. If you want to save a flow with a custom bijector, you either need to add the bijector to that module, or handle the saving and loading on your end.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>Path to where the ensemble will be saved. Extension <code>.pkl</code> will be appended if not already present.</p> required Source code in <code>pzflow/flowEnsemble.py</code> <pre><code>def save(self, file: str) -&gt; None:\n    \"\"\"Saves the ensemble to a file.\n\n    Pickles the ensemble and saves it to a file that can be passed as\n    the `file` argument during flow instantiation.\n\n    WARNING: Currently, this method only works for bijectors that are\n    implemented in the `bijectors` module. If you want to save a flow\n    with a custom bijector, you either need to add the bijector to that\n    module, or handle the saving and loading on your end.\n\n    Parameters\n    ----------\n    file : str\n        Path to where the ensemble will be saved.\n        Extension `.pkl` will be appended if not already present.\n    \"\"\"\n    save_dict = {\n        \"data_columns\": self.data_columns,\n        \"conditional_columns\": self.conditional_columns,\n        \"latent_info\": self.latent.info,\n        \"data_error_model\": self.data_error_model,\n        \"condition_error_model\": self.condition_error_model,\n        \"info\": self.info,\n        \"class\": self.__class__.__name__,\n        \"ensemble\": {\n            name: flow._save_dict()\n            for name, flow in self._ensemble.items()\n        },\n    }\n\n    with open(file, \"wb\") as handle:\n        pickle.dump(save_dict, handle, recurse=True)\n</code></pre>"},{"location":"API/flowEnsemble/#pzflow.flowEnsemble.FlowEnsemble.train","title":"<code>train(inputs, val_set=None, train_weight=None, val_weight=None, epochs=50, batch_size=1024, optimizer=None, loss_fn=None, convolve_errs=False, patience=None, best_params=True, seed=0, verbose=False, progress_bar=False, initial_loss=True)</code>","text":"<p>Trains the normalizing flows on the provided inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataFrame</code> <p>Data on which to train the normalizing flows. Must have columns matching self.data_columns.</p> required <code>val_set</code> <code>pd.DataFrame; default=None</code> <p>Validation set, of same format as inputs. If provided, validation loss will be calculated at the end of each epoch.</p> <code>None</code> <code>train_weight</code> <code>ndarray</code> <p>Array of weights for each sample in the training set.</p> <code>None</code> <code>val_weight</code> <code>ndarray</code> <p>Array of weights for each sample in the validation set.</p> <code>None</code> <code>epochs</code> <code>int; default=50</code> <p>Number of epochs to train.</p> <code>50</code> <code>batch_size</code> <code>int; default=1024</code> <p>Batch size for training.</p> <code>1024</code> <code>optimizer</code> <code>optax optimizer</code> <p>An optimizer from Optax. default = optax.adam(learning_rate=1e-3) see https://optax.readthedocs.io/en/latest/index.html for more.</p> <code>None</code> <code>loss_fn</code> <code>Callable; optional</code> <p>A function to calculate the loss: loss = loss_fn(params, x). If not provided, will be -mean(log_prob).</p> <code>None</code> <code>convolve_errs</code> <code>bool; default=False</code> <p>Whether to draw new data from the error distributions during each epoch of training. Method will look for error columns in <code>inputs</code>. Error columns must end in <code>_err</code>. E.g. the error column for the variable <code>u</code> must be <code>u_err</code>. Zero error assumed for any missing error columns. The error distribution is set during ensemble instantiation.</p> <code>False</code> <code>patience</code> <code>int; optional</code> <p>Factor that controls early stopping. Training will stop if the loss doesn't decrease for this number of epochs.</p> <code>None</code> <code>best_params</code> <code>bool; default=True</code> <p>Whether to use the params from the epoch with the lowest loss. Note if a validation set is provided, the epoch with the lowest validation loss is chosen. If False, the params from the final epoch are saved.</p> <code>True</code> <code>seed</code> <code>int; default=0</code> <p>A random seed to control the batching and the (optional) error sampling.</p> <code>0</code> <code>verbose</code> <code>bool; default=False</code> <p>If true, print the training loss every 5% of epochs.</p> <code>False</code> <code>progress_bar</code> <code>bool; default=False</code> <p>If true, display a tqdm progress bar during training.</p> <code>False</code> <code>initial_loss</code> <code>bool; default=True</code> <p>If true, start by calculating the initial loss.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of training losses from every epoch for each flow in the ensemble.</p> Source code in <code>pzflow/flowEnsemble.py</code> <pre><code>def train(\n    self,\n    inputs: pd.DataFrame,\n    val_set: pd.DataFrame = None,\n    train_weight: np.ndarray = None,\n    val_weight: np.ndarray = None,\n    epochs: int = 50,\n    batch_size: int = 1024,\n    optimizer: Callable = None,\n    loss_fn: Callable = None,\n    convolve_errs: bool = False,\n    patience: int = None,\n    best_params: bool = True,\n    seed: int = 0,\n    verbose: bool = False,\n    progress_bar: bool = False,\n    initial_loss: bool = True,\n) -&gt; dict:\n    \"\"\"Trains the normalizing flows on the provided inputs.\n\n    Parameters\n    ----------\n    inputs : pd.DataFrame\n        Data on which to train the normalizing flows.\n        Must have columns matching self.data_columns.\n    val_set : pd.DataFrame; default=None\n        Validation set, of same format as inputs. If provided,\n        validation loss will be calculated at the end of each epoch.\n    train_weight: np.ndarray; default=None\n        Array of weights for each sample in the training set.\n    val_weight: np.ndarray; default=None\n        Array of weights for each sample in the validation set.\n    epochs : int; default=50\n        Number of epochs to train.\n    batch_size : int; default=1024\n        Batch size for training.\n    optimizer : optax optimizer\n        An optimizer from Optax. default = optax.adam(learning_rate=1e-3)\n        see https://optax.readthedocs.io/en/latest/index.html for more.\n    loss_fn : Callable; optional\n        A function to calculate the loss: loss = loss_fn(params, x).\n        If not provided, will be -mean(log_prob).\n    convolve_errs : bool; default=False\n        Whether to draw new data from the error distributions during\n        each epoch of training. Method will look for error columns in\n        `inputs`. Error columns must end in `_err`. E.g. the error column\n        for the variable `u` must be `u_err`. Zero error assumed for\n        any missing error columns. The error distribution is set during\n        ensemble instantiation.\n    patience : int; optional\n        Factor that controls early stopping. Training will stop if the\n        loss doesn't decrease for this number of epochs.\n    best_params : bool; default=True\n        Whether to use the params from the epoch with the lowest loss.\n        Note if a validation set is provided, the epoch with the lowest\n        validation loss is chosen. If False, the params from the final\n        epoch are saved.\n    seed : int; default=0\n        A random seed to control the batching and the (optional)\n        error sampling.\n    verbose : bool; default=False\n        If true, print the training loss every 5% of epochs.\n    progress_bar : bool; default=False\n        If true, display a tqdm progress bar during training.\n    initial_loss : bool; default=True\n        If true, start by calculating the initial loss.\n\n    Returns\n    -------\n    dict\n        Dictionary of training losses from every epoch for each flow\n        in the ensemble.\n    \"\"\"\n\n    # generate random seeds for each flow\n    rng = np.random.default_rng(seed)\n    seeds = rng.integers(1e9, size=len(self._ensemble))\n\n    loss_dict = dict()\n\n    for i, (name, flow) in enumerate(self._ensemble.items()):\n        if verbose:\n            print(name)\n\n        loss_dict[name] = flow.train(\n            inputs=inputs,\n            val_set=val_set,\n            train_weight=train_weight,\n            val_weight=val_weight,\n            epochs=epochs,\n            batch_size=batch_size,\n            optimizer=optimizer,\n            loss_fn=loss_fn,\n            convolve_errs=convolve_errs,\n            patience=patience,\n            best_params=best_params,\n            seed=seeds[i],\n            verbose=verbose,\n            progress_bar=progress_bar,\n            initial_loss=initial_loss,\n        )\n\n    return loss_dict\n</code></pre>"},{"location":"API/utils/","title":"Utils","text":"<p>Define utility functions for use in other modules.</p>"},{"location":"API/utils/#pzflow.utils.DenseReluNetwork","title":"<code>DenseReluNetwork(out_dim, hidden_layers, hidden_dim)</code>","text":"<p>Create a dense neural network with Relu after hidden layers.</p> <p>Parameters:</p> Name Type Description Default <code>out_dim</code> <code>int</code> <p>The output dimension.</p> required <code>hidden_layers</code> <code>int</code> <p>The number of hidden layers</p> required <code>hidden_dim</code> <code>int</code> <p>The dimension of the hidden layers</p> required <p>Returns:</p> Name Type Description <code>init_fun</code> <code>function</code> <p>The function that initializes the network. Note that this is the init_function defined in the Jax stax module, which is different from the functions of my InitFunction class.</p> <code>forward_fun</code> <code>function</code> <p>The function that passes the inputs through the neural network.</p> Source code in <code>pzflow/utils.py</code> <pre><code>def DenseReluNetwork(\n    out_dim: int, hidden_layers: int, hidden_dim: int\n) -&gt; Tuple[Callable, Callable]:\n    \"\"\"Create a dense neural network with Relu after hidden layers.\n\n    Parameters\n    ----------\n    out_dim : int\n        The output dimension.\n    hidden_layers : int\n        The number of hidden layers\n    hidden_dim : int\n        The dimension of the hidden layers\n\n    Returns\n    -------\n    init_fun : function\n        The function that initializes the network. Note that this is the\n        init_function defined in the Jax stax module, which is different\n        from the functions of my InitFunction class.\n    forward_fun : function\n        The function that passes the inputs through the neural network.\n    \"\"\"\n    init_fun, forward_fun = serial(\n        *(Dense(hidden_dim), LeakyRelu) * hidden_layers,\n        Dense(out_dim),\n    )\n    return init_fun, forward_fun\n</code></pre>"},{"location":"API/utils/#pzflow.utils.RationalQuadraticSpline","title":"<code>RationalQuadraticSpline(inputs, W, H, D, B, periodic=False, inverse=False)</code>","text":"<p>Apply rational quadratic spline to inputs and return outputs with log_det.</p> <p>Applies the piecewise rational quadratic spline developed in [1].</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>ndarray</code> <p>The inputs to be transformed.</p> required <code>W</code> <code>ndarray</code> <p>The widths of the spline bins.</p> required <code>H</code> <code>ndarray</code> <p>The heights of the spline bins.</p> required <code>D</code> <code>ndarray</code> <p>The derivatives of the inner spline knots.</p> required <code>B</code> <code>float</code> <p>Range of the splines. Outside of (-B,B), the transformation is just the identity.</p> required <code>inverse</code> <code>bool; default=False</code> <p>If True, perform the inverse transformation. Otherwise perform the forward transformation.</p> <code>False</code> <code>periodic</code> <code>bool; default=False</code> <p>Whether to make this a periodic, Circular Spline [2].</p> <code>False</code> <p>Returns:</p> Name Type Description <code>outputs</code> <code>ndarray</code> <p>The result of applying the splines to the inputs.</p> <code>log_det</code> <code>ndarray</code> <p>The log determinant of the Jacobian at the inputs.</p> References <p>[1] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios.     Neural Spline Flows. arXiv:1906.04032, 2019.     https://arxiv.org/abs/1906.04032 [2] Rezende, Danilo Jimenez et al.     Normalizing Flows on Tori and Spheres. arxiv:2002.02428, 2020     http://arxiv.org/abs/2002.02428</p> Source code in <code>pzflow/utils.py</code> <pre><code>def RationalQuadraticSpline(\n    inputs: jnp.ndarray,\n    W: jnp.ndarray,\n    H: jnp.ndarray,\n    D: jnp.ndarray,\n    B: float,\n    periodic: bool = False,\n    inverse: bool = False,\n) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"Apply rational quadratic spline to inputs and return outputs with log_det.\n\n    Applies the piecewise rational quadratic spline developed in [1].\n\n    Parameters\n    ----------\n    inputs : jnp.ndarray\n        The inputs to be transformed.\n    W : jnp.ndarray\n        The widths of the spline bins.\n    H : jnp.ndarray\n        The heights of the spline bins.\n    D : jnp.ndarray\n        The derivatives of the inner spline knots.\n    B : float\n        Range of the splines.\n        Outside of (-B,B), the transformation is just the identity.\n    inverse : bool; default=False\n        If True, perform the inverse transformation.\n        Otherwise perform the forward transformation.\n    periodic : bool; default=False\n        Whether to make this a periodic, Circular Spline [2].\n\n    Returns\n    -------\n    outputs : jnp.ndarray\n        The result of applying the splines to the inputs.\n    log_det : jnp.ndarray\n        The log determinant of the Jacobian at the inputs.\n\n    References\n    ----------\n    [1] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios.\n        Neural Spline Flows. arXiv:1906.04032, 2019.\n        https://arxiv.org/abs/1906.04032\n    [2] Rezende, Danilo Jimenez et al.\n        Normalizing Flows on Tori and Spheres. arxiv:2002.02428, 2020\n        http://arxiv.org/abs/2002.02428\n    \"\"\"\n    # knot x-positions\n    xk = jnp.pad(\n        -B + jnp.cumsum(W, axis=-1),\n        [(0, 0)] * (len(W.shape) - 1) + [(1, 0)],\n        mode=\"constant\",\n        constant_values=-B,\n    )\n    # knot y-positions\n    yk = jnp.pad(\n        -B + jnp.cumsum(H, axis=-1),\n        [(0, 0)] * (len(H.shape) - 1) + [(1, 0)],\n        mode=\"constant\",\n        constant_values=-B,\n    )\n    # knot derivatives\n    if periodic:\n        dk = jnp.pad(D, [(0, 0)] * (len(D.shape) - 1) + [(1, 0)], mode=\"wrap\")\n    else:\n        dk = jnp.pad(\n            D,\n            [(0, 0)] * (len(D.shape) - 1) + [(1, 1)],\n            mode=\"constant\",\n            constant_values=1,\n        )\n    # knot slopes\n    sk = H / W\n\n    # if not periodic, out-of-bounds inputs will have identity applied\n    # if periodic, we map the input into the appropriate region inside\n    # the period. For now, we will pretend all inputs are periodic.\n    # This makes sure that out-of-bounds inputs don't cause problems\n    # with the spline, but for the non-periodic case, we will replace\n    # these with their original values at the end\n    out_of_bounds = (inputs &lt;= -B) | (inputs &gt;= B)\n    masked_inputs = jnp.where(out_of_bounds, jnp.abs(inputs) - B, inputs)\n\n    # find bin for each input\n    if inverse:\n        idx = jnp.sum(yk &lt;= masked_inputs[..., None], axis=-1)[..., None] - 1\n    else:\n        idx = jnp.sum(xk &lt;= masked_inputs[..., None], axis=-1)[..., None] - 1\n\n    # get kx, ky, kyp1, kd, kdp1, kw, ks for the bin corresponding to each input\n    input_xk = jnp.take_along_axis(xk, idx, -1)[..., 0]\n    input_yk = jnp.take_along_axis(yk, idx, -1)[..., 0]\n    input_dk = jnp.take_along_axis(dk, idx, -1)[..., 0]\n    input_dkp1 = jnp.take_along_axis(dk, idx + 1, -1)[..., 0]\n    input_wk = jnp.take_along_axis(W, idx, -1)[..., 0]\n    input_hk = jnp.take_along_axis(H, idx, -1)[..., 0]\n    input_sk = jnp.take_along_axis(sk, idx, -1)[..., 0]\n\n    if inverse:\n        # [1] Appendix A.3\n        # quadratic formula coefficients\n        a = (input_hk) * (input_sk - input_dk) + (masked_inputs - input_yk) * (\n            input_dkp1 + input_dk - 2 * input_sk\n        )\n        b = (input_hk) * input_dk - (masked_inputs - input_yk) * (\n            input_dkp1 + input_dk - 2 * input_sk\n        )\n        c = -input_sk * (masked_inputs - input_yk)\n\n        relx = 2 * c / (-b - jnp.sqrt(b**2 - 4 * a * c))\n        outputs = relx * input_wk + input_xk\n        # if not periodic, replace out-of-bounds values with original values\n        if not periodic:\n            outputs = jnp.where(out_of_bounds, inputs, outputs)\n\n        # [1] Appendix A.2\n        # calculate the log determinant\n        dnum = (\n            input_dkp1 * relx**2\n            + 2 * input_sk * relx * (1 - relx)\n            + input_dk * (1 - relx) ** 2\n        )\n        dden = input_sk + (input_dkp1 + input_dk - 2 * input_sk) * relx * (\n            1 - relx\n        )\n        log_det = 2 * jnp.log(input_sk) + jnp.log(dnum) - 2 * jnp.log(dden)\n        # if not periodic, replace log_det for out-of-bounds values = 0\n        if not periodic:\n            log_det = jnp.where(out_of_bounds, 0, log_det)\n        log_det = log_det.sum(axis=1)\n\n        return outputs, -log_det\n\n    else:\n        # [1] Appendix A.1\n        # calculate spline\n        relx = (masked_inputs - input_xk) / input_wk\n        num = input_hk * (input_sk * relx**2 + input_dk * relx * (1 - relx))\n        den = input_sk + (input_dkp1 + input_dk - 2 * input_sk) * relx * (\n            1 - relx\n        )\n        outputs = input_yk + num / den\n        # if not periodic, replace out-of-bounds values with original values\n        if not periodic:\n            outputs = jnp.where(out_of_bounds, inputs, outputs)\n\n        # [1] Appendix A.2\n        # calculate the log determinant\n        dnum = (\n            input_dkp1 * relx**2\n            + 2 * input_sk * relx * (1 - relx)\n            + input_dk * (1 - relx) ** 2\n        )\n        dden = input_sk + (input_dkp1 + input_dk - 2 * input_sk) * relx * (\n            1 - relx\n        )\n        log_det = 2 * jnp.log(input_sk) + jnp.log(dnum) - 2 * jnp.log(dden)\n        # if not periodic, replace log_det for out-of-bounds values = 0\n        if not periodic:\n            log_det = jnp.where(out_of_bounds, 0, log_det)\n        log_det = log_det.sum(axis=1)\n\n        return outputs, log_det\n</code></pre>"},{"location":"API/utils/#pzflow.utils.build_bijector_from_info","title":"<code>build_bijector_from_info(info)</code>","text":"<p>Build a Bijector from a Bijector_Info object</p> Source code in <code>pzflow/utils.py</code> <pre><code>def build_bijector_from_info(info: tuple) -&gt; tuple:\n    \"\"\"Build a Bijector from a Bijector_Info object\"\"\"\n\n    # recurse through chains\n    if info[0] == \"Chain\":\n        return bijectors.Chain(*(build_bijector_from_info(i) for i in info[1]))\n    # build individual bijector from name and parameters\n    else:\n        return getattr(bijectors, info[0])(*info[1])\n</code></pre>"},{"location":"API/utils/#pzflow.utils.gaussian_error_model","title":"<code>gaussian_error_model(key, X, Xerr, nsamples)</code>","text":"<p>Default Gaussian error model were X are the means and Xerr are the stds.</p> Source code in <code>pzflow/utils.py</code> <pre><code>def gaussian_error_model(\n    key, X: jnp.ndarray, Xerr: jnp.ndarray, nsamples: int\n) -&gt; jnp.ndarray:\n    \"\"\"\n    Default Gaussian error model were X are the means and Xerr are the stds.\n    \"\"\"\n\n    eps = random.normal(key, shape=(X.shape[0], nsamples, X.shape[1]))\n\n    return X[:, None, :] + eps * Xerr[:, None, :]\n</code></pre>"},{"location":"API/utils/#pzflow.utils.sub_diag_indices","title":"<code>sub_diag_indices(inputs)</code>","text":"<p>Return indices for diagonal of 2D blocks in 3D array</p> Source code in <code>pzflow/utils.py</code> <pre><code>def sub_diag_indices(\n    inputs: jnp.ndarray,\n) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n    \"\"\"Return indices for diagonal of 2D blocks in 3D array\"\"\"\n    if inputs.ndim != 3:\n        raise ValueError(\"Input must be a 3D array.\")\n    nblocks = inputs.shape[0]\n    ndiag = min(inputs.shape[1], inputs.shape[2])\n    idx = (\n        jnp.repeat(jnp.arange(nblocks), ndiag),\n        jnp.tile(jnp.arange(ndiag), nblocks),\n        jnp.tile(jnp.arange(ndiag), nblocks),\n    )\n    return idx\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Below are example notebooks demonstrating how to use PZFlow. Each contains a link to open the notebook on Google Colab, as well as a link to the source code on Github.</p>"},{"location":"tutorials/#basic","title":"Basic","text":"<ul> <li>Introduction to PZFlow - using the default flow to train, sample, and calculate posteriors</li> <li>Conditional Flows - building a conditional flow to model conditional distributions</li> <li>Convolving Gaussian Errors - convolving Gaussian errors during training and posterior calculation</li> <li>Flow Ensembles - using <code>FlowEnsemble</code> to create an ensemble of normalizing flows</li> <li>Training Weights - giving different weights to your training samples</li> </ul>"},{"location":"tutorials/#intermediate","title":"Intermediate","text":"<ul> <li>Customizing the flow - Customizing the bijector and latent space</li> <li>Modeling Variables with Periodic Topology - using circular splines to model data with periodic topology, e.g. positions on a sphere</li> </ul>"},{"location":"tutorials/#advanced","title":"Advanced","text":"<ul> <li>Marginalizing Variables - marginalizing over missing variables during posterior calculation</li> <li>Convolving Non-Gaussian Errors - convolving non-Gaussian errors during training and posterior calculation</li> </ul>"},{"location":"tutorials/conditional_demo/","title":"Conditional Flows","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre># !pip install pzflow matplotlib\n</pre> # !pip install pzflow matplotlib In\u00a0[1]: Copied! <pre>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfrom pzflow import Flow\nfrom pzflow.examples import get_twomoons_data\n</pre> import jax.numpy as jnp import matplotlib.pyplot as plt  from pzflow import Flow from pzflow.examples import get_twomoons_data In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\" <p>First let's load the two moons data set again:</p> In\u00a0[3]: Copied! <pre>data = get_twomoons_data()\ndata\n</pre> data = get_twomoons_data() data Out[3]: x y 0 -0.748695 0.777733 1 1.690101 -0.207291 2 2.008558 0.285932 3 1.291547 -0.441167 4 0.808686 -0.481017 ... ... ... 99995 1.642738 -0.221286 99996 0.981221 0.327815 99997 0.990856 0.182546 99998 -0.343144 0.877573 99999 1.851718 0.008531 <p>100000 rows \u00d7 2 columns</p> <p>We can build the conditional flow in the same way we built the regular flow in the intro. All we have to do is provide the name of the column the distribution is conditioned on.</p> In\u00a0[4]: Copied! <pre>flow = Flow(data_columns=[\"x\"], conditional_columns=[\"y\"])\n</pre> flow = Flow(data_columns=[\"x\"], conditional_columns=[\"y\"]) <pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <p>Now we are ready to train the flow:</p> In\u00a0[5]: Copied! <pre>losses = flow.train(data, verbose=True)\n</pre> losses = flow.train(data, verbose=True) <pre>Training 100 epochs \nLoss:\n(0) 1.4365\n(1) 0.1589\n(6) -0.1398\n(11) -0.1478\n(16) -0.1447\n(21) -0.1649\n(26) -0.1784\n(31) -0.1631\n(36) -0.1793\n(41) -0.1570\n(46) -0.1478\n(51) -0.1729\n(56) -0.1537\n(61) -0.1768\n(66) -0.1641\n(71) -0.1798\n(76) -0.1568\n(81) -0.1651\n(86) -0.1396\n(91) -0.1830\n(96) -0.1805\n(100) -0.1686\n</pre> In\u00a0[6]: Copied! <pre>plt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training loss\")\nplt.show()\n</pre> plt.plot(losses) plt.xlabel(\"Epoch\") plt.ylabel(\"Training loss\") plt.show() <p>Great!</p> <p>You can use this flow very similarly to how we used the regular flow. We can sample and calculate posteriors -- the big difference is that we now provide conditions to condition the distribution on.</p> <p>So for example, we can condition the distribution on the first value in our DataFrame, and draw 1000 samples:</p> In\u00a0[7]: Copied! <pre>samples = flow.sample(1000, conditions=data[:1], seed=0)\nsamples\n</pre> samples = flow.sample(1000, conditions=data[:1], seed=0) samples Out[7]: x y 0 -0.706093 0.777733 0 -0.602977 0.777733 0 -0.532370 0.777733 0 -0.615776 0.777733 0 0.616342 0.777733 ... ... ... 0 0.592016 0.777733 0 -0.556290 0.777733 0 -0.581105 0.777733 0 -0.607906 0.777733 0 -0.538562 0.777733 <p>1000 rows \u00d7 2 columns</p> <p>Notice how the <code>y</code> value is the same for all of these samples? That is because all of these samples were conditioned on the same <code>y</code> value!</p> <p>Let's plot the histogram of these samples, along with the posterior for this y value:</p> In\u00a0[10]: Copied! <pre>plt.hist(samples.x, bins=20, density=True)\n\nx_grid = jnp.linspace(-1, 1, 1000)\nx_posterior = flow.posterior(data[:1], column=\"x\", grid=x_grid)[0]\nplt.plot(x_grid, x_posterior)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$p(x|y=0.78)$\")\n</pre> plt.hist(samples.x, bins=20, density=True)  x_grid = jnp.linspace(-1, 1, 1000) x_posterior = flow.posterior(data[:1], column=\"x\", grid=x_grid)[0] plt.plot(x_grid, x_posterior) plt.xlabel(\"$x$\") plt.ylabel(\"$p(x|y=0.78)$\") Out[10]: <pre>Text(0, 0.5, '$p(x|y=0.78)$')</pre> <p>You can see they match well. There is a small bump between the two peaks - it's not clear if it's real or not. Depending on the context, you may want to validate or regularize this bump.</p> <p>We can also easily draw multiple samples from multiple conditions. For example, if we take the first 10 rows of <code>data</code> and set <code>nsamples=10</code>, we will get 100 samples - 10 for each condition in <code>data</code>:</p> In\u00a0[9]: Copied! <pre>flow.sample(nsamples=10, conditions=data[:10])\n</pre> flow.sample(nsamples=10, conditions=data[:10]) Out[9]: x y 0 -0.513450 0.777733 0 -0.626765 0.777733 0 0.622857 0.777733 0 -0.452066 0.777733 0 0.596286 0.777733 ... ... ... 9 0.714883 0.756692 9 -0.594839 0.756692 9 -0.667886 0.756692 9 0.676397 0.756692 9 -0.688675 0.756692 <p>100 rows \u00d7 2 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/conditional_demo/#conditional-flow-example","title":"Conditional flow example\u00b6","text":"<p>This notebook uses the same data as the introduction, but this time, instead of modeling the full joint probability distribution, we will model the distribution of <code>x</code> conditioned on the value of <code>y</code>.</p> <p>In math, instead of modeling $p(x, y)$, we will model $p(x|y)$.</p> <p>Note that this just a basic 1-dimensional example, but it is also possible to condition a high-dimesional distribution on multiple variables. For example, you might build the 7-dimensional flow from the redshift example, but condition the flow on right ascension and declination.</p>"},{"location":"tutorials/customizing_example/","title":"Customizing the flow","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre># !pip install pzflow matplotlib corner\n</pre> # !pip install pzflow matplotlib corner In\u00a0[1]: Copied! <pre>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport corner\n\nimport pzflow\nfrom pzflow import Flow\nfrom pzflow.bijectors import Chain, ShiftBounds, RollingSplineCoupling\nfrom pzflow.distributions import Uniform\nfrom pzflow.examples import get_galaxy_data\n</pre> import jax.numpy as jnp import matplotlib.pyplot as plt import corner  import pzflow from pzflow import Flow from pzflow.bijectors import Chain, ShiftBounds, RollingSplineCoupling from pzflow.distributions import Uniform from pzflow.examples import get_galaxy_data In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\" <p>For this example, we will use the example galaxy data included with PZFlow.</p> <p>It's okay if you don't know what this data is - it's just a 7-dimensional space of galaxy data.</p> <p>If you are familiar with galaxy data:</p> <ul> <li>the first column is the galaxy's true redshift</li> <li>the following 6 columns are the galaxy's true apparent magnitudes in the LSST photometric bands</li> </ul> <p>Again, don't worry if you are unfamiliar with this kind of data - the astronomy details are unimportant for understanding this notebook.</p> In\u00a0[3]: Copied! <pre>data = get_galaxy_data()\ndata\n</pre> data = get_galaxy_data() data Out[3]: redshift u g r i z y 0 0.287087 26.759261 25.901778 25.187710 24.932318 24.736903 24.671623 1 0.293313 27.428358 26.679299 25.977161 25.700094 25.522763 25.417632 2 1.497276 27.294001 26.068798 25.450055 24.460507 23.887221 23.206112 3 0.283310 28.154075 26.283166 24.599570 23.723491 23.214108 22.860012 4 1.545183 29.276065 27.878301 27.333528 26.543374 26.061941 25.383056 ... ... ... ... ... ... ... ... 99995 0.661883 25.665363 25.703183 25.362422 24.936967 24.771529 24.691300 99996 0.864290 28.342505 27.654294 26.638448 25.615190 25.108037 24.938998 99997 0.973176 27.781264 27.386962 26.760144 26.107564 25.584028 25.403601 99998 2.281685 29.324894 28.148004 27.312318 26.883849 26.416446 25.762161 99999 0.254427 22.077757 20.985672 20.143253 19.805022 19.594315 19.471030 <p>100000 rows \u00d7 7 columns</p> <p>We will build a Flow to model this data.</p> <p>You must understand a little about how normalizing flows work. Normalizing flows start with input data that is from some complicated data distribution. They then apply a series of transformations (bijections) that transform the data distribution into a simple latent distribution.</p> <p>All of the heavy lifting of transforming the data distribution into the latent distribution is handled by a <code>RollingSplineCoupling</code>. This bijection alternates Neural Spline Couplings and \"Rolling\" layers. <code>RollingSplineCoupling</code> takes several parameters, which you can see in the documentation/API, but the only required parameter is <code>nlayers</code>.</p> <p><code>nlayers</code> sets the number of <code>(Neural Spline, Roll)</code> couplets that are chained together inside of <code>RollingSplineCoupling</code>. We will set <code>nlayers = 7</code> to match the dimension of the input data.</p> <p>The other really important parameter is the range of the splines, <code>B</code>. You don't have to provide <code>B</code> -- it's set to <code>B=5</code> by default. However, it is important to understand that the splines are only defined on the range [-B, B]. Any data outside of this range will not be transformed (it just has the identity applied).</p> <p>If you look at the DataFrame above, you will see that lots of our data lies outside the range [-5, 5]! It is our job to make sure that our data lies within that range before it is passed into the <code>RollingSplineCoupling</code>.</p> <p>We can do this using the <code>ShiftBounds</code> bijector. <code>ShiftBounds</code> has 3 parameters:</p> <ul> <li><code>mins</code> - the minima of the columns in your data</li> <li><code>maxs</code> - the maxima of the columns in your data</li> <li><code>B</code> - the range your data will be mapped onto: [-B, B].</li> </ul> <p>In other words, if we had some data <code>x</code> that ran from 3 to 7, then <code>ShiftBounds(3, 7, 2)</code> would transform <code>x</code> so that its range ran from [-2, 2].</p> <p>In our case, since we left <code>B=5</code> in the <code>RollingSplineCoupling</code>, we need to make sure that all of our data columns lay in the range [-5, 5]. However, <code>RollingSplineCoupling</code> performs better when the data isn't too close to the edge of its support (it will still work - you will just see some weird edge artifacts when sampling from the flow). To avoid edge effects, we will map all of our data columns into the range [-4, 4].</p> <p>So our final bijection will be: <code>ShiftBounds(mins, maxs, B=4) --&gt; RollingSplineCoupling(7, B=5)</code>. In PZFlow, you chain these together using the <code>Chain</code> bijector:</p> In\u00a0[4]: Copied! <pre># get minima and maxima for each column\nmins = jnp.array(data.min(axis=0))\nmaxs = jnp.array(data.max(axis=0))\n\n# get the number of dimensions\nndim = data.shape[1]\n\n# build the bijector\nbijector = Chain(\n    ShiftBounds(mins, maxs, B=4),\n    RollingSplineCoupling(ndim, B=5),\n)\n</pre> # get minima and maxima for each column mins = jnp.array(data.min(axis=0)) maxs = jnp.array(data.max(axis=0))  # get the number of dimensions ndim = data.shape[1]  # build the bijector bijector = Chain(     ShiftBounds(mins, maxs, B=4),     RollingSplineCoupling(ndim, B=5), ) <pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <p>Now we have our bijector!</p> <p>We also need a latent space. By default, PZFlow uses a uniform distribution over the range [-5, 5]. This is because the <code>RollingSplineCoupling</code> has a default range [-5, 5], and we need our bijector to match this range. You could choose any compact distribution that matches this range, but uniform is the simplest choice.</p> <p>PZFlow uses this distribution by default, but just like the bijector, we will manually construct it just for demonstration:</p> In\u00a0[5]: Copied! <pre>latent = Uniform(input_dim=ndim, B=5)\n</pre> latent = Uniform(input_dim=ndim, B=5) <p>We are finally ready to construct the normalizing flow! Just like before, we must tell it the names of our data columns, but this time we will explicitly pass a bijector and a latent distribution</p> <p>Note: the order of the column names you give to PZFlow MUST match the order of the columns implicit in the <code>mins</code> and <code>maxs</code> you gave to <code>ShiftBounds</code> above. This is the ONLY time you have to keep track of the column ordering. After creating the flow, PZFlow will always use the column names to access the correct data.</p> In\u00a0[6]: Copied! <pre>flow = Flow(data.columns, bijector=bijector, latent=latent)\n</pre> flow = Flow(data.columns, bijector=bijector, latent=latent) <p>Now let's train the flow!</p> In\u00a0[7]: Copied! <pre>%%time \nlosses = flow.train(data, epochs=200, verbose=True)\n</pre> %%time  losses = flow.train(data, epochs=200, verbose=True) <pre>Training 200 epochs \nLoss:\n(0) 20.4908\n(1) 2.1882\n(11) -3.0561\n(21) -3.7004\n(31) -3.5159\n(41) -4.3430\n(51) -4.7489\n(61) -4.0210\n(71) -5.0300\n(81) -3.9104\n(91) 0.3195\n(101) -5.2164\n(111) -5.3040\n(121) -4.9196\n(131) -6.1689\n(141) -5.9213\n(151) -6.3125\n(161) -6.4114\n(171) -6.6141\n(181) -6.0520\n(191) -6.2788\n(200) -5.7158\nCPU times: user 1h 7min 33s, sys: 44min 50s, total: 1h 52min 24s\nWall time: 21min 29s\n</pre> <p>Now let's plot the training losses to make sure everything looks like we expect it to...</p> In\u00a0[10]: Copied! <pre>plt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training loss\")\nplt.show()\n</pre> plt.plot(losses) plt.xlabel(\"Epoch\") plt.ylabel(\"Training loss\") plt.show() <p>Great! It looks like we could get better performance if we train for longer or use some kind of regularization, but this is sufficient for this demo.</p> <p>Now we can draw samples from the flow, and compare these samples to the original data set.</p> In\u00a0[11]: Copied! <pre>samples = flow.sample(10000, seed=0)\n</pre> samples = flow.sample(10000, seed=0) In\u00a0[14]: Copied! <pre>fig = plt.figure(figsize=(12,12))\n\nranges = [(-0.1,2.4), (19.5,33), (19,32), (19,29), (19,29), (19,28), (19,28)]\n\ncorner.corner(samples, fig=fig, color='C3', bins=20, range=ranges, hist_bin_factor=2, data_kwargs={'ms':3}, contour_kwargs={'linewidths':2})\n\ncorner.corner(data[:10000], fig=fig, bins=20, range=ranges, hist_bin_factor=2, color='C0', data_kwargs={'ms':3}, show_titles=True, labels=data.columns);\n</pre> fig = plt.figure(figsize=(12,12))  ranges = [(-0.1,2.4), (19.5,33), (19,32), (19,29), (19,29), (19,28), (19,28)]  corner.corner(samples, fig=fig, color='C3', bins=20, range=ranges, hist_bin_factor=2, data_kwargs={'ms':3}, contour_kwargs={'linewidths':2})  corner.corner(data[:10000], fig=fig, bins=20, range=ranges, hist_bin_factor=2, color='C0', data_kwargs={'ms':3}, show_titles=True, labels=data.columns); <p>The original data is in blue, while the distribution learned by the normalizing flow is in red.</p> <p>This looks great!</p> <p>We can also use the flow to calculate redshift posteriors for galaxies:</p> In\u00a0[15]: Copied! <pre>grid = jnp.linspace(0, 2, 100)\npdfs = flow.posterior(samples, column=\"redshift\", grid=grid)\n</pre> grid = jnp.linspace(0, 2, 100) pdfs = flow.posterior(samples, column=\"redshift\", grid=grid) <p>Let's plot a few different posteriors (in blue), along with the true redshifts of the galaxies (in red):</p> In\u00a0[16]: Copied! <pre>fig, axes = plt.subplots(1,3,figsize=(7,2.2), dpi=120)\n\nidx = [0, 3, 12]\nfor i,ax in zip(idx, axes):\n    true_z = samples['redshift'][i]\n    ax.axvline(true_z, 0, 1, c=\"C3\",\n               label='True z')\n    ax.plot(grid, pdfs[i])\n    ax.set(xlabel=\"redshift\",\n           xticks=[0,0.5,1,1.5,2],\n           yticks=[])\naxes[0].legend()\naxes[0].set(ylabel='$p(z)$')\nplt.show()\n</pre> fig, axes = plt.subplots(1,3,figsize=(7,2.2), dpi=120)  idx = [0, 3, 12] for i,ax in zip(idx, axes):     true_z = samples['redshift'][i]     ax.axvline(true_z, 0, 1, c=\"C3\",                label='True z')     ax.plot(grid, pdfs[i])     ax.set(xlabel=\"redshift\",            xticks=[0,0.5,1,1.5,2],            yticks=[]) axes[0].legend() axes[0].set(ylabel='$p(z)$') plt.show() <p>We can also draw posteriors for magnitudes. For example, lets draw posteriors for the $u$ band:</p> In\u00a0[17]: Copied! <pre>u_grid = jnp.arange(24, 32, 0.1)\nu_pdfs = flow.posterior(samples, column='u', grid=u_grid)\n</pre> u_grid = jnp.arange(24, 32, 0.1) u_pdfs = flow.posterior(samples, column='u', grid=u_grid) <p>Let's plot the $u$ band posteriors for the same galaxies above:</p> In\u00a0[18]: Copied! <pre>fig, axes = plt.subplots(1,3,figsize=(7,2.2), dpi=120)\n\nidx = [0, 3, 12]\nfor i,ax in zip(idx, axes):\n    true_u = samples['u'][i]\n    ax.axvline(true_u, 0, 1, c=\"C3\",\n               label='True $u$')\n    ax.plot(u_grid, u_pdfs[i])\n    ax.set(xlabel=\"$u$ mag\", \n           yticks=[])\naxes[2].legend()\naxes[0].set(ylabel='$p(u)$')\nplt.show()\n</pre> fig, axes = plt.subplots(1,3,figsize=(7,2.2), dpi=120)  idx = [0, 3, 12] for i,ax in zip(idx, axes):     true_u = samples['u'][i]     ax.axvline(true_u, 0, 1, c=\"C3\",                label='True $u$')     ax.plot(u_grid, u_pdfs[i])     ax.set(xlabel=\"$u$ mag\",             yticks=[]) axes[2].legend() axes[0].set(ylabel='$p(u)$') plt.show() <p>Now let's store some information with the flow about the data it was trained on.</p> In\u00a0[21]: Copied! <pre>flow.info = f\"\"\"\nThis is an example flow, trained on 100,000 simulated galaxies with \nredshifts in the range (0, 2.3) and photometry in the LSST ugrizy bands.\n\nThe data set used to train this flow is available in the `examples` module:\n&gt;&gt;&gt; from pzflow.examples import get_galaxy_data\n&gt;&gt;&gt; data = get_galaxy_data()\n\nThis flow was created with pzflow version {pzflow.__version__}\n\"\"\"\n</pre> flow.info = f\"\"\" This is an example flow, trained on 100,000 simulated galaxies with  redshifts in the range (0, 2.3) and photometry in the LSST ugrizy bands.  The data set used to train this flow is available in the `examples` module: &gt;&gt;&gt; from pzflow.examples import get_galaxy_data &gt;&gt;&gt; data = get_galaxy_data()  This flow was created with pzflow version {pzflow.__version__} \"\"\" In\u00a0[22]: Copied! <pre>print(flow.info)\n</pre> print(flow.info) <pre>\nThis is an example flow, trained on 100,000 simulated galaxies with \nredshifts in the range (0, 2.3) and photometry in the LSST ugrizy bands.\n\nThe data set used to train this flow is available in the `examples` module:\n&gt;&gt;&gt; from pzflow.examples import get_galaxy_data\n&gt;&gt;&gt; data = get_galaxy_data()\n\nThis flow was created with pzflow version 3.0.0\n\n</pre> <p>This example flow is also packaged with PZFlow, so you can load it and play with it whenever you want:</p> In\u00a0[25]: Copied! <pre>from pzflow.examples import get_example_flow\nflow = get_example_flow()\n</pre> from pzflow.examples import get_example_flow flow = get_example_flow() <p>See! It's the same one:</p> In\u00a0[26]: Copied! <pre>print(flow.info)\n</pre> print(flow.info) <pre>\nThis is an example flow, trained on 100,000 simulated galaxies with \nredshifts in the range (0, 2.3) and photometry in the LSST ugrizy bands.\n\nThe data set used to train this flow is available in the `examples` module:\n&gt;&gt;&gt; from pzflow.examples import get_galaxy_data\n&gt;&gt;&gt; data = get_galaxy_data()\n\nThis flow was created with pzflow version 3.0.0\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/customizing_example/#customizing-the-bijector-and-latent-space","title":"Customizing the bijector and latent space\u00b6","text":"<p>This example notebook demonstrates how to chain multiple bijectors to make a custom bijector, and how to choose the latent space We will build the default bijector that PZFlow automatically uses. You don't need to do this, as PZFlow will automatically build this bijector for you, but it's good to see how it is done, plus it allows me to explain the design decisions a little bit.</p> <p>See the following notebooks for genuine examples of when you might want to build a custom bijector.</p>"},{"location":"tutorials/dequantization/","title":"Dequantization","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pzflow import Flow\nfrom pzflow.bijectors import Chain, ShiftBounds, RollingSplineCoupling, UniformDequantizer\nfrom pzflow.examples import get_checkerboard_data\n\nfrom optax import adam\n</pre> import numpy as np import matplotlib.pyplot as plt  from pzflow import Flow from pzflow.bijectors import Chain, ShiftBounds, RollingSplineCoupling, UniformDequantizer from pzflow.examples import get_checkerboard_data  from optax import adam <p>PZFlow includes example data that is discrete in a checkerboard pattern:</p> In\u00a0[2]: Copied! <pre>data = get_checkerboard_data()\ndata\n</pre> data = get_checkerboard_data() data Out[2]: x y 0 2 0 1 3 1 2 1 1 3 1 3 4 3 3 ... ... ... 99995 3 3 99996 0 2 99997 2 2 99998 0 2 99999 0 2 <p>100000 rows \u00d7 2 columns</p> In\u00a0[51]: Copied! <pre>import pandas as  pd\n</pre> import pandas as  pd In\u00a0[140]: Copied! <pre>np.random.seed(0)\nsamples1 = np.abs(np.random.normal([2.5, 7.2], [1, 1], size=(20_000, 2))).astype(int)\nsamples2 = np.abs(np.random.normal([7.2, 3.2], [2, 2], size=(65_000, 2))).astype(int)\nsamples3 = np.abs(np.random.normal([2.1, 2.4], [1, 1], size=(15_000, 2))).astype(int)\nsamples = np.concatenate((samples1, samples2, samples3))\n\nsamples = np.clip(samples, 0, 9)\n\ndata = pd.DataFrame(samples, columns=(\"x\", \"y\"))\n</pre> np.random.seed(0) samples1 = np.abs(np.random.normal([2.5, 7.2], [1, 1], size=(20_000, 2))).astype(int) samples2 = np.abs(np.random.normal([7.2, 3.2], [2, 2], size=(65_000, 2))).astype(int) samples3 = np.abs(np.random.normal([2.1, 2.4], [1, 1], size=(15_000, 2))).astype(int) samples = np.concatenate((samples1, samples2, samples3))  samples = np.clip(samples, 0, 9)  data = pd.DataFrame(samples, columns=(\"x\", \"y\")) In\u00a0[141]: Copied! <pre>data\n</pre> data Out[141]: x y 0 4 7 1 3 9 2 4 6 3 3 7 4 2 7 ... ... ... 99995 2 1 99996 1 4 99997 0 2 99998 2 3 99999 1 2 <p>100000 rows \u00d7 2 columns</p> In\u00a0[142]: Copied! <pre># Let's plot this distribution\nfig, ax = plt.subplots(figsize=(3,3), dpi=150)\nR = 10\nax.hist2d(data[\"x\"], data[\"y\"], bins=R, range=((0, R), (0, R)))\nax.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(R))\nplt.show()\n</pre> # Let's plot this distribution fig, ax = plt.subplots(figsize=(3,3), dpi=150) R = 10 ax.hist2d(data[\"x\"], data[\"y\"], bins=R, range=((0, R), (0, R))) ax.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(R)) plt.show() <p>Let's see how the default normalizing flow performs with this data:</p> In\u00a0[143]: Copied! <pre>flow = Flow(data.columns)\nlosses = flow.train(data, verbose=True)\n#losses = flow.train(data, epochs=35, optimizer=adam(1e-5), verbose=True)\n#losses += flow.train(data, epochs=20, optimizer=adam(1e-6), verbose=True)\n</pre> flow = Flow(data.columns) losses = flow.train(data, verbose=True) #losses = flow.train(data, epochs=35, optimizer=adam(1e-5), verbose=True) #losses += flow.train(data, epochs=20, optimizer=adam(1e-6), verbose=True) <pre>Training 100 epochs \nLoss:\n(0) 4.8493\n(1) 2.9534\n(6) 2.5506\n(11) 2.3200\n(16) 3.9691\n(21) 1.7155\n(26) 2.6619\n(31) 1.3618\n(36) 1.8318\n(41) 1.1798\n(46) 3.2903\n(51) 2.0116\n(56) 0.6866\n(61) 0.0556\n(66) 0.2943\n(71) -0.3578\n(76) 0.7877\n(81) 0.2823\n(86) -0.2371\n(91) -0.3916\n(96) -1.0014\n(100) 1.7795\n</pre> <p>First, let's plot the training loss:</p> In\u00a0[144]: Copied! <pre># plot the training losses\nplt.plot(losses)\n</pre> # plot the training losses plt.plot(losses) Out[144]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f25844b51c0&gt;]</pre> <p>This doesn't look good...</p> <p>Let's also plot some samples and compare to the truth data.</p> In\u00a0[145]: Copied! <pre># plot some samples\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(6.2, 6.2), dpi=150)\n\nax1.hist2d(data[\"x\"], data[\"y\"], bins=R, range=((0, R), (0, R)))\nax1.set(ylabel=\"y\", title=\"Truth\", yticks=np.arange(5))\nax3.scatter(data[\"x\"], data[\"y\"], marker=\".\", c=\"k\")\nax3.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(5))\n\nsamples = flow.sample(data.shape[0])\nax2.hist2d(samples[\"x\"], samples[\"y\"], bins=R, range=((0, R), (0, R)))\nax2.set(title=\"Samples\", yticks=np.arange(5))\nax4.scatter(samples[\"x\"], samples[\"y\"], marker=\".\", c=\"k\", s=1)\nax4.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(5))\n\nplt.show()\n</pre> # plot some samples  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(6.2, 6.2), dpi=150)  ax1.hist2d(data[\"x\"], data[\"y\"], bins=R, range=((0, R), (0, R))) ax1.set(ylabel=\"y\", title=\"Truth\", yticks=np.arange(5)) ax3.scatter(data[\"x\"], data[\"y\"], marker=\".\", c=\"k\") ax3.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(5))  samples = flow.sample(data.shape[0]) ax2.hist2d(samples[\"x\"], samples[\"y\"], bins=R, range=((0, R), (0, R))) ax2.set(title=\"Samples\", yticks=np.arange(5)) ax4.scatter(samples[\"x\"], samples[\"y\"], marker=\".\", c=\"k\", s=1) ax4.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(5))  plt.show() <p>Well that's weird... (I think the bottom right kind of looks like the head of a dog, with the CMB power spectrum on its cheek haha)</p> <p>Clearly the default flow does a really bad job with discrete data!</p> <p>We can do much better if we add a dequantizer to the bijector chain. The dequantizer will add uniform noise in the range (0, 1) to each variable, and thereby smooth out the distribution. This allows the <code>RollingSplineCoupling</code> to perform better.</p> <p>When sampling, everything acts in reverse, so the dequantizer will quantize the samples, resulting in a discrete distribution.</p> <p>(Note the dequantizer isn't technically bijective, but that's okay)</p> In\u00a0[150]: Copied! <pre># we build the exact same flow, except we add a dequantizer to the bijector chain\nbijector = Chain(\n    UniformDequantizer([0, 1]), # dequantize the data\n    ShiftBounds(0, 10, B=5), # shift bounds of data from (0, 4) -&gt; (-5, 5)\n    RollingSplineCoupling(nlayers=2, B=5), # transform distribution\n)\n\ndq_flow = Flow(data.columns, bijector)\n\n#dq_losses = dq_flow.train(data, epochs=50, optimizer=adam(1e-3), verbose=True)\n#dq_losses = dq_flow.train(data, epochs=30, optimizer=adam(1e-5))\n#dq_losses += dq_flow.train(data, epochs=200, optimizer=adam(1e-6))\ndq_losses = dq_flow.train(data, verbose=True, optimizer=adam(1e-4))\n</pre> # we build the exact same flow, except we add a dequantizer to the bijector chain bijector = Chain(     UniformDequantizer([0, 1]), # dequantize the data     ShiftBounds(0, 10, B=5), # shift bounds of data from (0, 4) -&gt; (-5, 5)     RollingSplineCoupling(nlayers=2, B=5), # transform distribution )  dq_flow = Flow(data.columns, bijector)  #dq_losses = dq_flow.train(data, epochs=50, optimizer=adam(1e-3), verbose=True) #dq_losses = dq_flow.train(data, epochs=30, optimizer=adam(1e-5)) #dq_losses += dq_flow.train(data, epochs=200, optimizer=adam(1e-6)) dq_losses = dq_flow.train(data, verbose=True, optimizer=adam(1e-4)) <pre>Training 100 epochs \nLoss:\n(0) 4.6678\n(1) 3.3847\n(6) 2.5783\n(11) 3.0548\n(16) 2.4242\n(21) 2.9061\n(26) 2.5543\n(31) 1.1161\n(36) 3.6363\n(41) 1.4428\n(46) 2.0146\n(51) 2.3853\n(56) 1.8829\n(61) 1.0884\n(66) 1.0035\n(71) 2.3484\n(76) 1.1558\n(81) 1.0942\n(86) 0.9255\n(91) 0.9621\n(96) 0.7448\n(100) 1.1812\n</pre> In\u00a0[45]: Copied! <pre>dq_losses += dq_flow.train(data, epochs=200, optimizer=adam(1e-6))\n</pre> dq_losses += dq_flow.train(data, epochs=200, optimizer=adam(1e-6)) In\u00a0[151]: Copied! <pre># plot the training losses\nplt.plot(dq_losses)\n</pre> # plot the training losses plt.plot(dq_losses) Out[151]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f25641e1fd0&gt;]</pre> In\u00a0[152]: Copied! <pre># plot some samples\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(6.2, 6.2), dpi=150)\n\nax1.hist2d(data[\"x\"], data[\"y\"], bins=R, range=((0, R), (0, R)))\nax1.set(ylabel=\"y\", title=\"Truth\", yticks=np.arange(5))\nax3.scatter(data[\"x\"], data[\"y\"], marker=\".\", c=\"k\")\nax3.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(4))\n\ndq_samples = dq_flow.sample(data.shape[0])\nax2.hist2d(dq_samples[\"x\"], dq_samples[\"y\"], bins=R, range=((0, R), (0, R)))\nax2.set(title=\"Samples\", yticks=np.arange(5))\nax4.scatter(dq_samples[\"x\"], dq_samples[\"y\"], marker=\".\", c=\"k\")\nax4.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(4))\n\nplt.show()\n</pre> # plot some samples  fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(6.2, 6.2), dpi=150)  ax1.hist2d(data[\"x\"], data[\"y\"], bins=R, range=((0, R), (0, R))) ax1.set(ylabel=\"y\", title=\"Truth\", yticks=np.arange(5)) ax3.scatter(data[\"x\"], data[\"y\"], marker=\".\", c=\"k\") ax3.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(4))  dq_samples = dq_flow.sample(data.shape[0]) ax2.hist2d(dq_samples[\"x\"], dq_samples[\"y\"], bins=R, range=((0, R), (0, R))) ax2.set(title=\"Samples\", yticks=np.arange(5)) ax4.scatter(dq_samples[\"x\"], dq_samples[\"y\"], marker=\".\", c=\"k\") ax4.set(xlabel=\"x\", ylabel=\"y\", yticks=np.arange(4))  plt.show() <p>We can see that this flow produces only discrete samples that lie on the grid, and that the distribution of samples closely matches that of the training data. In addition, the training was a lot easier - we could just do a single round of training with default settings, rather than needing to adjust the learning rate schedule.</p> <p>One more thing - You can model discrete and continuous variables side-by-side in the same normalizing flow! Just drop in <code>UniformDequantizer</code> with <code>column_idx</code> equal to the list of column indices corresponding to the discrete variables.</p> <p>For example, suppose we want to model data with column names [\"a\", \"b\", \"c\", \"d\"], and that \"a\" and \"c\" are continuous variables, while \"b\" and \"d\" are discrete variables. We can use a bijector like this:</p> <pre><code>bijector = Chain(\n    UniformDequantizer(column_idx=[1, 3]),\n    ...,\n)\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/dequantization/#dequantization-example","title":"Dequantization example\u00b6","text":"<p>Normalizing flows are designed to model continuous variables, however they can be easily adapted to model discrete data as well. This is achieved by inserting a \"dequantizer\" into the bijector chain. We will demonstrate the use of a Uniform Dequantizer to model checkerboard data.</p>"},{"location":"tutorials/ensemble_demo/","title":"Flow Ensembles","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre># !pip install pzflow matplotlib\n</pre> # !pip install pzflow matplotlib In\u00a0[1]: Copied! <pre>from pzflow import FlowEnsemble\nfrom pzflow.examples import get_twomoons_data\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n</pre> from pzflow import FlowEnsemble from pzflow.examples import get_twomoons_data  import jax.numpy as jnp import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\" <p>Let's load the two moons data set:</p> In\u00a0[3]: Copied! <pre>data = get_twomoons_data()\ndata\n</pre> data = get_twomoons_data() data Out[3]: x y 0 -0.748695 0.777733 1 1.690101 -0.207291 2 2.008558 0.285932 3 1.291547 -0.441167 4 0.808686 -0.481017 ... ... ... 99995 1.642738 -0.221286 99996 0.981221 0.327815 99997 0.990856 0.182546 99998 -0.343144 0.877573 99999 1.851718 0.008531 <p>100000 rows \u00d7 2 columns</p> <p>Let's plot it to see what it looks like.</p> In\u00a0[4]: Copied! <pre>plt.hist2d(data['x'], data['y'], bins=200)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n</pre> plt.hist2d(data['x'], data['y'], bins=200) plt.xlabel('x') plt.ylabel('y') plt.show() <p>Now we will build an ensemble of normalizing flows using the <code>FlowEnsemble</code> class. It is constructed identically to <code>Flow</code>, except now we also have a parameter <code>N</code> which controls how many different copies of the Flow are created. Each of these Flows is identical, just with different random parameters instantiated. Let's make an ensemble of 4 Flows.</p> In\u00a0[5]: Copied! <pre>flowEns = FlowEnsemble(data.columns, N=4)\n</pre> flowEns = FlowEnsemble(data.columns, N=4) <p>Now we can train the Ensemble. Just like a refular <code>Flow</code>, this is as simple as calling <code>flowEns.train(data)</code>.</p> In\u00a0[6]: Copied! <pre>%%time\nlosses = flowEns.train(data, verbose=True)\n</pre> %%time losses = flowEns.train(data, verbose=True) <pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <pre>Flow 0\nTraining 50 epochs \nLoss:\n(0) 2.3370\n(1) 0.7259\n(3) 0.4162\n(5) 0.3592\n(7) 0.3221\n(9) 0.3759\n(11) 0.3107\n(13) 0.3167\n(15) 0.3075\n(17) 0.3274\n(19) 0.3528\n(21) 0.3151\n(23) 0.3165\n(25) 0.3040\n(27) 0.3033\n(29) 0.3080\n(31) 0.3027\n(33) 0.3136\n(35) 0.3066\n(37) 0.3024\n(39) 0.3243\n(41) 0.3022\n(43) 0.3095\n(45) 0.3059\n(47) 0.3004\n(49) 0.2994\n(50) 0.2967\nFlow 1\nTraining 50 epochs \nLoss:\n(0) 2.3284\n(1) 0.7375\n(3) 0.3661\n(5) 0.3464\n(7) 0.3462\n(9) 0.3234\n(11) 0.3281\n(13) 0.3144\n(15) 0.3145\n(17) 0.3101\n(19) 0.3084\n(21) 0.3180\n(23) 0.3074\n(25) 0.3131\n(27) 0.3110\n(29) 0.3013\n(31) 0.3059\n(33) 0.3105\n(35) 0.3047\n(37) 0.3038\n(39) 0.3097\n(41) 0.3102\n(43) 0.3028\n(45) 0.3023\n(47) 0.2999\n(49) 0.3092\n(50) 0.3024\nFlow 2\nTraining 50 epochs \nLoss:\n(0) 2.3538\n(1) 0.7529\n(3) 0.3626\n(5) 0.3337\n(7) 0.3332\n(9) 0.3190\n(11) 0.3107\n(13) 0.3072\n(15) 0.3155\n(17) 0.3028\n(19) 0.3102\n(21) 0.3037\n(23) 0.3286\n(25) 0.3076\n(27) 0.3027\n(29) 0.3105\n(31) 0.3024\n(33) 0.3078\n(35) 0.3043\n(37) 0.3030\n(39) 0.3021\n(41) 0.3017\n(43) 0.3038\n(45) 0.2991\n(47) 0.3008\n(49) 0.3003\n(50) 0.2968\nFlow 3\nTraining 50 epochs \nLoss:\n(0) 2.2244\n(1) 0.6808\n(3) 0.3840\n(5) 0.3401\n(7) 0.3282\n(9) 0.3423\n(11) 0.3291\n(13) 0.3147\n(15) 0.3249\n(17) 0.3200\n(19) 0.3098\n(21) 0.3080\n(23) 0.3015\n(25) 0.3106\n(27) 0.3192\n(29) 0.3043\n(31) 0.3100\n(33) 0.3017\n(35) 0.2988\n(37) 0.3098\n(39) 0.2996\n(41) 0.3074\n(43) 0.3054\n(45) 0.3015\n(47) 0.3046\n(49) 0.2989\n(50) 0.3028\nCPU times: user 10min 18s, sys: 1min 36s, total: 11min 54s\nWall time: 2min 39s\n</pre> <p>The losses returned by a <code>FlowEnsemble</code> are saved in a dictionary. Let's plot them.</p> In\u00a0[7]: Copied! <pre>for n, l in losses.items():\n    plt.plot(l, label=n)\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training loss\")\nplt.show()\n</pre> for n, l in losses.items():     plt.plot(l, label=n) plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Training loss\") plt.show() <p>Perfect!</p> <p>Now we can draw samples from the ensemble, using the <code>sample</code> method. Let's draw 10000 samples and make another histogram to see if it matches the data.</p> In\u00a0[8]: Copied! <pre>samples = flowEns.sample(10000, seed=0)\n</pre> samples = flowEns.sample(10000, seed=0) In\u00a0[9]: Copied! <pre>plt.hist2d(samples['x'], samples['y'], bins=200)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n</pre> plt.hist2d(samples['x'], samples['y'], bins=200) plt.xlabel('x') plt.ylabel('y') plt.show() <p>Looks great!</p> <p>Note that when sampling from the ensemble, it will draw uniformly from the constituent flows in the ensemble. So, e.g., if your ensemble has 4 flows and you ask for 16 samples, it will draw 4 sample from each of the 4 flows in the sample. The process is slightly different if the number of flows doesn't evenly divide the number of samples. If you have 4 flows and ask for 17 samples, it will draw 5 samples from each flow, so that you now have 20 samples, and then it will randomly select 17 of those.</p> <p>There is also one more important feature to note. In the <code>sample</code> method, if you set <code>returnEnsemble=True</code>, then it will sample <code>nsamples</code> from each flow, and then will return all of those samples, sorted by the flow they came from. This allows you to e.g. compare samples from each of the constituent flows and see if they're consistent with one another.</p> <p>Let's draw 4 samples from each flow and print them so we can get an idea of how this works:</p> In\u00a0[10]: Copied! <pre>flowEns.sample(4, seed=0, returnEnsemble=True)\n</pre> flowEns.sample(4, seed=0, returnEnsemble=True) Out[10]: x y Flow 0 0 0.153830 0.084099 1 0.036924 1.021060 2 -0.215726 1.051067 3 -0.223551 0.964875 Flow 1 0 0.137350 0.013202 1 0.141463 1.031508 2 -0.247361 1.053012 3 -0.209796 0.986897 Flow 2 0 0.185029 0.064171 1 -0.129488 1.030470 2 -0.432348 0.987865 3 -0.419133 0.906174 Flow 3 0 0.087466 0.151030 1 0.047206 1.040986 2 -0.293748 1.041880 3 -0.255163 0.969795 <p>We can also use the ensemble to calculate redshift posteriors using the <code>posterior</code> method. This works by calculating the unnormalized posterior for each constituent flow, averaging over them all, then normalizing. (Note we can do the same thing with the <code>log_prob</code> method)</p> In\u00a0[11]: Copied! <pre>grid = jnp.linspace(-2, 2, 100)\npdfs = flowEns.posterior(data[:100], column=\"x\", grid=grid)\n</pre> grid = jnp.linspace(-2, 2, 100) pdfs = flowEns.posterior(data[:100], column=\"x\", grid=grid) <p>Let's plot the first posterior.</p> In\u00a0[12]: Copied! <pre>plt.plot(grid, pdfs[0])\nplt.title(f\"$y$ = {data['y'][0]:.2f}\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$p(x|y=0.78)$\")\nplt.show()\n</pre> plt.plot(grid, pdfs[0]) plt.title(f\"$y$ = {data['y'][0]:.2f}\") plt.xlabel(\"$x$\") plt.ylabel(\"$p(x|y=0.78)$\") plt.show() <p>Similarly to the <code>sample</code> method, we can set <code>returnEnsemble=True</code> to return the pdfs for each of the constituent flows.</p> In\u00a0[13]: Copied! <pre>ensemble_pdfs = flowEns.posterior(data[:100], column=\"x\", grid=grid, returnEnsemble=True)\n</pre> ensemble_pdfs = flowEns.posterior(data[:100], column=\"x\", grid=grid, returnEnsemble=True) <p>the returned array has shape (nsamples, nflows, grid.size):</p> In\u00a0[14]: Copied! <pre>ensemble_pdfs.shape\n</pre> ensemble_pdfs.shape Out[14]: <pre>(100, 4, 100)</pre> <p>Let's plot all the pdfs from the ensemble for the first galaxy:</p> In\u00a0[15]: Copied! <pre>for i in range(ensemble_pdfs.shape[1]):\n    plt.plot(grid, ensemble_pdfs[0,i])\nplt.plot(grid, pdfs[0], ls=\"--\", c=\"k\", label=\"Mean\")\nplt.legend()\nplt.title(f\"$y$ = {data['y'][0]:.2f}\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$p(x|y=0.78)$\")\nplt.show()\n</pre> for i in range(ensemble_pdfs.shape[1]):     plt.plot(grid, ensemble_pdfs[0,i]) plt.plot(grid, pdfs[0], ls=\"--\", c=\"k\", label=\"Mean\") plt.legend() plt.title(f\"$y$ = {data['y'][0]:.2f}\") plt.xlabel(\"$x$\") plt.ylabel(\"$p(x|y=0.78)$\") plt.show() <p>Now let's save the ensemble to a file that can be loaded later:</p> In\u00a0[18]: Copied! <pre>flowEns.save(\"example-ensemble.pzflow.pkl\")\n</pre> flowEns.save(\"example-ensemble.pzflow.pkl\") <p>This file can be loaded on Flow instantiation:</p> In\u00a0[19]: Copied! <pre>flowEns = FlowEnsemble(file=\"example-ensemble.pzflow.pkl\")\n</pre> flowEns = FlowEnsemble(file=\"example-ensemble.pzflow.pkl\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/ensemble_demo/#flow-ensemble-demo","title":"Flow ensemble demo\u00b6","text":"<p>This notebook demonstrates building a deep ensemble of normalizing flows with PZFlow. A deep ensemble instantiates multiple Flows with different randomized initial parameters, and trains them on the same data. This ensemble of trained flows accounts for epistemic uncertainty, and is a form of approximate Bayesian machine learning.</p>"},{"location":"tutorials/gaussian_errors/","title":"Convolving Gaussian Errors","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre># !pip install pzflow matplotlib\n</pre> # !pip install pzflow matplotlib In\u00a0[1]: Copied! <pre>from pzflow import Flow\nfrom pzflow.examples import get_twomoons_data\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n</pre> from pzflow import Flow from pzflow.examples import get_twomoons_data  import jax.numpy as jnp import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\" <p>Let's load the two moons data again. This time, we will add columns for errors. Let's set them at the 10% level.</p> In\u00a0[3]: Copied! <pre># load the data\ndata = get_twomoons_data()\n# 10% errors\ndata[\"x_err\"] = 0.10 * jnp.abs(data[\"x\"].values)\ndata[\"y_err\"] = 0.10 * jnp.abs(data[\"y\"].values)\ndata\n</pre> # load the data data = get_twomoons_data() # 10% errors data[\"x_err\"] = 0.10 * jnp.abs(data[\"x\"].values) data[\"y_err\"] = 0.10 * jnp.abs(data[\"y\"].values) data <pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> Out[3]: x y x_err y_err 0 -0.748695 0.777733 0.074870 0.077773 1 1.690101 -0.207291 0.169010 0.020729 2 2.008558 0.285932 0.200856 0.028593 3 1.291547 -0.441167 0.129155 0.044117 4 0.808686 -0.481017 0.080869 0.048102 ... ... ... ... ... 99995 1.642738 -0.221286 0.164274 0.022129 99996 0.981221 0.327815 0.098122 0.032781 99997 0.990856 0.182546 0.099086 0.018255 99998 -0.343144 0.877573 0.034314 0.087757 99999 1.851718 0.008531 0.185172 0.000853 <p>100000 rows \u00d7 4 columns</p> <p>Notice how I saved the errors in columns with the suffix \"_err\". Anytime you tell PZFlow to convolve errors (see below), it will look for columns with \"_err\" suffixes. If it can't find an error column for a certain variable, it will assume that variable has zero error. It is also important to know that PZFlow will assume the errors are Gaussian, unless you explicitly tell it otherwise, like in this example.</p> <p>Also, note that I didn't actually add the Gaussian noise to the <code>x</code> and <code>y</code> variables themselves. This is just so we can clearly see the effects of convolving errors below. The initial data will be noiseless, but since we will convolve errors during training, the samples from the trained flow will be noisy.</p> <p>Here is the noiseless data:</p> In\u00a0[4]: Copied! <pre>plt.hist2d(data['x'], data['y'], bins=200)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n</pre> plt.hist2d(data['x'], data['y'], bins=200) plt.xlabel('x') plt.ylabel('y') plt.show() <p>Now we will build the same flow we built in the Intro.</p> In\u00a0[5]: Copied! <pre>flow = Flow([\"x\", \"y\"])\n</pre> flow = Flow([\"x\", \"y\"]) <p>This time during training, we will set <code>convolve_errs=True</code>. This means that during each epoch of training, we will resample the training set from the Gaussian distributions set by <code>x_err</code> and <code>y_err</code>.</p> In\u00a0[6]: Copied! <pre>%%time\nlosses = flow.train(data, convolve_errs=True, verbose=True)\n</pre> %%time losses = flow.train(data, convolve_errs=True, verbose=True) <pre>Training 100 epochs \nLoss:\n(0) 2.3212\n(1) 0.8941\n(6) 0.6283\n(11) 0.6253\n(16) 0.6220\n(21) 0.5972\n(26) 0.6049\n(31) 0.5934\n(36) 0.6154\n(41) 0.5999\n(46) 0.6087\n(51) 0.5977\n(56) 0.6036\n(61) 0.5937\n(66) 0.5781\n(71) 0.5865\n(76) 0.5848\n(81) 0.5930\n(86) 0.5863\n(91) 0.5867\n(96) 0.6018\n(100) 0.6002\nCPU times: user 5min 13s, sys: 48 s, total: 6min 1s\nWall time: 1min 22s\n</pre> <p>Now let's plot the training losses to make sure everything looks like we expect it to...</p> In\u00a0[7]: Copied! <pre>plt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training loss\")\nplt.show()\n</pre> plt.plot(losses) plt.xlabel(\"Epoch\") plt.ylabel(\"Training loss\") plt.show() <p>Perfect!</p> <p>Now we can draw samples from the flow, using the <code>sample</code> method.</p> In\u00a0[8]: Copied! <pre>samples = flow.sample(10000, seed=0)\n</pre> samples = flow.sample(10000, seed=0) In\u00a0[9]: Copied! <pre>plt.hist2d(samples['x'], samples['y'], bins=200)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n</pre> plt.hist2d(samples['x'], samples['y'], bins=200) plt.xlabel('x') plt.ylabel('y') plt.show() <p>Notice how the learned distribution is noisy - that is because we resampled the training set from the Gaussian error distributions on each epoch of training. In particular, notice how the noise increases as the magnitude of $x$ and $y$ increases - this is exactly what we expected by setting the error of each to 10% of the true value.</p> <p>We can also calculate posteriors with error convolution by setting the <code>err_samples</code> parameter. E.g. setting <code>err_samples=100</code> tells the posterior method to draw 100 samples from the Gaussian error distribution for each galaxy, calculate posteriors for each of those samples, and then average over the samples.</p> <p>Below, we will select one galaxy and calculate the posterior for various values of <code>err_samples</code>.</p> In\u00a0[10]: Copied! <pre>grid = jnp.linspace(-2, 2, 100)\npdfs = dict()\nfor err_samples in [1, 10, 100, 1000, 10000]:\n    pdfs[err_samples] = flow.posterior(data[:1], column=\"x\", grid=grid, err_samples=err_samples, seed=0)[0]\n</pre> grid = jnp.linspace(-2, 2, 100) pdfs = dict() for err_samples in [1, 10, 100, 1000, 10000]:     pdfs[err_samples] = flow.posterior(data[:1], column=\"x\", grid=grid, err_samples=err_samples, seed=0)[0] In\u00a0[11]: Copied! <pre>for i, pdf in pdfs.items():\n    plt.plot(grid, pdf, label=i)\nplt.legend()\nplt.title(f\"$y$ = {data['y'][0]:.2f}\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$p(x|y=0.78)$\")\nplt.show()\n</pre> for i, pdf in pdfs.items():     plt.plot(grid, pdf, label=i) plt.legend() plt.title(f\"$y$ = {data['y'][0]:.2f}\") plt.xlabel(\"$x$\") plt.ylabel(\"$p(x|y=0.78)$\") plt.show() <p>We can see how drawing more samples in the error convolution smooths out the posterior as you might expect.</p> <p>Note that the <code>log_prob</code> method works identically.</p>"},{"location":"tutorials/gaussian_errors/#convolving-gaussian-errors","title":"Convolving Gaussian errors\u00b6","text":"<p>This notebook demonstrates how to train a flow on data that has Gaussian errors/uncertainties, as well as convolving those errors in log_prob and posterior calculations. We will use the two moons data set again.</p>"},{"location":"tutorials/intro/","title":"Introduction","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre># !pip install pzflow matplotlib\n</pre> # !pip install pzflow matplotlib  In\u00a0[1]: Copied! <pre>from pzflow import Flow\nfrom pzflow.examples import get_twomoons_data\n\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n</pre> from pzflow import Flow from pzflow.examples import get_twomoons_data  import jax.numpy as jnp import matplotlib.pyplot as plt  In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\"  <p>First let's load some example data. It's the familiar two moons data set from scikit-learn, loaded in a Pandas DataFrame, which is the data format PZFlow uses on the user-end.</p> In\u00a0[3]: Copied! <pre>data = get_twomoons_data()\ndata\n</pre> data = get_twomoons_data() data  Out[3]: x y 0 -0.748695 0.777733 1 1.690101 -0.207291 2 2.008558 0.285932 3 1.291547 -0.441167 4 0.808686 -0.481017 ... ... ... 99995 1.642738 -0.221286 99996 0.981221 0.327815 99997 0.990856 0.182546 99998 -0.343144 0.877573 99999 1.851718 0.008531 <p>100000 rows \u00d7 2 columns</p> <p>Let's plot it to see what it looks like.</p> In\u00a0[4]: Copied! <pre>plt.hist2d(data[\"x\"], data[\"y\"], bins=200)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n</pre> plt.hist2d(data[\"x\"], data[\"y\"], bins=200) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show()  <p>Now let's build a normalizing flow. The details of constructing a normalizing flow are explored in the following tutorial notebooks, but for now, we can use the default flow built into PZFlow. This flow was designed to work well out-of-the-box for most data sets.</p> <p>The only thing you are required to supply is the name of the columns in your data set. As you can see in the pandas DataFrame above, our columns are named <code>\"x\"</code> and <code>\"y\"</code>.</p> In\u00a0[5]: Copied! <pre>flow = Flow([\"x\", \"y\"])\n</pre> flow = Flow([\"x\", \"y\"])  <p>Now we can train our normalizing flow. This is as simple as calling <code>flow.train(data)</code>. There are several training parameters you can set, including the number of epochs, the batch size, the optimizer, and the random seed. See the <code>Flow</code> documentation for more details. For this example, let's use the defaults, but set <code>verbose=True</code> so that training losses are printed throughout the training process.</p> In\u00a0[6]: Copied! <pre>%%time\nlosses = flow.train(data, verbose=True)\n</pre> %%time losses = flow.train(data, verbose=True) <pre>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <pre>Training 100 epochs \nLoss:\n(0) 2.3212\n(1) 0.7040\n(6) 0.3620\n(11) 0.3410\n(16) 0.3193\n(21) 0.3124\n(26) 0.3140\n(31) 0.3226\n(36) 0.3028\n(41) 0.3105\n(46) 0.3054\n(51) 0.2984\n(56) 0.3031\n(61) 0.2963\n(66) 0.3060\n(71) 0.3016\n(76) 0.3027\n(81) 0.2962\n(86) 0.3023\n(91) 0.3067\n(96) 0.2991\n(100) 0.2978\nCPU times: user 4min 29s, sys: 46.5 s, total: 5min 16s\nWall time: 1min 35s\n</pre> <p>Now let's plot the training losses to make sure everything looks like we expect it to...</p> In\u00a0[9]: Copied! <pre>plt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training loss\")\nplt.show()\n</pre> plt.plot(losses) plt.xlabel(\"Epoch\") plt.ylabel(\"Training loss\") plt.show()  <p>Perfect!</p> <p>Now we can draw samples from the flow, using the <code>sample</code> method. Let's draw 10000 samples and make another histogram to see if it matches the data.</p> In\u00a0[10]: Copied! <pre>samples = flow.sample(10_000, seed=0)\n</pre> samples = flow.sample(10_000, seed=0)  In\u00a0[11]: Copied! <pre>plt.hist2d(samples[\"x\"], samples[\"y\"], bins=200)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n</pre> plt.hist2d(samples[\"x\"], samples[\"y\"], bins=200) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show()  <p>Looks great!</p> <p>We can also use the flow to calculate redshift posteriors using the <code>posterior</code> method. We need to provide the name of the column we want to calculate a posterior for, as well as a grid on which to calculate the posterior.</p> In\u00a0[12]: Copied! <pre>grid = jnp.linspace(-2, 2, 100)\npdfs = flow.posterior(data, column=\"x\", grid=grid)\n</pre> grid = jnp.linspace(-2, 2, 100) pdfs = flow.posterior(data, column=\"x\", grid=grid)  <p>The result is a big array of posteriors:</p> In\u00a0[13]: Copied! <pre>pdfs\n</pre> pdfs  Out[13]: <pre>Array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 7.9137310e-05,\n        2.5412094e-04, 3.7483254e-04],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.2376558e-02,\n        1.1772527e-02, 7.5370832e-03],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.0527854e+00,\n        1.8525983e+00, 1.8694268e+00],\n       ...,\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.6310947e+00,\n        1.8701038e+00, 1.2859117e+00],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 2.3276938e-05,\n        7.8079924e-05, 1.1970673e-04],\n       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 1.7900342e+00,\n        8.5315311e-01, 1.5609255e-01]], dtype=float32)</pre> <p>Let's plot the first posterior.</p> In\u00a0[14]: Copied! <pre>plt.plot(grid, pdfs[0])\nplt.title(f\"$y$ = {data['y'][0]:.2f}\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$p(x|y)$\")\nplt.show()\n</pre> plt.plot(grid, pdfs[0]) plt.title(f\"$y$ = {data['y'][0]:.2f}\") plt.xlabel(\"$x$\") plt.ylabel(\"$p(x|y)$\") plt.show()  <p>Now let's store some information with the flow about the data it was trained on.</p> In\u00a0[15]: Copied! <pre>import pzflow\n\nflow.info = f\"\"\"\nThis is an example flow, trained on 100,000 points from the scitkit-learn\ntwo moons data set.\n\nThe data set used to train this flow is available in the `examples` module:\n&gt;&gt;&gt; from pzflow.examples import two_moons_data\n&gt;&gt;&gt; data = get_twomoons_data()\n\nThis flow was created with pzflow version {pzflow.__version__}\n\"\"\"\n</pre> import pzflow  flow.info = f\"\"\" This is an example flow, trained on 100,000 points from the scitkit-learn two moons data set.  The data set used to train this flow is available in the `examples` module: &gt;&gt;&gt; from pzflow.examples import two_moons_data &gt;&gt;&gt; data = get_twomoons_data()  This flow was created with pzflow version {pzflow.__version__} \"\"\"  In\u00a0[16]: Copied! <pre>print(flow.info)\n</pre> print(flow.info)  <pre>\nThis is an example flow, trained on 100,000 points from the scitkit-learn\ntwo moons data set.\n\nThe data set used to train this flow is available in the `examples` module:\n&gt;&gt;&gt; from pzflow.examples import two_moons_data\n&gt;&gt;&gt; data = get_twomoons_data()\n\nThis flow was created with pzflow version 3.1.0\n\n</pre> <p>Now let's save the flow to a file that can be loaded later:</p> In\u00a0[17]: Copied! <pre>flow.save(\"example_flow.pzflow.pkl\")\n</pre> flow.save(\"example_flow.pzflow.pkl\")  <p>This file can be loaded on Flow instantiation:</p> In\u00a0[18]: Copied! <pre>flow = Flow(file=\"example_flow.pzflow.pkl\")\n</pre> flow = Flow(file=\"example_flow.pzflow.pkl\")  In\u00a0[19]: Copied! <pre># 80% for the training set\ntrain_set = data[: int(0.8 * len(data))]\n\n# 20% for the training set\nval_set = data[int(0.8 * len(data)) :]\n</pre> # 80% for the training set train_set = data[: int(0.8 * len(data))]  # 20% for the training set val_set = data[int(0.8 * len(data)) :]  In\u00a0[23]: Copied! <pre>%%time\nflow = Flow([\"x\", \"y\"])\ntrain_losses, val_losses = flow.train(train_set, val_set, verbose=True)\n</pre> %%time flow = Flow([\"x\", \"y\"]) train_losses, val_losses = flow.train(train_set, val_set, verbose=True) <pre>Training 100 epochs \nLoss:\n(0) 2.3199  2.3266\n(1) 0.8355  0.8384\n(6) 0.3615  0.3666\n(11) 0.3367  0.3402\n(16) 0.3243  0.3289\n(21) 0.3074  0.3112\n(26) 0.3118  0.3160\n(31) 0.3527  0.3557\n(36) 0.3215  0.3239\n(41) 0.3218  0.3250\n(46) 0.3075  0.3121\n(51) 0.3153  0.3196\n(56) 0.3564  0.3570\n(61) 0.3089  0.3132\n(66) 0.3024  0.3075\n(71) 0.3291  0.3334\n(76) 0.3014  0.3073\n(81) 0.3025  0.3098\n(86) 0.3099  0.3153\n(91) 0.3163  0.3203\n(96) 0.2967  0.3035\n(100) 0.3042  0.3093\nCPU times: user 3min 45s, sys: 43.2 s, total: 4min 28s\nWall time: 1min 22s\n</pre> <p>Now during training, the training losses are printed on the left, and the validation losses on the right.</p> <p>Let's plot the losses again:</p> In\u00a0[28]: Copied! <pre>plt.plot(train_losses, label=\"Training\")\nplt.plot(val_losses, label=\"Validation\")\nplt.legend()\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()\n</pre> plt.plot(train_losses, label=\"Training\") plt.plot(val_losses, label=\"Validation\") plt.legend() plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.show()  <p>In this example, the validation loss closely tracks the training loss. This is expected, since both sets were drawn from the same distribution.</p> <p>Note that by default, PZFlow will use the parameters corresponding to the epoch with the lowest validation loss.</p>"},{"location":"tutorials/intro/#intro-to-pzflow","title":"Intro to PZFlow\u00b6","text":"<p>This notebook demonstrates building a normalizing flow with PZFlow to learn the joint probability distribution of some 2-D data.</p> <p>You do not need to have any previous knowledge of normalizing flows to get started with PZFlow.</p> <p>However if you are interested, here are some good sources:</p> <ul> <li>Eric Jang's tutorial: part 1, part 2</li> <li>Here is a list of papers, blogs, videos, and packages</li> <li>Two good intro papers using Coupling Layers: NICE, Real NVP</li> <li>The paper on Neural Spline Couplings</li> </ul>"},{"location":"tutorials/intro/#tracking-the-validation-loss","title":"Tracking the validation loss\u00b6","text":"<p>Often in machine learning applications, we want to track validation loss as we train to make sure that we don't overfit. This is very easy to do with PZFlow. Below, we will repeat the training above, except this time we will provide a validation set to the training function.</p> <p>First, let's split the data set into a training and validation set:</p>"},{"location":"tutorials/marginalization/","title":"Marginalizing Variables","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[\u00a0]: Copied! <pre># !pip install pzflow matplotlib\n</pre> # !pip install pzflow matplotlib In\u00a0[1]: Copied! <pre>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom pzflow.examples import get_example_flow\n</pre> import jax.numpy as jnp import matplotlib.pyplot as plt from pzflow.examples import get_example_flow In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\" <p>First let's load the pre-trained flow, and use it to generate some samples:</p> In\u00a0[47]: Copied! <pre>flow = get_example_flow()\nsamples = flow.sample(2, seed=0)\n</pre> flow = get_example_flow() samples = flow.sample(2, seed=0) In\u00a0[48]: Copied! <pre>samples\n</pre> samples Out[48]: redshift u g r i z y 0 0.386102 28.406404 27.508736 26.419685 26.196583 25.901255 25.889046 1 2.118688 28.220659 27.798203 27.371502 27.211376 26.869678 26.502558 <p>Remember that we can calculate posteriors for the data in samples. For example, let's plot redshift posteriors:</p> In\u00a0[58]: Copied! <pre>grid = jnp.linspace(0.1, 2.4, 100)\npdfs = flow.posterior(samples, column=\"redshift\", grid=grid)\n</pre> grid = jnp.linspace(0.1, 2.4, 100) pdfs = flow.posterior(samples, column=\"redshift\", grid=grid) In\u00a0[59]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(4, 2), dpi=120, constrained_layout=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    \n    ax.plot(grid, pdfs[i], label=\"Redshift posterior\")\n    \n    ztrue = samples[\"redshift\"][i]\n    ax.axvline(ztrue, c=\"C3\", label=\"True redshift\")\n    ax.set(\n        xlabel=\"redshift\", \n        xlim=(ztrue - 0.25, ztrue + 0.25), \n        yticks=[]\n    )\n    \naxes[0].legend(\n    bbox_to_anchor=(0.25, 1.05, 1.5, 0.2), \n    loc=\"lower left\",\n    mode=\"expand\", \n    borderaxespad=0, \n    ncol=2,\n    fontsize=8,\n)\n\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(4, 2), dpi=120, constrained_layout=True)  for i, ax in enumerate(axes.flatten()):          ax.plot(grid, pdfs[i], label=\"Redshift posterior\")          ztrue = samples[\"redshift\"][i]     ax.axvline(ztrue, c=\"C3\", label=\"True redshift\")     ax.set(         xlabel=\"redshift\",          xlim=(ztrue - 0.25, ztrue + 0.25),          yticks=[]     )      axes[0].legend(     bbox_to_anchor=(0.25, 1.05, 1.5, 0.2),      loc=\"lower left\",     mode=\"expand\",      borderaxespad=0,      ncol=2,     fontsize=8, )  plt.show() <p>But what if we have missing values? E.g. let's imagine that galaxy 1 wasn't observed in the u band, while galaxy 2 wasn't observed in the u or y bands. We will mark these non-observations with the value 99:</p> In\u00a0[60]: Copied! <pre># make a new copy of the samples\nsamples2 = samples.copy()\n# make the non-observations\nsamples2.iloc[0, 1] = 99\nsamples2.iloc[1, 1] = 99\nsamples2.iloc[1, -1] = 99\n# print the new samples\nsamples2\n</pre> # make a new copy of the samples samples2 = samples.copy() # make the non-observations samples2.iloc[0, 1] = 99 samples2.iloc[1, 1] = 99 samples2.iloc[1, -1] = 99 # print the new samples samples2 Out[60]: redshift u g r i z y 0 0.386102 99.0 27.508736 26.419685 26.196583 25.901255 25.889046 1 2.118688 99.0 27.798203 27.371502 27.211376 26.869678 99.000000 <p>Now if we want to calculate posteriors, we can't simply call <code>flow.posterior()</code> as before because the flow will think that 99 is the actual value for those bands, rather than just a flag for a missing value. What we can do, however, is pass <code>marg_rules</code>, which is a dictionary of rules that tells the Flow how to marginalize over missing variables.</p> <p><code>marg_rules</code> must include:</p> <ul> <li>\"flag\": 99, which tells the posterior method that 99 is the flag for missing values</li> <li>\"u\": callable, which returns an array of values for the u band over which to marginalize</li> <li>\"y\": callable, which returns an array of values for the y band over which to marginalize</li> </ul> <p>\"u\" and \"y\" both map to callable, because you can use a function of the other values to decide what values of u and y to marginalize over. For example, maybe you expect the value of u to be close to the value of g. In which case you might use:</p> <pre><code>\"u\": lambda row: np.linspace(row[\"g\"] - 1, row[\"g\"] + 1, 100)\n</code></pre> <p>The only constraint is that regardless of the values of the other variables, the callable must always return an array of the same length.</p> <p>For this example, we won't make the marginalization rules a function of the other variables, but will instead return a fixed array.</p> In\u00a0[63]: Copied! <pre>marg_rules = {\n    \"flag\": 99, # tells the posterior method that 99 means missing value\n    \"u\": lambda row: jnp.linspace(27, 29, 40), # the array of u values to marginalize over\n    \"y\": lambda row: jnp.linspace(25, 27, 40), # the array of y values to marginalize over\n}\n\npdfs2 = flow.posterior(samples2, column=\"redshift\", grid=grid, marg_rules=marg_rules)\n</pre> marg_rules = {     \"flag\": 99, # tells the posterior method that 99 means missing value     \"u\": lambda row: jnp.linspace(27, 29, 40), # the array of u values to marginalize over     \"y\": lambda row: jnp.linspace(25, 27, 40), # the array of y values to marginalize over }  pdfs2 = flow.posterior(samples2, column=\"redshift\", grid=grid, marg_rules=marg_rules) In\u00a0[64]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(4, 2), dpi=120, constrained_layout=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    \n    ax.plot(grid, pdfs[i], label=\"Posterior w/ all bands\")\n    ax.plot(grid, pdfs2[i], label=\"Posterior w/ missing bands marginalized\")\n    \n    ztrue = samples[\"redshift\"][i]\n    ax.axvline(ztrue, c=\"C3\", label=\"True redshift\")\n    ax.set(\n        xlabel=\"redshift\", \n        xlim=(ztrue - 0.25, ztrue + 0.25), \n        yticks=[]\n    )\n    \naxes[0].legend(\n    bbox_to_anchor=(0, 1.05, 2, 0.2), \n    loc=\"lower left\",\n    mode=\"expand\", \n    borderaxespad=0, \n    ncol=2,\n    fontsize=7.5,\n)\n\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(4, 2), dpi=120, constrained_layout=True)  for i, ax in enumerate(axes.flatten()):          ax.plot(grid, pdfs[i], label=\"Posterior w/ all bands\")     ax.plot(grid, pdfs2[i], label=\"Posterior w/ missing bands marginalized\")          ztrue = samples[\"redshift\"][i]     ax.axvline(ztrue, c=\"C3\", label=\"True redshift\")     ax.set(         xlabel=\"redshift\",          xlim=(ztrue - 0.25, ztrue + 0.25),          yticks=[]     )      axes[0].legend(     bbox_to_anchor=(0, 1.05, 2, 0.2),      loc=\"lower left\",     mode=\"expand\",      borderaxespad=0,      ncol=2,     fontsize=7.5, )  plt.show() <p>You can see that for the low-redshift galaxy, throwing away the $u$ band has almost no impact. But for the high-redshift galaxy, throwing away the $u$ band biases the posterior towards higher redshifts.</p> <p>Warning that marginalizing over fine grids quickly gets very computationally expensive, especially when you have rows in your data frame that are missing multiple values.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/marginalization/#marginalization-during-posterior-calculation","title":"Marginalization during posterior calculation\u00b6","text":"<p>This example notebook demonstrates how to marginalize over missing variables during posterior calculation. We will use the Flow trained in the redshift example.</p>"},{"location":"tutorials/nongaussian_errors/","title":"Convolving Non-Gaussian Errors","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[\u00a0]: Copied! <pre># !pip install pzflow matplotlib\n</pre> # !pip install pzflow matplotlib In\u00a0[5]: Copied! <pre>import numpy as np\nimport jax.numpy as jnp\nfrom jax import random\nimport matplotlib.pyplot as plt\n\nfrom pzflow import Flow\nfrom pzflow.examples import get_galaxy_data\n</pre> import numpy as np import jax.numpy as jnp from jax import random import matplotlib.pyplot as plt  from pzflow import Flow from pzflow.examples import get_galaxy_data In\u00a0[6]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\" <p>First let's load the example galaxy data set included with PZFlow, and include photometric errors. For simplicity, we will assume all bands have error = 0.1</p> In\u00a0[7]: Copied! <pre>data = get_galaxy_data()\nfor col in data.columns[1:]:\n    data[f\"{col}_err\"] = 0.1 * np.ones(data.shape[0])\ndata.head()\n</pre> data = get_galaxy_data() for col in data.columns[1:]:     data[f\"{col}_err\"] = 0.1 * np.ones(data.shape[0]) data.head() Out[7]: redshift u g r i z y u_err g_err r_err i_err z_err y_err 0 0.287087 26.759261 25.901778 25.187710 24.932318 24.736903 24.671623 0.1 0.1 0.1 0.1 0.1 0.1 1 0.293313 27.428358 26.679299 25.977161 25.700094 25.522763 25.417632 0.1 0.1 0.1 0.1 0.1 0.1 2 1.497276 27.294001 26.068798 25.450055 24.460507 23.887221 23.206112 0.1 0.1 0.1 0.1 0.1 0.1 3 0.283310 28.154075 26.283166 24.599570 23.723491 23.214108 22.860012 0.1 0.1 0.1 0.1 0.1 0.1 4 1.545183 29.276065 27.878301 27.333528 26.543374 26.061941 25.383056 0.1 0.1 0.1 0.1 0.1 0.1 <p>Now, we need to build the machinery to handle non-Gaussian errors. PZFlow convolves errors by sampling from an error model, which by default is Gaussian. If we want to convolve non-Gaussian errors, we need to pass the <code>Flow</code> constructor an error model that tells it how to sample from our non-Gaussian distribution.</p> <p>The error model must be a callable that takes the following arguments:</p> <ul> <li>key is a jax rng key, e.g. jax.random.PRNGKey(0)</li> <li>X is a 2 dimensional array of data variables, where the order of variables matches the order of the columns in data_columns</li> <li>Xerr is the corresponding 2 dimensional array of errors</li> <li>nsamples is the number of samples to draw from the error distribution</li> </ul> <p>and it must return an array of samples with the shape <code>(X.shape[0], nsamples, X.shape[1])</code></p> <p>Below we build a photometric error model, which takes the exponential of the magnitudes to convert to flux values, adds Gaussian flux errors, then takes the log to convert back to magnitudes.</p> In\u00a0[9]: Copied! <pre>def photometric_error_model(\n    key,\n    X: np.ndarray,\n    Xerr: np.ndarray,\n    nsamples: int,\n) -&gt; np.ndarray:\n    \n    # calculate fluxes\n    F = 10 ** (X / -2.5)\n    # calculate flux errors\n    dF = np.log(10) / 2.5 * F * Xerr\n    \n    # add Gaussian errors\n    eps = random.normal(key, shape=(F.shape[0], nsamples, F.shape[1]))\n    F = F[:, None, :] + eps * dF[:, None, :]\n    \n    # add a flux floor to avoid infinite magnitudes\n    # this flux corresponds to a max magnitude of 30\n    F = np.clip(F, 1e-12, None)\n    \n    # calculate magnitudes\n    M = -2.5 * np.log10(F)\n    \n    return M\n</pre> def photometric_error_model(     key,     X: np.ndarray,     Xerr: np.ndarray,     nsamples: int, ) -&gt; np.ndarray:          # calculate fluxes     F = 10 ** (X / -2.5)     # calculate flux errors     dF = np.log(10) / 2.5 * F * Xerr          # add Gaussian errors     eps = random.normal(key, shape=(F.shape[0], nsamples, F.shape[1]))     F = F[:, None, :] + eps * dF[:, None, :]          # add a flux floor to avoid infinite magnitudes     # this flux corresponds to a max magnitude of 30     F = np.clip(F, 1e-12, None)          # calculate magnitudes     M = -2.5 * np.log10(F)          return M <p>Now we can construct the Flow, this time passing the error model</p> In\u00a0[10]: Copied! <pre>flow = Flow(\n    [\"redshift\"] + list(\"ugrizy\"), \n    data_error_model=photometric_error_model,\n)\n</pre> flow = Flow(     [\"redshift\"] + list(\"ugrizy\"),      data_error_model=photometric_error_model, ) <p>Now that we have set up the Flow with the new error model, we can train the flow, calculate posteriors, etc. just like we did in the Gaussian error example.</p> <p>For example, to train with error convolution:</p> In\u00a0[11]: Copied! <pre>%%time\nlosses = flow.train(data, epochs=200, convolve_errs=True, verbose=True)\n</pre> %%time losses = flow.train(data, epochs=200, convolve_errs=True, verbose=True) <pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <pre>Training 200 epochs \nLoss:\n(0) 20.3297\n(1) 2.9366\n(11) 0.1215\n(21) -0.1496\n(31) -0.0601\n(41) -0.1246\n(51) -0.2496\n(61) -0.0759\n(71) 0.1112\n(81) -0.2375\n(91) -0.2352\n(101) -0.2811\n(111) -0.2134\n(121) -0.2033\n(131) -0.2952\n(141) -0.2706\n(151) -0.2759\n(161) -0.2222\n(171) -0.1448\n(181) -0.2966\n(191) -0.1698\n(200) -0.2314\nCPU times: user 1h 8min 26s, sys: 45min 42s, total: 1h 54min 8s\nWall time: 22min 21s\n</pre> <p>And to calculate posteriors with error convolution:</p> In\u00a0[12]: Copied! <pre>grid = np.linspace(0, 3, 100)\npdfs = flow.posterior(data[:10], column=\"redshift\", grid=grid, err_samples=int(1e3))\n</pre> grid = np.linspace(0, 3, 100) pdfs = flow.posterior(data[:10], column=\"redshift\", grid=grid, err_samples=int(1e3)) In\u00a0[13]: Copied! <pre>fig, axes = plt.subplots(2, 2, figsize=(4, 4), dpi=120, constrained_layout=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    \n    ax.plot(grid, pdfs[i], label=\"Posterior\")\n    ax.axvline(data[\"redshift\"][i], c=\"C3\", label=\"True redshift\")\n    ax.set(xlabel=\"redshift\", yticks=[])\n\naxes[0,0].legend()\nplt.show()\n</pre> fig, axes = plt.subplots(2, 2, figsize=(4, 4), dpi=120, constrained_layout=True)  for i, ax in enumerate(axes.flatten()):          ax.plot(grid, pdfs[i], label=\"Posterior\")     ax.axvline(data[\"redshift\"][i], c=\"C3\", label=\"True redshift\")     ax.set(xlabel=\"redshift\", yticks=[])  axes[0,0].legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/nongaussian_errors/#convolving-non-gaussian-errors","title":"Convolving non-Gaussian errors\u00b6","text":"<p>This notebook demonstrates how to train a flow on data that has non-Gaussian errors/uncertainties, as well as convolving those errors in log_prob and posterior calculations. We will use the example galaxy data again.</p> <p>For an example of how to handle Gaussian errors, which is much easier, see the notebook on Gaussian errors.</p>"},{"location":"tutorials/spherical_flow_example/","title":"Modeling Variables with Periodic Topology","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre># !pip install pzflow matplotlib healpy\n</pre> # !pip install pzflow matplotlib healpy In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport healpy as hp\nimport jax.numpy as jnp\n\nfrom pzflow import Flow\nfrom pzflow.bijectors import Chain, NeuralSplineCoupling, Roll, StandardScaler, ShiftBounds\nfrom pzflow.distributions import Joint, Normal, Uniform\nfrom pzflow.examples import get_city_data\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import healpy as hp import jax.numpy as jnp  from pzflow import Flow from pzflow.bijectors import Chain, NeuralSplineCoupling, Roll, StandardScaler, ShiftBounds from pzflow.distributions import Joint, Normal, Uniform from pzflow.examples import get_city_data In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\" <p>First let's load the city data included with pzflow.</p> In\u00a0[3]: Copied! <pre>cities = get_city_data()\ncities\n</pre> cities = get_city_data() cities Out[3]: Country City Population Latitude Longitude 0 Andorra Andorra la Vella 20430.0 42.500000 1.516667 1 Andorra Canillo 3292.0 42.566667 1.600000 2 Andorra Encamp 11224.0 42.533333 1.583333 3 Andorra La Massana 7211.0 42.550000 1.516667 4 Andorra Les Escaldes 15854.0 42.500000 1.533333 ... ... ... ... ... ... 47961 Zimbabwe Redcliffe 38231.0 -19.033333 29.783333 47962 Zimbabwe Rusape 23761.0 -18.533333 32.116667 47963 Zimbabwe Shurugwi 17107.0 -19.666667 30.000000 47964 Zimbabwe Victoria Falls 36702.0 -17.933333 25.833333 47965 Zimbabwe Zvishavane 79876.0 -20.333333 30.033333 <p>47966 rows \u00d7 5 columns</p> <p>We can see we have a list of about 48,000 cities, along with their locations and populations. We will use this to approximately model the population density of the Earth.</p> <p>Let's convert the angles to radians and calculate the logarithm of the populations. In addition, let's convert latitude to a zenith angle, and longitude so that its range is $[0,2\\pi]$ instead of $[-\\pi,+\\pi]$.</p> In\u00a0[4]: Copied! <pre>logpop = np.log10(cities['Population'].to_numpy())\ntheta = np.radians(90 - cities['Latitude'].to_numpy())\nphi = np.radians(180 + cities['Longitude'].to_numpy())\n</pre> logpop = np.log10(cities['Population'].to_numpy()) theta = np.radians(90 - cities['Latitude'].to_numpy()) phi = np.radians(180 + cities['Longitude'].to_numpy()) <p>Let's use <code>healpy</code> to plot a Mollweide projected histogram of our data.</p> In\u00a0[5]: Copied! <pre>def mollweide_hist(theta, phi, weights=None, nside=64, **kwargs):\n    \"\"\"Make a healpix histogram with a Mollweide projection.\n    \n    theta : array\n        Array of zenith angles.\n    phi : array\n        Array of azimuth angles.\n    weights : array\n        Weight of each point.\n    nside : int, default=64\n        HEALPix nside of the target map\n    \"\"\"\n    \n    if weights is not None:\n        theta = np.repeat(theta, np.rint(weights).astype(int))\n        phi = np.repeat(phi, np.rint(weights).astype(int))\n    \n    # convert to HEALPix indices\n    indices = hp.ang2pix(nside, theta, phi - np.pi)\n    idx, counts = np.unique(indices, return_counts=True)\n    npix = hp.nside2npix(nside)\n    \n    # fill the fullsky map\n    hpx_map = np.zeros(npix, dtype=int)\n    hpx_map[idx] = counts\n    \n    # plot histogram\n    hp.mollview(np.log10(hpx_map+1), **kwargs)\n</pre> def mollweide_hist(theta, phi, weights=None, nside=64, **kwargs):     \"\"\"Make a healpix histogram with a Mollweide projection.          theta : array         Array of zenith angles.     phi : array         Array of azimuth angles.     weights : array         Weight of each point.     nside : int, default=64         HEALPix nside of the target map     \"\"\"          if weights is not None:         theta = np.repeat(theta, np.rint(weights).astype(int))         phi = np.repeat(phi, np.rint(weights).astype(int))          # convert to HEALPix indices     indices = hp.ang2pix(nside, theta, phi - np.pi)     idx, counts = np.unique(indices, return_counts=True)     npix = hp.nside2npix(nside)          # fill the fullsky map     hpx_map = np.zeros(npix, dtype=int)     hpx_map[idx] = counts          # plot histogram     hp.mollview(np.log10(hpx_map+1), **kwargs) In\u00a0[6]: Copied! <pre>mollweide_hist(theta, phi, \n               weights=logpop, \n               nside=64, \n               title=\"Human population density\", \n               flip=\"geo\", \n               cbar=False)\n</pre> mollweide_hist(theta, phi,                 weights=logpop,                 nside=64,                 title=\"Human population density\",                 flip=\"geo\",                 cbar=False) <p>Now we want to model this data with a normalizing flow. As we mentioned above, the latitude and longitude variables must be treated with special care, as the live in $S^2$ instead of $\\mathbb{R}^2$. However, the sphere $S^2$ can be decomposed into an interval and a circle. In other words, the map $(\\theta, \\phi) \\to (\\cos(\\theta), \\phi)$ maps the sphere to a cylinder, $S^2 \\to [-1,1] \\times S^1$.</p> <p>What this means is we can model $\\cos(\\theta)$ in the usual way, as long as we map it to a latent distribution with compact support, e.g. the uniform distribution $\\mathcal{U}(-5, 5)$.</p> <p>However, $\\phi$ still lives on the circle. Fear not, we can still model $\\phi$ with our usual bijectors, as long as we enforce the following boundary conditions: $$ f(0) = 0, \\quad f(2\\pi) = 2\\pi, \\quad \\nabla\\! f(0)= \\nabla\\! f(2\\pi), \\quad \\nabla\\! f(\\phi) &gt; 0.$$ We will see that these conditions are easy to impose with our Neural Spline Coupling.</p> <p>First, let's create a DataFrame with our transformed data:</p> In\u00a0[7]: Copied! <pre>data = pd.DataFrame(np.array([phi, np.cos(theta), logpop]).T, columns=(\"phi\", \"cos(theta)\", \"logpop\"))\n</pre> data = pd.DataFrame(np.array([phi, np.cos(theta), logpop]).T, columns=(\"phi\", \"cos(theta)\", \"logpop\")) <p>Now let's create our bijector. It'll be very similar to what we've done before, with small changes:</p> <ol> <li>First, when we use <code>ShiftBounds</code>, rather than setting <code>B=4</code> to avoid the boundary, we will set <code>B=5</code>, because we don't want the flow to be able to sample beyond the limits of our angles.</li> <li>Instead of using <code>RollingSplineCoupling</code>, we will manually use <code>NeuralSplineCoupling</code> so that we can set <code>periodic=True</code> on the splines that transform $\\phi$.</li> <li>We will set <code>transformed_dim=1</code> to make sure that each spline only transforms one of the dimensions at a time.</li> <li>I will increase the resolution of the splines (by increasing <code>K</code>), and increase the depth of the neural networks that parameterize the splines (by increasing <code>hidden_layers</code>).</li> </ol> <p>Since the data columns in <code>data</code> are <code>(\"phi\", \"cos(theta)\", \"logpop\")</code>, this means that the first <code>NeuralSplineCoupling</code> transforms <code>logpop</code>, and the second (after the <code>Roll</code>) transforms $\\cos(\\theta)$. Thus after the second <code>Roll</code>, $\\phi$ is now in the last column.</p> <p>We can now apply a <code>NeuralSplineCoupling</code> with <code>periodic=True</code>. This is all we have to do to enforce the periodic constraints we listed above!</p> <p>After one more <code>Roll</code>, all of the columns are back in the same order they started in. This is important, as it ensures that each dimension lines up the correct dimension of the latent space that we set up above.</p> In\u00a0[8]: Copied! <pre>mins = jnp.array(data.min(axis=0))\nmaxs = jnp.array(data.max(axis=0))\n\nbijector = Chain(\n    ShiftBounds(mins, maxs, 5),\n    NeuralSplineCoupling(K=128, hidden_layers=4, transformed_dim=1),\n    Roll(),\n    NeuralSplineCoupling(K=128, hidden_layers=4, transformed_dim=1),\n    Roll(),\n    NeuralSplineCoupling(K=128, hidden_layers=4, transformed_dim=1, periodic=True),\n    Roll(),\n)\n\nflow = Flow(data.columns, bijector)\n</pre> mins = jnp.array(data.min(axis=0)) maxs = jnp.array(data.max(axis=0))  bijector = Chain(     ShiftBounds(mins, maxs, 5),     NeuralSplineCoupling(K=128, hidden_layers=4, transformed_dim=1),     Roll(),     NeuralSplineCoupling(K=128, hidden_layers=4, transformed_dim=1),     Roll(),     NeuralSplineCoupling(K=128, hidden_layers=4, transformed_dim=1, periodic=True),     Roll(), )  flow = Flow(data.columns, bijector) <pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n</pre> <p>Now we can train our flow:</p> In\u00a0[9]: Copied! <pre>losses = flow.train(data, epochs=200, verbose=True)\n</pre> losses = flow.train(data, epochs=200, verbose=True) <pre>Training 200 epochs \nLoss:\n(0) 4.3741\n(1) 2.7319\n(11) 1.0787\n(21) 0.7544\n(31) 0.5492\n(41) 0.5004\n(51) 0.3901\n(61) 0.4068\n(71) 0.2925\n(81) 0.2543\n(91) 0.2410\n(101) 0.1949\n(111) 0.1393\n(121) 0.1146\n(131) 0.1445\n(141) 0.0759\n(151) 0.0442\n(161) 0.0292\n(171) 0.0186\n(181) 0.0108\n(191) -0.0175\n(200) 0.0124\n</pre> <p>Let's plot the training losses to make sure everything looks okay.</p> In\u00a0[10]: Copied! <pre>plt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training loss\")\nplt.show()\n</pre> plt.plot(losses) plt.xlabel(\"Epoch\") plt.ylabel(\"Training loss\") plt.show() <p>Now let's draw samples from the trained flow, and plot them together with the truth data.</p> In\u00a0[11]: Copied! <pre># sample from the trained flow\nsamples = flow.sample(data.shape[0], seed=0)\n</pre> # sample from the trained flow samples = flow.sample(data.shape[0], seed=0) In\u00a0[12]: Copied! <pre>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,4), dpi=250)\n\nnside = 128\nflip = \"geo\"\ncbar = False\n\n# trained flow\nplt.axes(ax1)\nmollweide_hist(\n    np.arccos(samples[\"cos(theta)\"].to_numpy()),\n    samples[\"phi\"].to_numpy(),\n    weights=samples[\"logpop\"],\n    title=\"Estimated\",\n    nside=nside,  \n    flip=flip, \n    cbar=cbar,\n    hold=True,\n)\n\n# truth\nplt.axes(ax2)\nmollweide_hist(\n    theta,\n    phi,\n    weights=logpop,\n    title=\"Truth\",\n    nside=nside,  \n    flip=flip, \n    cbar=cbar,\n    hold=True,\n)\n</pre> fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,4), dpi=250)  nside = 128 flip = \"geo\" cbar = False  # trained flow plt.axes(ax1) mollweide_hist(     np.arccos(samples[\"cos(theta)\"].to_numpy()),     samples[\"phi\"].to_numpy(),     weights=samples[\"logpop\"],     title=\"Estimated\",     nside=nside,       flip=flip,      cbar=cbar,     hold=True, )  # truth plt.axes(ax2) mollweide_hist(     theta,     phi,     weights=logpop,     title=\"Truth\",     nside=nside,       flip=flip,      cbar=cbar,     hold=True, ) <p>Pretty good! You could get even better resolution by training for longer and doing some hyperparameter tuning.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/spherical_flow_example/#modeling-data-with-a-compact-domain-and-periodic-topology","title":"Modeling data with a compact domain and periodic topology\u00b6","text":"<p>This example notebook demonstrates using PZFlow to model data on a sphere (specifically the Earth). This is slightly more complicated, as two of our variables, specifically latitude and longitude, belong to a space with non-Euclidean topology.</p>"},{"location":"tutorials/weighted/","title":"Training Weights","text":"<p>If running in Colab, to switch to GPU, go to the menu and select Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>In addition, uncomment and run the following code:</p> In\u00a0[1]: Copied! <pre># !pip install pzflow matplotlib\n</pre> # !pip install pzflow matplotlib  In\u00a0[1]: Copied! <pre>from pzflow import Flow\nfrom pzflow.examples import get_twomoons_data\n\nimport jax.numpy as jnp\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> from pzflow import Flow from pzflow.examples import get_twomoons_data  import jax.numpy as jnp import numpy as np import matplotlib.pyplot as plt  In\u00a0[2]: Copied! <pre>plt.rcParams[\"figure.facecolor\"] = \"white\"\n</pre> plt.rcParams[\"figure.facecolor\"] = \"white\"  <p>First let's load the two moons data set again:</p> In\u00a0[3]: Copied! <pre>data = get_twomoons_data()\ndata\n</pre> data = get_twomoons_data() data  Out[3]: x y 0 -0.748695 0.777733 1 1.690101 -0.207291 2 2.008558 0.285932 3 1.291547 -0.441167 4 0.808686 -0.481017 ... ... ... 99995 1.642738 -0.221286 99996 0.981221 0.327815 99997 0.990856 0.182546 99998 -0.343144 0.877573 99999 1.851718 0.008531 <p>100000 rows \u00d7 2 columns</p> <p>Let's plot it to see what it looks like.</p> In\u00a0[4]: Copied! <pre>plt.hist2d(data[\"x\"], data[\"y\"], bins=200)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n</pre> plt.hist2d(data[\"x\"], data[\"y\"], bins=200) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show()  <p>We will  build the same normalizing flow as in the intro:</p> In\u00a0[5]: Copied! <pre>flow = Flow([\"x\", \"y\"])\n</pre> flow = Flow([\"x\", \"y\"])  <p>But this time we will provide training weights. This might be useful if you want some of your training samples to matter more during the training. Here we will down-weight each of the samples with $y &lt; 0$ by a factor of 10.</p> In\u00a0[6]: Copied! <pre>weights = np.where(data[\"y\"] &gt; 0, 1.0, 0.1)\nweights\n</pre> weights = np.where(data[\"y\"] &gt; 0, 1.0, 0.1) weights Out[6]: <pre>array([1. , 0.1, 1. , ..., 1. , 1. , 1. ], shape=(100000,))</pre> <p>Now let's train with these weights</p> In\u00a0[8]: Copied! <pre>%%time\nlosses = flow.train(data, train_weight=weights, verbose=True)\n</pre> %%time losses = flow.train(data, train_weight=weights, verbose=True) <pre>WARNING:2025-03-31 16:53:45,320:jax._src.xla_bridge:967: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n</pre> <pre>Training 100 epochs \nLoss:\n(0) 2.2899\n(1) 1.5460\n(6) 0.7903\n(11) 0.7530\n(16) 0.7329\n(21) 0.7621\n(26) 0.7386\n(31) 0.7128\n(36) 0.6850\n(41) 0.7337\n(46) 0.7127\n(51) 0.7068\n(56) 0.7009\n(61) 0.7088\n(66) 0.7496\n(71) 0.6928\n(76) 0.7147\n(81) 0.7337\n(86) 0.7417\n(91) 0.7321\n(96) 0.7316\n(100) 0.6802\nCPU times: user 6min 34s, sys: 1min 19s, total: 7min 54s\nWall time: 1min 1s\n</pre> <p>Now let's plot the training losses to make sure everything looks like we expect it to...</p> In\u00a0[9]: Copied! <pre>plt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training loss\")\nplt.show()\n</pre> plt.plot(losses) plt.xlabel(\"Epoch\") plt.ylabel(\"Training loss\") plt.show()  <p>Perfect!</p> <p>Now we can draw samples from the flow, using the <code>sample</code> method. Let's draw 10000 samples and make another histogram to see if it matches the data.</p> In\u00a0[10]: Copied! <pre>samples = flow.sample(10_000, seed=0)\n</pre> samples = flow.sample(10_000, seed=0)  <p>This time we see that the flow really suppresses samples for $y &lt; 0$, as expected given our training weights!</p> In\u00a0[11]: Copied! <pre>plt.hist2d(samples[\"x\"], samples[\"y\"], bins=200)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()\n</pre> plt.hist2d(samples[\"x\"], samples[\"y\"], bins=200) plt.xlabel(\"x\") plt.ylabel(\"y\") plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/weighted/#using-sample-weights-in-model-training","title":"Using sample weights in model training\u00b6","text":"<p>This notebook uses the same data as the introduction, but we will train the network while giving different weights to our training samples.</p>"}]}